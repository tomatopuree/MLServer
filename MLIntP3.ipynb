{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is its own cell because it takes a while to load this thing\n",
    "from gensim import models\n",
    "\n",
    "# takes a little bit. increase limit at own risk.\n",
    "# model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  \n",
    "model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and list of scrape times.\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import requests\n",
    "import json\n",
    "import _pickle as pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "import cv2\n",
    "import urllib.request\n",
    "import sqlite3 as sql\n",
    "import base64\n",
    "import ffmpy3\n",
    "from file_read_backwards import FileReadBackwards\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from nltk.data import load\n",
    "import nltk\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import xml\n",
    "import xml.dom.minidom\n",
    "\n",
    "\n",
    "\n",
    "list_of_scrape_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## class Loader\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, list_of_ids = [], list_of_times = []):\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    filedir = \"\"\n",
    "\n",
    "    ## 'file' omitted because it's not used in generating features\n",
    "    list_of_types = ['text','log','contact','calender','gps','tweets','Instagram', 'Instagram media', 'audio','phq']\n",
    "    \n",
    "    def deletdatboi(self, nombre):\n",
    "        shutil.rmtree('./datafor' + list_of_scrape_times[nombre])\n",
    "        \n",
    "    def lids(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        list_of_approved_ids = []\n",
    "        text_file = open(\"final_codes.txt\", \"r\")\n",
    "        list_of_approved_ids = text_file.read().split('\\n')\n",
    "        del list_of_approved_ids[-1]\n",
    "        \n",
    "        self.list_of_ids = []\n",
    "        \n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            if (row[0] in list_of_approved_ids):\n",
    "                self.list_of_ids.append(row[0])\n",
    "        \n",
    "        return self.list_of_ids\n",
    "    \n",
    "    def lits(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        list_of_approved_ids = []\n",
    "        text_file = open(\"final_codes.txt\", \"r\")\n",
    "        list_of_approved_ids = text_file.read().split('\\n')\n",
    "        del list_of_approved_ids[-1]\n",
    "        \n",
    "        self.list_of_times = []\n",
    "        \n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            if (row[0] in list_of_approved_ids):\n",
    "                self.list_of_times.append(row[1])\n",
    "    \n",
    "        return self.list_of_times\n",
    "    \n",
    "    def downloadAndLabel(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        self.list_of_ids = []\n",
    "        \n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            self.list_of_ids.append(row[0])\n",
    "            self.list_of_times.append(row[1])\n",
    "\n",
    "        number_cols = len(self.list_of_types)\n",
    "        number_rows = len(self.list_of_ids)\n",
    "\n",
    "        ### create directory for this particular scrape/pull\n",
    "        timenow  = str(int(time.time())) # for temporal congruency\n",
    "        timereadable = datetime.datetime.fromtimestamp(int(timenow)).strftime('%H:%M')\n",
    "        list_of_scrape_times.append(timereadable)\n",
    "        os.mkdir('./datafor' + timereadable)\n",
    "        Loader.filedir = './datafor' + timereadable\n",
    "\n",
    "        # alex gave me a list of approved mturk ids\n",
    "        list_of_approved_ids = []\n",
    "        text_file = open(\"final_codes.txt\", \"r\")\n",
    "        list_of_approved_ids = text_file.read().split('\\n')\n",
    "        del list_of_approved_ids[-1]\n",
    "        \n",
    "        for i in range(0,number_rows):\n",
    "            for j in range(0,number_cols):\n",
    "                \n",
    "                if (self.list_of_ids[i] not in list_of_approved_ids):\n",
    "                    break\n",
    "                \n",
    "                apickle = []\n",
    "\n",
    "                exampleLookup = (self.list_of_ids[i], self.list_of_types[j])\n",
    "                for row in c.execute('SELECT DISTINCT* FROM data WHERE id=? AND type=?', exampleLookup):\n",
    "                    # the row with the ID, type, and content\n",
    "                    apickle.append(row[2])\n",
    "                    \n",
    "                    # WEIRD: duplicate ids in data table with different data\n",
    "                    if exampleLookup[1] == \"phq\" or exampleLookup[1] == \"audio\":\n",
    "                        break\n",
    "\n",
    "                ## if there is data, there is data\n",
    "                ## if there is no data, there is no data\n",
    "                if(len(apickle) != 0):\n",
    "                    pickle.dump(apickle, open( Loader.filedir  + \"/DP\" + self.list_of_ids[i] +  self.list_of_types[j] + \".p\", \"wb\" ))\n",
    "\n",
    "        \n",
    "        ## OLD CODE FOR INTERFACING THROUGH THE WEBSERVER\n",
    "        ## CODE ABOVE ACCESSES A .DB FILE LOCALLY\n",
    "        '''\n",
    "        \n",
    "        r = requests.get('http://depressionmqp.wpi.edu:8080/getids')\n",
    "        list_of_idtime = r.json()\n",
    "        \n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_ids.append( list_of_idtime[i]['id'].encode('ascii','ignore') )\n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_times.append( list_of_idtime[i]['date'] )\n",
    "\n",
    "        number_cols = len(self.list_of_types)\n",
    "        number_rows = len(self.list_of_ids)\n",
    "        \n",
    "        ### create directory for this particular scrape/pull\n",
    "        timenow  = str(int(time.time())) # for temporal congruency\n",
    "        timereadable = datetime.datetime.fromtimestamp(int(timenow)).strftime('%H:%M')\n",
    "        list_of_scrape_times.append(timereadable)\n",
    "        os.mkdir('./datafor' + timereadable)\n",
    "        Loader.filedir = './datafor' + timereadable\n",
    "\n",
    "\n",
    "        for i in range(0,number_rows):\n",
    "            for j in range(0,number_cols):\n",
    "                \n",
    "                temp = requests.get('http://depressionmqp.wpi.edu:8080/getdata?id=' + str(int(self.list_of_ids[i])) + '&type=' + self.list_of_types[j])\n",
    "                fintemp = json.loads(temp.text)[\"data\"]\n",
    "                \n",
    "                while(json.loads(temp.text)[\"nextURL\"] != ''):\n",
    "                    temp = requests.get('http://depressionmqp.wpi.edu:8080' + json.loads(temp.text)[\"nextURL\"])\n",
    "                    fintemp += json.loads(temp.text)[\"data\"]\n",
    "                \n",
    "                pickle.dump(fintemp, open( Loader.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  self.list_of_types[j] + \".p\", \"wb\" ))\n",
    "\n",
    "                # loads it into memory. i will not use this for now\n",
    "                # for sake of architectural sanity\n",
    "                # list_of_jsons[i][j] = temp\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Featurizer featurizes methods that convert json objects of the appropriate type into features\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.name = name\n",
    "\n",
    "    # takes in text pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of texts sent in the 24 hours before the scrape\n",
    "    # the last element is count of texts sent on the 24 hour window 14 days before the scrapedate\n",
    "    def textFreqVec14(self, text, scrapedate):\n",
    "        \n",
    "        textFreqVec = np.zeros((14,))\n",
    "        \n",
    "        # moving average index\n",
    "        n = 5\n",
    "    \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # assuming unordered texts (WHICH TURNS OUT IS THE CASE)\n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+n)*mseconds_in_day)\n",
    "            for t in range(0,len(text)):\n",
    "                text_date = int(json.loads(text[t])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > text_date) and (text_date > time_during_week_lb)):\n",
    "                    textFreqVec[day] += 1.0\n",
    "                    \n",
    "        return textFreqVec/n\n",
    "    \n",
    "\n",
    "    # takes in call pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of calls sent in the 24 hours before the scrape\n",
    "    # the last element is count of calls sent on the 24 hour window 14 days before the scrapedate\n",
    "    def callFreqVec14(self, call, scrapedate):\n",
    "        \n",
    "        callFreqVec = np.zeros((14,))\n",
    "        \n",
    "        # moving average index\n",
    "        n = 5\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+n)*mseconds_in_day)\n",
    "            for c in range(0,len(call)):\n",
    "                call_date = int(json.loads(call[c])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > call_date) and (call_date > time_during_week_lb)):\n",
    "                    callFreqVec[day] += 1.0\n",
    "                    \n",
    "        return callFreqVec/n\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, return master vector\n",
    "    def embeddingToMastersum(self, tweets):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(tweets)):\n",
    "            try:\n",
    "                masterSum += self.tweetToEmbedding(tweets[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is one single tweet, returns vector embedding of entire tweet.\n",
    "    # eg: responseobject.json()[0]\n",
    "    def tweetToEmbedding(self, tweet):\n",
    "\n",
    "        q = tweet['text'].split()\n",
    "        \n",
    "        sumVector = np.zeros((300,))\n",
    "        \n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "        \n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, returns follow count\n",
    "    def followerCount(self, tweets):\n",
    "\n",
    "        followerCount = 0\n",
    "\n",
    "        try:\n",
    "            followerCount = json.loads(tweets[0])['user']['followers_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followerCount\n",
    "\n",
    "    # input is tweets pickle, returns friend count\n",
    "    def followingCount(self, tweets):\n",
    "\n",
    "        followingCount = 0\n",
    "        \n",
    "        try:\n",
    "            followingCount = json.loads(tweets[0])['user']['friends_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followingCount\n",
    "\n",
    "    # input is tweets pickle, return avg likes per post for the last 2 weeks\n",
    "    def twitterLikeFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitLikeVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitLikeVec[0] += 1.0\n",
    "                    \n",
    "        if(twitLikeVec[0] == 0):\n",
    "            return np.zeros((1,))\n",
    "                    \n",
    "        return twitLikeVec/14\n",
    "\n",
    "\n",
    "    # input is tweets pickle, return avg retweets per post for the last 2 weeks\n",
    "    def twitterRetweetFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitRTVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitRTVec[0] += json.loads(tweets[t])['favorite_count']\n",
    "                    \n",
    "        return twitRTVec/14\n",
    "    \n",
    "    # input is contacts pickle, returns number of contacts\n",
    "    def numOfContacts(self, contacts):\n",
    "        \n",
    "        return len(contacts)\n",
    "    \n",
    "    # input is instagram pickle, return two features: follows count, followed by count  \n",
    "    def instagramThings(self, instagram):\n",
    "        \n",
    "        followsFollowed = np.zeros((2,))\n",
    "        \n",
    "        try:\n",
    "            if(type(json.loads(instagram[0])) == str):\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            else: # its a dictionary like it's supposed to be\n",
    "                followsFollowed[0] = json.loads(instagram[0])['data']['counts']['follows']\n",
    "                followsFollowed[1] = json.loads(instagram[0])['data']['counts']['followed_by']\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        return followsFollowed\n",
    "    \n",
    "    # takes in instagramMedia pickle scrape date, spits out filter usage frequency for the past 2 weeks\n",
    "    def instagramFilterFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        instaFiltVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                if(json.loads(instagramMedia[t])['filter'] == \"Normal\"):\n",
    "                    instaFiltVec[0] += 1.0\n",
    "                    \n",
    "        \n",
    "        return instaFiltVec/14\n",
    "        \n",
    "       \n",
    "    \n",
    "    # takes in instagramMedia pickle and scrape date, returns a vector that\n",
    "    # contains a normalized percentage (0-1) for the usage of the filters\n",
    "    # listed below for the past 2 weeks: \n",
    "    # 'Amaro': 0,\n",
    "    #  'Crema': 0,\n",
    "    #  'Hefe': 5,\n",
    "    #  'Inkwell': 0,\n",
    "    #  'Rise': 0,\n",
    "    #  'Valencia': 0,\n",
    "    #  'Willow': 0,\n",
    "    #  'X-Pro II': 0}\n",
    "    \n",
    "    # output example: [0.5,0,0,0,0,0.5,0,0] \n",
    "    # interpretation: user used valencia half the time, Willow the other half\n",
    "    # of time, for the past 2 weeks of Instagram posts.1`sas\n",
    "    def instagramFilterVector(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        filters = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "        filtervec = np.zeros((8,))\n",
    "        \n",
    "        numposts = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                numposts += 1\n",
    "                \n",
    "        if(numposts == 0):\n",
    "            return np.zeros((8,))\n",
    "                \n",
    "        for i in range(0,numposts):\n",
    "            filt = json.loads(instagramMedia[i])['filter']\n",
    "            if(filt in filters):\n",
    "                filters[filt] += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for i in range(0,8):\n",
    "            filtervec[i] = filters[list(filters)[i]]\n",
    "\n",
    "        # percentage of how much the filters are used as a normalized vector\n",
    "        return filtervec/(numposts)\n",
    "    \n",
    "    # takes in InstagramMedia, returns comment and like frequency for the \n",
    "    # past 2 weeks.\n",
    "    def instagramLikeComFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        counts = np.zeros((2,))\n",
    "\n",
    "        postcount = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                counts[0] += json.loads(instagramMedia[t])['likes']['count']\n",
    "                counts[1] += json.loads(instagramMedia[t])['comments']['count']\n",
    "                postcount += 1\n",
    "        \n",
    "        if ((counts[0] + counts[1] == 0) or (postcount == 0)):\n",
    "            return np.zeros((2,))\n",
    "\n",
    "        return counts/postcount\n",
    "\n",
    "\n",
    "    # takes in instagramMedia pickle and scrapedate, returns IG post \n",
    "    # frequency for the past 2 weeks\n",
    "    def instagramPostFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        postcount = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount[0] += 1.0\n",
    "                \n",
    "        return postcount/14\n",
    "        \n",
    "        \n",
    "    # takes instagramMedia, returns pixelwise average values for [H,S,V]\n",
    "    # (Hue, Saturation, Value) for all posts in the past 2 weeks, the \n",
    "    # count of faces as a frequency of faces per picture, for the past\n",
    "    # 2 weeks as well.\n",
    "    \n",
    "    # testvariable:\n",
    "    # empty string: \"\" if not testing\n",
    "    # \n",
    "    def averageHSVF(self, instagramMedia, scrapedate):\n",
    "\n",
    "        avgs = np.zeros((4,))        \n",
    "        \n",
    "        postcount = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "                \n",
    "        sumHue = 0\n",
    "        sumSatur = 0\n",
    "        sumVal = 0\n",
    "        sumFaces = 0\n",
    "            \n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                \n",
    "                postcount += 1\n",
    "\n",
    "                if(os.path.exists(\"./ILLSTOPBLINKINGSOON\")):\n",
    "                    shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "                os.mkdir('./ILLSTOPBLINKINGSOON')\n",
    "                \n",
    "                url = json.loads(instagramMedia[t])['images']['thumbnail']['url']\n",
    "                \n",
    "                try:\n",
    "                    urllib.request.urlretrieve(url, './ILLSTOPBLINKINGSOON/' + \"img\" + '.jpg')\n",
    "                except HTTPError:\n",
    "                    postcount -= 1\n",
    "                    continue\n",
    "\n",
    "                # face_cascade here is a pre trained classifier for frontal faces \n",
    "                face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "                \n",
    "                ## BGR and not RGB because imread reads in BGR\n",
    "                img = cv2.imread('./ILLSTOPBLINKINGSOON/' + \"img\" + '.jpg')\n",
    "\n",
    "                hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "                grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "                sumFaces += len(faces)\n",
    "\n",
    "                for i in range(0,hsv.shape[0]):\n",
    "                    for j in range(0,hsv.shape[1]):\n",
    "                        sumHue += hsv[i,j,0]\n",
    "                        sumSatur += hsv[i,j,1]\n",
    "                        sumVal += hsv[i,j,2]\n",
    "\n",
    "                shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "                        \n",
    "        sums = [sumHue,sumSatur,sumVal]     \n",
    "\n",
    "        if(postcount == 0):\n",
    "            return avgs\n",
    "        \n",
    "        ## 22500 = 150x150 = instagram photo thumbnail shape\n",
    "        avgs = list(map(lambda x: x/(22500*postcount), sums)).append(sumFaces/postcount)        \n",
    "                \n",
    "        return avgs      \n",
    "\n",
    "        \n",
    "            \n",
    "    \n",
    "    #input is texts pickle, return master vector \n",
    "    def embeddingToMastersumText(self, texts):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(texts)):\n",
    "            try:\n",
    "                masterSum += self.textToEmbedding(texts[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is single text, returns vector embedding of entire text.\n",
    "    def textToEmbedding(self, text):\n",
    "        \n",
    "        q = json.loads(text)[\"body\"].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "\n",
    "\n",
    "    # give it a voice pickle, returns A LOT of features generated\n",
    "    # by openSMILE\n",
    "    def voiceFeaturizer(self, voice):\n",
    "\n",
    "        audiofeaturevec = np.zeros((1583,))\n",
    "\n",
    "        if(len(voice) == 0):\n",
    "            return audiofeaturevec\n",
    "\n",
    "        # base64 string -> bitstring -> bitstream -> write into 3gp file\n",
    "        bytestream = base64.b64decode(voice[0])\n",
    "        fh = open(\"audio.3gp\",\"wb\")\n",
    "        fh.write(bytestream)\n",
    "        fh.close()\n",
    "\n",
    "        # wav -> 3gp\n",
    "        ff = ffmpy3.FFmpeg( inputs={'audio.3gp': None}, outputs={'audio.wav': None})\n",
    "        ff.run()\n",
    "\n",
    "        os.remove(\"audio.3gp\")\n",
    "\n",
    "        # call to openSMILE\n",
    "        os.system( os.getcwd() + '/openSMILE-2.1.0/bin/linux_x64_standalone_static/SMILExtract -C ' + os.getcwd() + '/openSMILE-2.1.0/config/emobase2010.conf -I audio.wav -O \"out.csv\"')\n",
    "\n",
    "        os.remove(\"audio.wav\")\n",
    "\n",
    "        # csv file has a giant header. last line contains the features we want\n",
    "        # so we read the last line, cut out more useless string with 'l[9:]\n",
    "        with FileReadBackwards(\"out.csv\", encoding=\"utf-8\") as frb:\n",
    "            for l in frb:\n",
    "                b = l[9:].split(',')\n",
    "                break\n",
    "\n",
    "        os.remove(\"out.csv\")\n",
    "\n",
    "        # make list of string into list of floats\n",
    "        a = list(map(float, b))\n",
    "\n",
    "    \n",
    "    ## give it text pickle, gives you back a 45 element vector of how many \n",
    "    ## Parts Of Speech that person has used.\n",
    "    def POSTagger(self, text):\n",
    "\n",
    "        if(len(text) == 0):\n",
    "            return np.zeros((45,))\n",
    "\n",
    "        POSFreqVec = np.zeros((45,))\n",
    "\n",
    "        tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "        \n",
    "        newtagdic = {}\n",
    "        for key in tagdict.keys():\n",
    "            newtagdic[key] = 0\n",
    "\n",
    "        def text_process(mess):\n",
    "            # Remove all punctuation, stopwords\n",
    "            nopunc = [char for char in mess if char not in string.punctuation]\n",
    "            nopunc = ''.join(nopunc)\n",
    "            return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "        \n",
    "        for i in range(0,len(text)):\n",
    "            body = json.loads(text[i])[\"body\"]\n",
    "            q = text_process(body)\n",
    "            txt = nltk.Text(q)\n",
    "            tags = nltk.pos_tag(txt)\n",
    "\n",
    "            for tag in tags:\n",
    "                pos = tag[1]\n",
    "                newtagdic[pos] += 1\n",
    "\n",
    "        for i in range(0,len(newtagdic)):\n",
    "            POSFreqVec[i] = newtagdic[list(newtagdic)[i]]\n",
    "\n",
    "        return POSFreqVec/len(text)\n",
    "    \n",
    "    ## takes in text pickle and a scrapedate, gives you a 10 day moving average of \n",
    "    ## sentiment score for the past 14 days, courtesy of TextBlob\n",
    "    def SentAnalysis(self, text, scrapedate):\n",
    "\n",
    "        if(len(text) == 0):\n",
    "            return np.zeros((14,))\n",
    "\n",
    "        SentFreqVec = np.zeros((14,))\n",
    "\n",
    "        def text_process(mess):\n",
    "            # Remove all punctuation, stopwords\n",
    "            nopunc = [char for char in mess if char not in string.punctuation]\n",
    "            nopunc = ''.join(nopunc)\n",
    "            return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "        # moving average index\n",
    "        n = 5\n",
    "\n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "\n",
    "        # assuming unordered texts (WHICH TURNS OUT IS THE CASE)\n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+n)*mseconds_in_day)\n",
    "            for t in range(0,len(text)):\n",
    "                text_date = int(json.loads(text[t])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > text_date) and (text_date > time_during_week_lb)):\n",
    "                    body = json.loads(text[t])[\"body\"]\n",
    "                    blob = TextBlob(\" \".join(text_process(body)))\n",
    "\n",
    "                    SentFreqVec[day] += blob.sentiment.polarity\n",
    "\n",
    "        # normalize\n",
    "        return SentFreqVec\n",
    "\n",
    "\n",
    "    # returns [q1,q2,q3,q4,q5,q6,q7,q8,q9] and sum of all these scores\n",
    "    def labelGenerator(self, phq):\n",
    "        \n",
    "        labelVector = np.zeros((10,))\n",
    "        sumOfScores = 0\n",
    "        \n",
    "        # print(phq)\n",
    "        \n",
    "        for i in range(0,9):\n",
    "            temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "            labelVector[i] = temp\n",
    "            labelVector[9] += temp\n",
    "            \n",
    "        return labelVector\n",
    "    \n",
    "    \n",
    "    # NOTE: \n",
    "    # The below 5 functions are written by Bella.\n",
    "    \n",
    "    def numDiff(self, lizt):\n",
    "        suma = 0\n",
    "        for i in range(0, len(lizt)):\n",
    "            a = lizt[i]\n",
    "            xmldoc = xml.dom.minidom.parseString(a)\n",
    "            kml = xmldoc.getElementsByTagName(\"kml\")[0]\n",
    "            document = kml.getElementsByTagName(\"Document\")[0]\n",
    "            placemarks = document.getElementsByTagName(\"Placemark\")\n",
    "\n",
    "            for placemark in placemarks:\n",
    "                desc = placemark.getElementsByTagName(\"description\")[0].firstChild.data\n",
    "                suma = suma + 1\n",
    "\n",
    "                #print desc\n",
    "        return suma\n",
    "\n",
    "\n",
    "    def maxDist(self, lizt):\n",
    "        suma = 0\n",
    "        maxa = 0\n",
    "        for i in range(0, len(lizt)):\n",
    "            a = lizt[i]\n",
    "            xmldoc = xml.dom.minidom.parseString(a)\n",
    "            kml = xmldoc.getElementsByTagName(\"kml\")[0]\n",
    "            document = kml.getElementsByTagName(\"Document\")[0]\n",
    "            placemarks = document.getElementsByTagName(\"Placemark\")\n",
    "\n",
    "            for placemark in placemarks:\n",
    "                desc = placemark.getElementsByTagName(\"description\")[0].firstChild.data\n",
    "                list1 = desc.split(\"Distance\")\n",
    "                list2 = int(list1[1].split(\"m\")[0])\n",
    "                if(list2>maxa):\n",
    "                    maxa = list2\n",
    "                #print list2 , max\n",
    "        return maxa\n",
    "\n",
    "\n",
    "    def totDist(self, lizt):\n",
    "        suma = 0\n",
    "        for i in range(0, len(lizt)):\n",
    "            a = lizt[i]\n",
    "            xmldoc = xml.dom.minidom.parseString(a)\n",
    "            kml = xmldoc.getElementsByTagName(\"kml\")[0]\n",
    "            document = kml.getElementsByTagName(\"Document\")[0]\n",
    "            placemarks = document.getElementsByTagName(\"Placemark\")\n",
    "\n",
    "            for placemark in placemarks:\n",
    "                desc = placemark.getElementsByTagName(\"description\")[0].firstChild.data\n",
    "                list1 = desc.split(\"Distance\")\n",
    "                list2 = int(list1[1].split(\"m\")[0])\n",
    "                suma = suma + list2\n",
    "#         print(suma)\n",
    "        return suma\n",
    "\n",
    "\n",
    "    #number of times a user did activities\n",
    "    def activeFreq(self, lizt):\n",
    "        suma = 0\n",
    "        activeFreq = 0\n",
    "        for i in range(0, len(lizt)):\n",
    "            a = lizt[i]\n",
    "            xmldoc = xml.dom.minidom.parseString(a)\n",
    "            kml = xmldoc.getElementsByTagName(\"kml\")[0]\n",
    "            document = kml.getElementsByTagName(\"Document\")[0]\n",
    "            placemarks = document.getElementsByTagName(\"Placemark\")\n",
    "\n",
    "            for placemark in placemarks:\n",
    "                desc = placemark.getElementsByTagName(\"description\")[0].firstChild.data\n",
    "                splitActivity = desc.split(\" \")\n",
    "                activity = splitActivity[1]\n",
    "                suma = suma + 1\n",
    "\n",
    "                #print activity\n",
    "                if ( activity == \"Walking\" or activity == \"Biking\" or activity == \"Running\"):\n",
    "                    #print \"found\"\n",
    "                    activeFreq = activeFreq + 1\n",
    "                #print activity\n",
    "        #return sum\n",
    "        return activeFreq\n",
    "        \n",
    "    # distance ran by user\n",
    "    def distanceRan(self, lizt):\n",
    "        walkedDistance = 0\n",
    "        for i in range(0, len(lizt)):\n",
    "            a = lizt[i]\n",
    "            xmldoc = xml.dom.minidom.parseString(a)\n",
    "            kml = xmldoc.getElementsByTagName(\"kml\")[0]\n",
    "            document = kml.getElementsByTagName(\"Document\")[0]\n",
    "            placemarks = document.getElementsByTagName(\"Placemark\")\n",
    "\n",
    "            for placemark in placemarks:\n",
    "                desc = placemark.getElementsByTagName(\"description\")[0].firstChild.data\n",
    "                activitySplit = desc.split(\" \")  \n",
    "                activity = activitySplit[1]\n",
    "                distanceSplit = desc.split(\"Distance\")\n",
    "                distanceM = distanceSplit[1]\n",
    "                distanceMSplit = distanceM.split(\"m\")\n",
    "                distance = int( distanceMSplit[0])\n",
    "                #print desc\n",
    "                #print activity\n",
    "                #print distanceM\n",
    "                #print distance\n",
    "                if (activity == \"Walking\" or activity == \"Running\" or activity == \"Biking\"):\n",
    "                    walkedDistance = walkedDistance + distance\n",
    "        return walkedDistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(self, filedir, list_of_ids, list_of_times):\n",
    "        self.filedir = filedir\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.f = Featurizer()\n",
    "        self.num_of_people = len(list_of_ids)\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    BigMatrix = None\n",
    "    audioless = None\n",
    "    l = Loader()\n",
    "    num_of_people = len(l.lids())\n",
    "    \n",
    "    ### BEHOLD, MODALITY MATRICES\n",
    "    \n",
    "    LabelMtr = None\n",
    "    AudioMtr = None\n",
    "    IGMtr = None\n",
    "    ContactsMtr = None\n",
    "    TwitterMtr = None\n",
    "    TextMtr = None\n",
    "    CallMtr = None\n",
    "    GPSMtr = None\n",
    "    \n",
    "    \n",
    "    ### BEHOLD, FEATURE COLUMNS ###\n",
    "    \n",
    "    # call frequency\n",
    "    featureVectorCF = np.zeros((num_of_people,14))\n",
    "    # follower count\n",
    "    featureVectorFC = np.zeros((num_of_people,1))\n",
    "    # following count\n",
    "    featureVectorFC2 = np.zeros((num_of_people,1))\n",
    "    # twitter like frequency\n",
    "    featureVectorTWL = np.zeros((num_of_people,1))\n",
    "    # twitter retweet frequency\n",
    "    featureVectorTWRT = np.zeros((num_of_people,1))\n",
    "    # num of contacts\n",
    "    featureVectorNC = np.zeros((num_of_people,1))\n",
    "    # instagram follows, followed by\n",
    "    featureVectorIG1 = np.zeros((num_of_people,2))\n",
    "    # instagram filter usage freq\n",
    "    featureVectorIG2 = np.zeros((num_of_people,1))\n",
    "    # instagram filter vec: Valencia, X-Pro II, Hefe, Amaro, Rise, Willow, Crema, Inkwell\n",
    "    featureVectorIGFV = np.zeros((num_of_people,8))\n",
    "    # instagram like freq, comment freq\n",
    "    featureVectorIGLC = np.zeros((num_of_people,2))\n",
    "    # instagram post freq\n",
    "    featureVectorIG3 = np.zeros((num_of_people,1))\n",
    "    # instagram avg Hue, Saturation, Value, and total faces\n",
    "    featureVectorHSVF = np.zeros((num_of_people,4))\n",
    "    # audio features from openSMILE\n",
    "    featureVectorAUD = np.zeros((num_of_people,1583))\n",
    "    # text frequency\n",
    "    featureVectorTF = np.zeros((num_of_people,14))\n",
    "    # 45 long POS (part of speech) vector\n",
    "    POSFreqVec = np.zeros((num_of_people,45))\n",
    "    # 14 long vector, 10 day sentiment average for past 14 days\n",
    "    SentFreqVec = np.zeros((num_of_people,14))\n",
    "\n",
    "    # labels, indexes 0-8 for corresponding phq questions, last entry (index 9) is sum of them.\n",
    "    labelVector = np.zeros((num_of_people,10))\n",
    "    \n",
    "    # bag of words with word2vec (try last)\n",
    "    #featureVectorTW = np.zeros((self.num_of_people,300))\n",
    "    # bag of words with word2vec (try last)\n",
    "    #featureVectorTW2V = np.zeros((self.num_of_people,300))\n",
    "    \n",
    "    \n",
    "    gpsVector1 = np.zeros((num_of_people,1))\n",
    "    gpsVector2 = np.zeros((num_of_people,1))\n",
    "    gpsVector3 = np.zeros((num_of_people,1))\n",
    "    gpsVector4 = np.zeros((num_of_people,1))\n",
    "    gpsVector5 = np.zeros((num_of_people,1))\n",
    "\n",
    "    \n",
    "    ## The flow of operation is same under every title\n",
    "    ## if data file is there and has actual data -> feature\n",
    "    ## if data file is there and there is no data in it -> feature \n",
    "        ## ie. person shares text data and turns out they never text\n",
    "        ## it is sensible for us to consider this as valid data instead of discarding it\n",
    "    ## if data file isnt there -> NaN\n",
    "    \n",
    "    \n",
    "    ############# GPS ###############################\n",
    "    def genGPS(self):\n",
    "        \n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "            try:\n",
    "                a10 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"gps\" + \".p\", \"rb\" ))\n",
    "\n",
    "                Generator.gpsVector1[i] = self.f.numDiff(a10)\n",
    "                Generator.gpsVector2[i] = self.f.maxDist(a10)\n",
    "                Generator.gpsVector3[i] = self.f.totDist(a10)\n",
    "                Generator.gpsVector4[i] = self.f.activeFreq(a10)\n",
    "                Generator.gpsVector5[i] = self.f.distanceRan(a10)\n",
    "                \n",
    "\n",
    "            except FileNotFoundError:\n",
    "                Generator.gpsVector1[i:i+1,:] = np.nan\n",
    "                Generator.gpsVector2[i:i+1,:] = np.nan\n",
    "                Generator.gpsVector3[i:i+1,:] = np.nan\n",
    "                Generator.gpsVector4[i:i+1,:] = np.nan\n",
    "                Generator.gpsVector5[i:i+1,:] = np.nan\n",
    "                pass\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    ############# Call Log ###############################\n",
    "    def genCall(self):\n",
    "        \n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "            try:\n",
    "                a1 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"log\" + \".p\", \"rb\" ))\n",
    "    \n",
    "                Generator.featureVectorCF[i] = self.f.callFreqVec14(a1, self.list_of_times[i])\n",
    "                \n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorCF[i:i+1,:] = np.nan\n",
    "                pass     \n",
    "    \n",
    "\n",
    "    \n",
    "    ############# SMS Messages ###########################        \n",
    "    def genText(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "            try:\n",
    "                a2 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"text\" + \".p\", \"rb\" )) \n",
    "\n",
    "                Generator.POSFreqVec[i] = self.f.POSTagger(a2)\n",
    "                Generator.SentFreqVec[i] = self.f.SentAnalysis(a2, self.list_of_times[i])\n",
    "                Generator.featureVectorTF[i] = self.f.textFreqVec14(a2, self.list_of_times[i])\n",
    "                #featureVectorTW2V[i] = self.f.embeddingToMastersumText(a2)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorTF[i:i+1,:] = np.nan\n",
    "                #featureVectorTW2V[i] = np.nan\n",
    "                pass\n",
    "         \n",
    "            \n",
    "       \n",
    "    ############# Twitter ###############################        \n",
    "    def genTwitter(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "            try:\n",
    "                a3 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"tweets\" + \".p\", \"rb\" )) \n",
    "            \n",
    "                #featureVectorTW[i] = self.f.embeddingToMastersum(a3)\n",
    "                Generator.featureVectorFC[i] = self.f.followerCount(a3)\n",
    "                Generator.featureVectorFC2[i] = self.f.followingCount(a3)\n",
    "                Generator.featureVectorTWL[i] = self.f.twitterLikeFreq(a3, self.list_of_times[i])\n",
    "                Generator.featureVectorTWRT[i] = self.f.twitterRetweetFreq(a3, self.list_of_times[i])\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                #featureVectorTW[i] = np.nan\n",
    "                Generator.featureVectorFC[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorFC2[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorTWL[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorTWRT[i:i+1,:] = np.nan\n",
    "                pass\n",
    "    \n",
    "    \n",
    " \n",
    "    ############# Contacts ###############################\n",
    "    def genContacts(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "            try:\n",
    "                a4 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"contact\" + \".p\", \"rb\" )) \n",
    "\n",
    "                \n",
    "                Generator.featureVectorNC[i] = self.f.numOfContacts(a4)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorNC[i:i+1,:] = np.nan\n",
    "                pass\n",
    "    \n",
    "            \n",
    "  \n",
    "    ############# Instagram Media #########################      \n",
    "    def genIG(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "            try:\n",
    "                a5 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "\n",
    "            \n",
    "                Generator.featureVectorIG1[i] = self.f.instagramThings(a5)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorIG1[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                a6 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"Instagram media\" + \".p\", \"rb\" ))\n",
    "\n",
    "                \n",
    "                Generator.featureVectorIG2[i] = self.f.instagramFilterFreq(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorIGFV[i] = self.f.instagramFilterVector(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorIGLC[i] = self.f.instagramLikeComFreq(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorIG3[i] = self.f.instagramPostFreq(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorHSVF[i] = self.f.averageHSVF(a6, self.list_of_times[i])\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorIG2[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorIGFV[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorIGLC[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorIG3[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorHSVF[i:i+1,:] = np.nan\n",
    "                pass\n",
    "    \n",
    "            \n",
    "    ############# Audio #########################        \n",
    "    def genAudio(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "            try:\n",
    "                a7 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"audio\" + \".p\", \"rb\" ))\n",
    "\n",
    "                \n",
    "                Generator.featureVectorAUD [i] = self.f.voiceFeaturizer(a7)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorAUD [i:i+1,:] = np.nan\n",
    "                pass\n",
    "    \n",
    "         \n",
    "    ############# Label #########################\n",
    "    def genLabel(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "            try:\n",
    "                b1 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"phq\" + \".p\", \"rb\" )) \n",
    "\n",
    "            \n",
    "                Generator.labelVector[i] = self.f.labelGenerator(b1)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.labelVector[i:i+1,:] = np.nan\n",
    "                pass\n",
    "    \n",
    "    \n",
    "    def glueMatrixTogether(self):    \n",
    "        \n",
    "        GPSMtr = np.hstack((Generator.gpsVector1, \n",
    "                            Generator.gpsVector2, \n",
    "                            Generator.gpsVector3,\n",
    "                            Generator.gpsVector4, \n",
    "                            Generator.gpsVector5))\n",
    "        CallMtr = Generator.featureVectorCF        \n",
    "        TextMtr = np.hstack((Generator.featureVectorTF, \n",
    "                                        Generator.SentFreqVec, \n",
    "                                        Generator.POSFreqVec))         \n",
    "        TwitterMtr = np.hstack((Generator.featureVectorFC, \n",
    "                                         Generator.featureVectorFC2, \n",
    "                                         Generator.featureVectorTWL, \n",
    "                                         Generator.featureVectorTWRT))        \n",
    "        ContactsMtr = Generator.featureVectorNC        \n",
    "        IGMtr = np.hstack((Generator.featureVectorIG1, \n",
    "                                         Generator.featureVectorIG2, \n",
    "                                         Generator.featureVectorIGFV,\n",
    "                                         Generator.featureVectorIGLC, \n",
    "                                         Generator.featureVectorIG3, \n",
    "                                         Generator.featureVectorHSVF))        \n",
    "        AudioMtr = Generator.featureVectorAUD\n",
    "        LabelMtr = Generator.labelVector\n",
    "        \n",
    "        mtr = np.hstack((ContactsMtr, \n",
    "                                TwitterMtr, \n",
    "                                TextMtr, \n",
    "                                CallMtr, \n",
    "                                IGMtr,\n",
    "                                GPSMtr,\n",
    "                                AudioMtr, \n",
    "                                LabelMtr))\n",
    "        return mtr\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ContactsMtr = g.ContactsMtr # 1\n",
    "# TwitterMtr = g.TwitterMtr # 4\n",
    "# TextMtr = g.TextMtr # 73\n",
    "# CallMtr = g.CallMtr # 14\n",
    "# IGMtr = g.IGMtr # 18\n",
    "# GPSMtr =  # 5\n",
    "# AudioMtr = g.AudioMtr # 1583\n",
    "# LabelMtr = g.LabelMtr # 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1442.6251239776611\n",
      "1442.6270241737366\n"
     ]
    }
   ],
   "source": [
    "# Everything I coded in one cell!\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "l = Loader()\n",
    "# l.downloadAndLabel()\n",
    "\n",
    "\n",
    "g = Generator('./datafor22:19', l.lids(), l.lits())\n",
    "\n",
    "\n",
    "# 1-9 phq / 10 sum\n",
    "g.genLabel()\n",
    "\n",
    "# 1-1583 openSMILE\n",
    "g.genAudio() \n",
    "# 1-2 follow-follod / 3 filtfreq / 4-11 special filfreq / 12-13 like-com / 14 postfreq / 15-18 H-S-V-facecount\n",
    "g.genIG()\n",
    "# 1 num of contacts\n",
    "g.genContacts()\n",
    "# 1 followers / 2 following / 3 like freq / 4 RT freq\n",
    "g.genTwitter()\n",
    "# 14 text freq mov avg / 14 sentiment mov avg / 45 POS tags\n",
    "g.genText()\n",
    "# 14 call freq vec mov avg\n",
    "g.genCall()\n",
    "\n",
    "# 3 gps vecs\n",
    "g.genGPS()\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "mtr = g.glueMatrixTogether()\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RLASSO FEATURE IMPORTANCE EXPERIMENT\n",
    "\n",
    "[0.99 , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.97 , 0.955, 0.925,\n",
    "   0.95 , 0.94 , 0.895, 0.93 , 0.905, 0.95 , 0.965, 0.935, 0.875,\n",
    "   0.91 , 0.915, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   ,\n",
    "   0.   , 0.   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ((ContactsMtr,# 1\n",
    "# TwitterMtr, # 4\n",
    "# TextMtr, # 73\n",
    "# CallMtr, # 14\n",
    "# IGMtr, # 18\n",
    "# AudioMtr, #shitton \n",
    "# LabelMtr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RANDOM FOREST FEATURE IMPORTANCE EXPERIMENT\n",
    "# 14 text freq mov avg / 14 sentiment mov avg / 45 POS tags\n",
    "\n",
    "\n",
    "[0.00209353 0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.00334313 0.01855466\n",
    " 0.0046633  0.         0.01       0.01157963 0.00741232 0.00932736\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.00133495 0.         0.         0.         0.\n",
    " 0.00091267 0.028      0.         0.         0.00333141 0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.002      0.00282377 0.         0.00081539 0.         0.\n",
    " 0.         0.00142762 0.00079931 0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.00078331 0.         0.         0.         0.         0.00081749\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.00333382 0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.         0.\n",
    " 0.         0.         0.         0.         0.        ]\n",
    "\n",
    "\n",
    "# text mov avg days: 6,7,8,10,11,12,13\n",
    "# sent : 7,13,14\n",
    "# pos tags: 3,23,24,26,30,31,41\n",
    "# call: 1\n",
    "# instagram: follows count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1703)"
      ]
     },
     "execution_count": 1153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LAST EXECUTED: TESTED ALL FUNCTIONS AGAIN, RAN ON JAN 24 (PARTICIPANTS: 300})\n",
    "\n",
    "# mtr1.csv executed on Feb 12 (with gps)\n",
    "\n",
    "# df = pd.DataFrame(mtr)\n",
    "# df.to_csv(\"mtr2.csv\")\n",
    "\n",
    "# 79-92 -> CALL FREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PREPROCESSING (DEPRECATED, USELESS BUT ML EXPERIMENTS BELOW USES THEM)\n",
    "\n",
    "# n gives nth phq answer\n",
    "# 10 gives sum of all phqs\n",
    "def Yer(y, n):\n",
    "    return y[:,n-1:n]\n",
    "\n",
    "\n",
    "contactsStartEnd = [0,1]\n",
    "twitterStartEnd = [1,5]\n",
    "textStartEnd = [5,78]\n",
    "callStartEnd = [78,92]\n",
    "instagramStartEnd = [92,110]\n",
    "# gpsStartEnd = [110,113]\n",
    "\n",
    "audioStartEnd = [110,1693]\n",
    "\n",
    "allStartEnd = [contactsStartEnd[0], audioStartEnd[1]]\n",
    "\n",
    "\n",
    "ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"]\n",
    "\n",
    "# ftype = \"au\" audio / \"ig\" instagram / \"txt\" text / \"con\" contacts / \"tw\" twitter / \"call\" call\n",
    "# \"all\" = big matrix\n",
    "def Xer(X, ftype):\n",
    "    if(ftype == \"au\"):\n",
    "        return X[:,audioStartEnd[0]:audioStartEnd[1]]\n",
    "    if(ftype == \"ig\"):\n",
    "        return X[:,instagramStartEnd[0]:instagramStartEnd[1]]\n",
    "    if(ftype == \"txt\"):\n",
    "        return X[:,textStartEnd[0]:textStartEnd[1]]\n",
    "    if(ftype == \"con\"):\n",
    "        return X[:,contactsStartEnd[0]:contactsStartEnd[1]]\n",
    "    if(ftype == \"tw\"):\n",
    "        return X[:,twitterStartEnd[0]:twitterStartEnd[1]]\n",
    "    if(ftype == \"call\"):\n",
    "        return X[:,callStartEnd[0]:callStartEnd[1]]\n",
    "    if(ftype == \"gps\"):\n",
    "        return X[:,gpsStartEnd[0]:gpsStartEnd[1]]\n",
    "    if(ftype == \"all\"):\n",
    "        return X[:,allStartEnd[0]:allStartEnd[1]]\n",
    "    if(ftype == \"audioless\"):\n",
    "        return X[:,allStartEnd[0]:gpsStartEnd[1]]\n",
    "    if(ftype == \"gpsless\"):\n",
    "        return np.hstack((X[:,allStartEnd[0]:instagramStartEnd[1]], X[:,audioStartEnd[0]:audioStartEnd[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = mtr[:,allStartEnd[0]:allStartEnd[1]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(b).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.2       ,  0.2       ,  0.2       ,  0.2       ,\n",
       "         3.        ,  2.8       ,  2.8       ,  2.8       ,  3.        ,\n",
       "         0.2       ,  0.6       ,  0.6       ,  0.        ,  0.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        ,  0.34469697,\n",
       "         1.34469697,  1.34469697,  1.34469697,  1.34469697,  0.        ,\n",
       "         0.5       ,  0.5       ,  0.        ,  2.49038462,  0.        ,\n",
       "         0.        ,  0.02884615,  0.00961538,  0.        ,  0.15384615,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00961538,  0.05769231,\n",
       "         0.        ,  0.00961538,  0.        ,  1.73076923,  0.10576923,\n",
       "         1.48076923,  0.34615385,  0.        ,  0.        ,  0.17307692,\n",
       "         0.00961538,  0.00961538,  0.        ,  0.        ,  0.        ,\n",
       "         0.24038462,  0.00961538,  0.43269231,  3.5       ,  0.        ,\n",
       "         0.81730769,  0.56730769,  0.        ,  0.36538462,  0.00961538,\n",
       "         0.        ,  0.06730769,  0.2       ,  0.        ,  0.2       ,\n",
       "         0.4       ,  0.4       ,  0.4       ,  0.4       ,  0.2       ,\n",
       "         0.        ,  0.        ,  0.        ,  0.2       ,  0.6       ,\n",
       "         0.6       ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.3076923 ,\n",
       "         0.        ,  0.7865883 ]])"
      ]
     },
     "execution_count": 1152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtr[3:4,1:113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## ML PREPROCESSING (_\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "mtr = pd.read_csv(\"mtr1.csv\").values\n",
    "mtr = np.delete(mtr, 0, axis = 1) # because dataframe adds a rogue column\n",
    "#mtr.shape\n",
    "\n",
    "## PREPROCESSING\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "\n",
    "\n",
    "# replace missing values with mean of their corresponding features\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "data = imp.fit_transform(data)\n",
    "\n",
    "# If NaNs should be dropped instead:\n",
    "# mtr = mtr[~np.isnan(mtr).any(axis=1)]\n",
    "\n",
    "\n",
    "# normalize data (features now have gauss dist., 0 mean and unit variance)\n",
    "data = sklearn.preprocessing.scale(data)\n",
    "\n",
    "\n",
    "# THIS IS AN ALTERNATIVE TO 0 MEAN UNIT VARIANCE NORMALIZATION\n",
    "# do this to scale features to range 0-1\n",
    "# this must be done if a chi^2 is being performed\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# data = min_max_scaler.fit_transform(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DATA SPLIT (#nosnooping)\n",
    "\n",
    "# TEST DATA (%15 percent of data)\n",
    "numofppl_index = mtr.shape[0] - 1\n",
    "cut_index = int(mtr.shape[0] * 0.85)\n",
    "\n",
    "test_label = labels[cut_index:numofppl_index,:]\n",
    "test_data = data[cut_index:numofppl_index,:]\n",
    "\n",
    "# TRAINING AND VALIDATION DATA (%85 percent of data)\n",
    "\n",
    "train_label = labels[0:cut_index,:]\n",
    "train_data = data[0:cut_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 1693)"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features=None, max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [500], 'min_samples_leaf': [50, 51]},\n",
       "       pre_dispatch='2*n_jobs', refit=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 1119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################ RANDOM FOREST FEATURE SELECTION #############################\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "X = Xer(train_data, \"all\")\n",
    "y = Yer(train_label, 10).reshape(train_label.shape[0],)\n",
    "\n",
    "# rdc = RandomForestRegressor(max_depth=2, random_state=0)#,max_features=None)\n",
    "rdc = RandomForestRegressor(random_state=0, max_features=None)\n",
    "\n",
    "# clf.fit(X, y)\n",
    "\n",
    "n_estimators = list(range(500,501))\n",
    "max_features = [\"auto\",\"sqrt\",\"log2\",0.2,0.4,0.6,0.8,None]\n",
    "min_sample_leaf = list(range(50,52))\n",
    "# parameters = {'min_samples_leaf': min_sample_leaf}\n",
    "max_depth = list(range(50,52))\n",
    "# parameters = {'min_samples_leaf': min_sample_leaf, 'n_estimators':n_estimators}\n",
    "parameters = {'min_samples_leaf': min_sample_leaf, 'n_estimators':n_estimators}\n",
    "\n",
    "\n",
    "grid = GridSearchCV(rdc, parameters, cv=2, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutoff(train_label):\n",
    "\n",
    "    phqcutoffs = [10,15,20]\n",
    "\n",
    "    for j in range(0,len(phqcutoffs)):\n",
    "        for i in range(0,train_label[:,9:10].shape[0]):\n",
    "            if(train_label[i][9] > phqcutoffs[j]):\n",
    "                train_label[i][j] = 1\n",
    "            else:\n",
    "                train_label[i][j] = 0\n",
    "               \n",
    "    train_label_x = train_label\n",
    "        \n",
    "    return train_label_x               \n",
    "\n",
    "def YerCutOff(y, cutoff):\n",
    "    if (cutoff == 10):\n",
    "        return y[:,0:1]\n",
    "    if (cutoff == 15):\n",
    "        return y[:,1:2]\n",
    "    if (cutoff == 20):\n",
    "        return y[:,2:3]\n",
    "    \n",
    "train_label = cutoff(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1696)"
      ]
     },
     "execution_count": 957,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -45.66135, std: 0.68053, params: {'n_estimators': 500, 'min_samples_leaf': 50},\n",
       " mean: -45.66135, std: 0.68053, params: {'n_estimators': 500, 'min_samples_leaf': 51}]"
      ]
     },
     "execution_count": 1131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "grid.grid_scores_\n",
    "# grid.best_estimator_.get_params()\n",
    "# grid.best_score_\n",
    "# predictions = grid.predict(X)\n",
    "# print(classification_report(y, predictions))\n",
    "# print(confusion_matrix(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestRegressor' object has no attribute 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1132-47e5bd7e35ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrdc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestRegressor' object has no attribute 'transform'"
     ]
    }
   ],
   "source": [
    "rdc = RandomForestRegressor(max_depth=2, random_state=0, n_estimators=500, max_features=None, min_samples_leaf=50)\n",
    "rdc.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00209353 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00334313 0.01855466\n",
      " 0.0046633  0.         0.01       0.01157963 0.00741232 0.00932736\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00133495 0.         0.         0.         0.\n",
      " 0.00091267 0.028      0.         0.         0.00333141 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.002      0.00282377 0.         0.00081539 0.         0.\n",
      " 0.         0.00142762 0.00079931 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00078331 0.         0.         0.         0.         0.00081749\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00333382 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(rdc.feature_importances_[0:113])\n",
    "# print(rdc.feature_importances_[113:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 1269 (0.034217)\n",
      "2. feature 31 (0.028000)\n",
      "3. feature 954 (0.026477)\n",
      "4. feature 1072 (0.021559)\n",
      "5. feature 1466 (0.021270)\n",
      "6. feature 11 (0.018555)\n",
      "7. feature 1073 (0.018435)\n",
      "8. feature 403 (0.016764)\n",
      "9. feature 930 (0.013382)\n",
      "10. feature 556 (0.012997)\n",
      "11. feature 908 (0.012853)\n",
      "12. feature 638 (0.012519)\n",
      "13. feature 228 (0.012405)\n",
      "14. feature 1070 (0.012099)\n",
      "15. feature 1109 (0.011930)\n",
      "16. feature 15 (0.011580)\n",
      "17. feature 565 (0.010432)\n",
      "18. feature 1130 (0.010244)\n",
      "19. feature 256 (0.010129)\n",
      "20. feature 14 (0.010000)\n",
      "21. feature 17 (0.009327)\n",
      "22. feature 564 (0.009299)\n",
      "23. feature 436 (0.009287)\n",
      "24. feature 639 (0.008975)\n",
      "25. feature 867 (0.008485)\n",
      "26. feature 310 (0.008232)\n",
      "27. feature 315 (0.008000)\n",
      "28. feature 839 (0.008000)\n",
      "29. feature 365 (0.007972)\n",
      "30. feature 220 (0.007888)\n",
      "31. feature 1077 (0.007559)\n",
      "32. feature 460 (0.007552)\n",
      "33. feature 16 (0.007412)\n",
      "34. feature 432 (0.007215)\n",
      "35. feature 1119 (0.006808)\n",
      "36. feature 909 (0.006697)\n",
      "37. feature 514 (0.006354)\n",
      "38. feature 544 (0.006203)\n",
      "39. feature 449 (0.006091)\n",
      "40. feature 734 (0.006000)\n",
      "41. feature 1650 (0.005518)\n",
      "42. feature 1469 (0.005421)\n",
      "43. feature 394 (0.005180)\n",
      "44. feature 1121 (0.005180)\n",
      "45. feature 1671 (0.004867)\n",
      "46. feature 12 (0.004663)\n",
      "47. feature 420 (0.004463)\n",
      "48. feature 290 (0.004414)\n",
      "49. feature 1651 (0.004315)\n",
      "50. feature 928 (0.004184)\n",
      "51. feature 978 (0.004143)\n",
      "52. feature 1081 (0.004132)\n",
      "53. feature 373 (0.004072)\n",
      "54. feature 669 (0.004000)\n",
      "55. feature 1646 (0.004000)\n",
      "56. feature 711 (0.004000)\n",
      "57. feature 1572 (0.004000)\n",
      "58. feature 250 (0.003930)\n",
      "59. feature 825 (0.003890)\n",
      "60. feature 1575 (0.003860)\n",
      "61. feature 1417 (0.003758)\n",
      "62. feature 1286 (0.003710)\n",
      "63. feature 1068 (0.003673)\n",
      "64. feature 535 (0.003498)\n",
      "65. feature 1057 (0.003459)\n",
      "66. feature 1083 (0.003443)\n",
      "67. feature 415 (0.003408)\n",
      "68. feature 10 (0.003343)\n",
      "69. feature 91 (0.003334)\n",
      "70. feature 34 (0.003331)\n",
      "71. feature 1472 (0.003203)\n",
      "72. feature 619 (0.003190)\n",
      "73. feature 523 (0.003166)\n",
      "74. feature 1579 (0.003148)\n",
      "75. feature 1663 (0.003109)\n",
      "76. feature 1382 (0.003108)\n",
      "77. feature 1691 (0.003076)\n",
      "78. feature 1618 (0.003067)\n",
      "79. feature 369 (0.002980)\n",
      "80. feature 205 (0.002925)\n",
      "81. feature 143 (0.002922)\n",
      "82. feature 1599 (0.002896)\n",
      "83. feature 463 (0.002857)\n",
      "84. feature 55 (0.002824)\n",
      "85. feature 1455 (0.002810)\n",
      "86. feature 1369 (0.002808)\n",
      "87. feature 156 (0.002807)\n",
      "88. feature 1004 (0.002764)\n",
      "89. feature 794 (0.002731)\n",
      "90. feature 1035 (0.002721)\n",
      "91. feature 407 (0.002702)\n",
      "92. feature 1332 (0.002680)\n",
      "93. feature 1371 (0.002666)\n",
      "94. feature 341 (0.002618)\n",
      "95. feature 918 (0.002591)\n",
      "96. feature 1522 (0.002576)\n",
      "97. feature 276 (0.002575)\n",
      "98. feature 524 (0.002563)\n",
      "99. feature 321 (0.002546)\n",
      "100. feature 1352 (0.002541)\n",
      "101. feature 1649 (0.002534)\n",
      "102. feature 268 (0.002523)\n",
      "103. feature 459 (0.002477)\n",
      "104. feature 286 (0.002453)\n",
      "105. feature 728 (0.002338)\n",
      "106. feature 1450 (0.002318)\n",
      "107. feature 1685 (0.002300)\n",
      "108. feature 1617 (0.002280)\n",
      "109. feature 339 (0.002274)\n",
      "110. feature 1201 (0.002268)\n",
      "111. feature 878 (0.002254)\n",
      "112. feature 1046 (0.002220)\n",
      "113. feature 1656 (0.002205)\n",
      "114. feature 0 (0.002094)\n",
      "115. feature 130 (0.002093)\n",
      "116. feature 1597 (0.002087)\n",
      "117. feature 296 (0.002005)\n",
      "118. feature 1088 (0.002000)\n",
      "119. feature 124 (0.002000)\n",
      "120. feature 243 (0.002000)\n",
      "121. feature 725 (0.002000)\n",
      "122. feature 697 (0.002000)\n",
      "123. feature 1021 (0.002000)\n",
      "124. feature 594 (0.002000)\n",
      "125. feature 126 (0.002000)\n",
      "126. feature 1356 (0.002000)\n",
      "127. feature 550 (0.002000)\n",
      "128. feature 1542 (0.002000)\n",
      "129. feature 479 (0.002000)\n",
      "130. feature 953 (0.002000)\n",
      "131. feature 1367 (0.002000)\n",
      "132. feature 230 (0.002000)\n",
      "133. feature 229 (0.002000)\n",
      "134. feature 1414 (0.002000)\n",
      "135. feature 1416 (0.002000)\n",
      "136. feature 54 (0.002000)\n",
      "137. feature 132 (0.002000)\n",
      "138. feature 508 (0.002000)\n",
      "139. feature 730 (0.002000)\n",
      "140. feature 1038 (0.002000)\n",
      "141. feature 1039 (0.002000)\n",
      "142. feature 533 (0.002000)\n",
      "143. feature 733 (0.002000)\n",
      "144. feature 1471 (0.002000)\n",
      "145. feature 171 (0.002000)\n",
      "146. feature 841 (0.002000)\n",
      "147. feature 289 (0.002000)\n",
      "148. feature 632 (0.002000)\n",
      "149. feature 342 (0.002000)\n",
      "150. feature 919 (0.002000)\n",
      "151. feature 382 (0.002000)\n",
      "152. feature 842 (0.002000)\n",
      "153. feature 1563 (0.002000)\n",
      "154. feature 1150 (0.002000)\n",
      "155. feature 1576 (0.002000)\n",
      "156. feature 440 (0.002000)\n",
      "157. feature 348 (0.002000)\n",
      "158. feature 1276 (0.002000)\n",
      "159. feature 592 (0.002000)\n",
      "160. feature 988 (0.002000)\n",
      "161. feature 994 (0.002000)\n",
      "162. feature 442 (0.002000)\n",
      "163. feature 658 (0.001978)\n",
      "164. feature 543 (0.001966)\n",
      "165. feature 652 (0.001928)\n",
      "166. feature 621 (0.001927)\n",
      "167. feature 1392 (0.001918)\n",
      "168. feature 481 (0.001867)\n",
      "169. feature 421 (0.001767)\n",
      "170. feature 630 (0.001756)\n",
      "171. feature 437 (0.001716)\n",
      "172. feature 995 (0.001698)\n",
      "173. feature 302 (0.001691)\n",
      "174. feature 1152 (0.001633)\n",
      "175. feature 729 (0.001616)\n",
      "176. feature 409 (0.001590)\n",
      "177. feature 349 (0.001585)\n",
      "178. feature 1682 (0.001551)\n",
      "179. feature 753 (0.001546)\n",
      "180. feature 893 (0.001543)\n",
      "181. feature 375 (0.001537)\n",
      "182. feature 1120 (0.001515)\n",
      "183. feature 947 (0.001505)\n",
      "184. feature 607 (0.001480)\n",
      "185. feature 1655 (0.001464)\n",
      "186. feature 1164 (0.001459)\n",
      "187. feature 384 (0.001429)\n",
      "188. feature 61 (0.001428)\n",
      "189. feature 328 (0.001383)\n",
      "190. feature 1571 (0.001378)\n",
      "191. feature 703 (0.001371)\n",
      "192. feature 785 (0.001363)\n",
      "193. feature 876 (0.001352)\n",
      "194. feature 25 (0.001335)\n",
      "195. feature 941 (0.001311)\n",
      "196. feature 934 (0.001297)\n",
      "197. feature 1670 (0.001292)\n",
      "198. feature 1645 (0.001290)\n",
      "199. feature 868 (0.001269)\n",
      "200. feature 1233 (0.001266)\n",
      "201. feature 744 (0.001260)\n",
      "202. feature 1139 (0.001251)\n",
      "203. feature 1583 (0.001219)\n",
      "204. feature 501 (0.001203)\n",
      "205. feature 1540 (0.001201)\n",
      "206. feature 117 (0.001185)\n",
      "207. feature 1185 (0.001183)\n",
      "208. feature 1200 (0.001153)\n",
      "209. feature 144 (0.001141)\n",
      "210. feature 1397 (0.001131)\n",
      "211. feature 431 (0.001128)\n",
      "212. feature 217 (0.001122)\n",
      "213. feature 712 (0.001106)\n",
      "214. feature 568 (0.001104)\n",
      "215. feature 1637 (0.001104)\n",
      "216. feature 1411 (0.001094)\n",
      "217. feature 723 (0.001094)\n",
      "218. feature 1561 (0.001089)\n",
      "219. feature 996 (0.001087)\n",
      "220. feature 1110 (0.001085)\n",
      "221. feature 1551 (0.001080)\n",
      "222. feature 1365 (0.001073)\n",
      "223. feature 1667 (0.001064)\n",
      "224. feature 1023 (0.001061)\n",
      "225. feature 596 (0.001043)\n",
      "226. feature 1604 (0.001040)\n",
      "227. feature 940 (0.001030)\n",
      "228. feature 634 (0.001024)\n",
      "229. feature 468 (0.001020)\n",
      "230. feature 554 (0.001019)\n",
      "231. feature 977 (0.001016)\n",
      "232. feature 581 (0.001011)\n",
      "233. feature 154 (0.001010)\n",
      "234. feature 1361 (0.001005)\n",
      "235. feature 1155 (0.001001)\n",
      "236. feature 1157 (0.000997)\n",
      "237. feature 1487 (0.000989)\n",
      "238. feature 626 (0.000987)\n",
      "239. feature 1526 (0.000981)\n",
      "240. feature 1212 (0.000976)\n",
      "241. feature 1627 (0.000976)\n",
      "242. feature 812 (0.000970)\n",
      "243. feature 1221 (0.000960)\n",
      "244. feature 633 (0.000960)\n",
      "245. feature 487 (0.000956)\n",
      "246. feature 423 (0.000955)\n",
      "247. feature 662 (0.000939)\n",
      "248. feature 1669 (0.000936)\n",
      "249. feature 257 (0.000935)\n",
      "250. feature 452 (0.000933)\n",
      "251. feature 123 (0.000933)\n",
      "252. feature 418 (0.000930)\n",
      "253. feature 836 (0.000922)\n",
      "254. feature 1210 (0.000921)\n",
      "255. feature 1106 (0.000920)\n",
      "256. feature 30 (0.000913)\n",
      "257. feature 856 (0.000906)\n",
      "258. feature 585 (0.000906)\n",
      "259. feature 1457 (0.000905)\n",
      "260. feature 1179 (0.000901)\n",
      "261. feature 362 (0.000896)\n",
      "262. feature 1170 (0.000892)\n",
      "263. feature 927 (0.000885)\n",
      "264. feature 1336 (0.000883)\n",
      "265. feature 300 (0.000881)\n",
      "266. feature 1465 (0.000878)\n",
      "267. feature 1681 (0.000874)\n",
      "268. feature 318 (0.000872)\n",
      "269. feature 316 (0.000872)\n",
      "270. feature 261 (0.000869)\n",
      "271. feature 1112 (0.000867)\n",
      "272. feature 278 (0.000859)\n",
      "273. feature 136 (0.000859)\n",
      "274. feature 1402 (0.000855)\n",
      "275. feature 1265 (0.000852)\n",
      "276. feature 248 (0.000848)\n",
      "277. feature 1018 (0.000841)\n",
      "278. feature 303 (0.000836)\n",
      "279. feature 327 (0.000834)\n",
      "280. feature 235 (0.000823)\n",
      "281. feature 378 (0.000818)\n",
      "282. feature 77 (0.000817)\n",
      "283. feature 1011 (0.000816)\n",
      "284. feature 57 (0.000815)\n",
      "285. feature 910 (0.000815)\n",
      "286. feature 826 (0.000815)\n",
      "287. feature 267 (0.000811)\n",
      "288. feature 1105 (0.000809)\n",
      "289. feature 1534 (0.000804)\n",
      "290. feature 62 (0.000799)\n",
      "291. feature 852 (0.000791)\n",
      "292. feature 1394 (0.000789)\n",
      "293. feature 1603 (0.000788)\n",
      "294. feature 72 (0.000783)\n",
      "295. feature 1331 (0.000781)\n",
      "296. feature 294 (0.000775)\n",
      "297. feature 263 (0.000767)\n",
      "298. feature 367 (0.000764)\n",
      "299. feature 408 (0.000752)\n",
      "300. feature 381 (0.000749)\n",
      "301. feature 1115 (0.000747)\n",
      "302. feature 765 (0.000745)\n",
      "303. feature 1537 (0.000742)\n",
      "304. feature 1327 (0.000734)\n",
      "305. feature 259 (0.000734)\n",
      "306. feature 1454 (0.000731)\n",
      "307. feature 262 (0.000731)\n",
      "308. feature 1203 (0.000730)\n",
      "309. feature 1376 (0.000723)\n",
      "310. feature 343 (0.000722)\n",
      "311. feature 1482 (0.000717)\n",
      "312. feature 1043 (0.000710)\n",
      "313. feature 385 (0.000703)\n",
      "314. feature 835 (0.000689)\n",
      "315. feature 577 (0.000688)\n",
      "316. feature 448 (0.000686)\n",
      "317. feature 410 (0.000686)\n",
      "318. feature 1440 (0.000685)\n",
      "319. feature 696 (0.000680)\n",
      "320. feature 859 (0.000676)\n",
      "321. feature 1113 (0.000670)\n",
      "322. feature 113 (0.000666)\n",
      "323. feature 1660 (0.000659)\n",
      "324. feature 1165 (0.000657)\n",
      "325. feature 1554 (0.000649)\n",
      "326. feature 608 (0.000648)\n",
      "327. feature 196 (0.000648)\n",
      "328. feature 668 (0.000645)\n",
      "329. feature 1386 (0.000640)\n",
      "330. feature 672 (0.000639)\n",
      "331. feature 600 (0.000617)\n",
      "332. feature 216 (0.000605)\n",
      "333. feature 1236 (0.000601)\n",
      "334. feature 470 (0.000581)\n",
      "335. feature 623 (0.000580)\n",
      "336. feature 916 (0.000579)\n",
      "337. feature 965 (0.000569)\n",
      "338. feature 587 (0.000563)\n",
      "339. feature 417 (0.000562)\n",
      "340. feature 395 (0.000538)\n",
      "341. feature 872 (0.000536)\n",
      "342. feature 657 (0.000494)\n",
      "343. feature 370 (0.000493)\n",
      "344. feature 169 (0.000489)\n",
      "345. feature 237 (0.000485)\n",
      "346. feature 1101 (0.000468)\n",
      "347. feature 502 (0.000000)\n",
      "348. feature 517 (0.000000)\n",
      "349. feature 67 (0.000000)\n",
      "350. feature 573 (0.000000)\n",
      "351. feature 574 (0.000000)\n",
      "352. feature 518 (0.000000)\n",
      "353. feature 519 (0.000000)\n",
      "354. feature 572 (0.000000)\n",
      "355. feature 571 (0.000000)\n",
      "356. feature 520 (0.000000)\n",
      "357. feature 570 (0.000000)\n",
      "358. feature 521 (0.000000)\n",
      "359. feature 516 (0.000000)\n",
      "360. feature 513 (0.000000)\n",
      "361. feature 42 (0.000000)\n",
      "362. feature 512 (0.000000)\n",
      "363. feature 499 (0.000000)\n",
      "364. feature 40 (0.000000)\n",
      "365. feature 64 (0.000000)\n",
      "366. feature 591 (0.000000)\n",
      "367. feature 590 (0.000000)\n",
      "368. feature 589 (0.000000)\n",
      "369. feature 588 (0.000000)\n",
      "370. feature 586 (0.000000)\n",
      "371. feature 510 (0.000000)\n",
      "372. feature 584 (0.000000)\n",
      "373. feature 511 (0.000000)\n",
      "374. feature 41 (0.000000)\n",
      "375. feature 515 (0.000000)\n",
      "376. feature 583 (0.000000)\n",
      "377. feature 582 (0.000000)\n",
      "378. feature 522 (0.000000)\n",
      "379. feature 500 (0.000000)\n",
      "380. feature 63 (0.000000)\n",
      "381. feature 580 (0.000000)\n",
      "382. feature 579 (0.000000)\n",
      "383. feature 578 (0.000000)\n",
      "384. feature 576 (0.000000)\n",
      "385. feature 69 (0.000000)\n",
      "386. feature 575 (0.000000)\n",
      "387. feature 43 (0.000000)\n",
      "388. feature 45 (0.000000)\n",
      "389. feature 44 (0.000000)\n",
      "390. feature 549 (0.000000)\n",
      "391. feature 58 (0.000000)\n",
      "392. feature 534 (0.000000)\n",
      "393. feature 506 (0.000000)\n",
      "394. feature 553 (0.000000)\n",
      "395. feature 552 (0.000000)\n",
      "396. feature 509 (0.000000)\n",
      "397. feature 56 (0.000000)\n",
      "398. feature 551 (0.000000)\n",
      "399. feature 53 (0.000000)\n",
      "400. feature 536 (0.000000)\n",
      "401. feature 52 (0.000000)\n",
      "402. feature 537 (0.000000)\n",
      "403. feature 507 (0.000000)\n",
      "404. feature 65 (0.000000)\n",
      "405. feature 538 (0.000000)\n",
      "406. feature 49 (0.000000)\n",
      "407. feature 548 (0.000000)\n",
      "408. feature 539 (0.000000)\n",
      "409. feature 540 (0.000000)\n",
      "410. feature 541 (0.000000)\n",
      "411. feature 547 (0.000000)\n",
      "412. feature 546 (0.000000)\n",
      "413. feature 542 (0.000000)\n",
      "414. feature 545 (0.000000)\n",
      "415. feature 50 (0.000000)\n",
      "416. feature 51 (0.000000)\n",
      "417. feature 532 (0.000000)\n",
      "418. feature 505 (0.000000)\n",
      "419. feature 555 (0.000000)\n",
      "420. feature 48 (0.000000)\n",
      "421. feature 60 (0.000000)\n",
      "422. feature 525 (0.000000)\n",
      "423. feature 569 (0.000000)\n",
      "424. feature 66 (0.000000)\n",
      "425. feature 526 (0.000000)\n",
      "426. feature 527 (0.000000)\n",
      "427. feature 528 (0.000000)\n",
      "428. feature 503 (0.000000)\n",
      "429. feature 567 (0.000000)\n",
      "430. feature 566 (0.000000)\n",
      "431. feature 68 (0.000000)\n",
      "432. feature 46 (0.000000)\n",
      "433. feature 563 (0.000000)\n",
      "434. feature 504 (0.000000)\n",
      "435. feature 529 (0.000000)\n",
      "436. feature 59 (0.000000)\n",
      "437. feature 47 (0.000000)\n",
      "438. feature 562 (0.000000)\n",
      "439. feature 561 (0.000000)\n",
      "440. feature 560 (0.000000)\n",
      "441. feature 559 (0.000000)\n",
      "442. feature 558 (0.000000)\n",
      "443. feature 530 (0.000000)\n",
      "444. feature 531 (0.000000)\n",
      "445. feature 557 (0.000000)\n",
      "446. feature 145 (0.000000)\n",
      "447. feature 625 (0.000000)\n",
      "448. feature 593 (0.000000)\n",
      "449. feature 679 (0.000000)\n",
      "450. feature 701 (0.000000)\n",
      "451. feature 702 (0.000000)\n",
      "452. feature 704 (0.000000)\n",
      "453. feature 705 (0.000000)\n",
      "454. feature 706 (0.000000)\n",
      "455. feature 707 (0.000000)\n",
      "456. feature 708 (0.000000)\n",
      "457. feature 709 (0.000000)\n",
      "458. feature 710 (0.000000)\n",
      "459. feature 9 (0.000000)\n",
      "460. feature 713 (0.000000)\n",
      "461. feature 714 (0.000000)\n",
      "462. feature 715 (0.000000)\n",
      "463. feature 716 (0.000000)\n",
      "464. feature 717 (0.000000)\n",
      "465. feature 718 (0.000000)\n",
      "466. feature 719 (0.000000)\n",
      "467. feature 700 (0.000000)\n",
      "468. feature 699 (0.000000)\n",
      "469. feature 698 (0.000000)\n",
      "470. feature 688 (0.000000)\n",
      "471. feature 681 (0.000000)\n",
      "472. feature 682 (0.000000)\n",
      "473. feature 683 (0.000000)\n",
      "474. feature 684 (0.000000)\n",
      "475. feature 685 (0.000000)\n",
      "476. feature 686 (0.000000)\n",
      "477. feature 687 (0.000000)\n",
      "478. feature 689 (0.000000)\n",
      "479. feature 13 (0.000000)\n",
      "480. feature 690 (0.000000)\n",
      "481. feature 691 (0.000000)\n",
      "482. feature 692 (0.000000)\n",
      "483. feature 693 (0.000000)\n",
      "484. feature 18 (0.000000)\n",
      "485. feature 694 (0.000000)\n",
      "486. feature 695 (0.000000)\n",
      "487. feature 720 (0.000000)\n",
      "488. feature 721 (0.000000)\n",
      "489. feature 722 (0.000000)\n",
      "490. feature 752 (0.000000)\n",
      "491. feature 745 (0.000000)\n",
      "492. feature 746 (0.000000)\n",
      "493. feature 747 (0.000000)\n",
      "494. feature 748 (0.000000)\n",
      "495. feature 749 (0.000000)\n",
      "496. feature 750 (0.000000)\n",
      "497. feature 751 (0.000000)\n",
      "498. feature 1 (0.000000)\n",
      "499. feature 742 (0.000000)\n",
      "500. feature 754 (0.000000)\n",
      "501. feature 755 (0.000000)\n",
      "502. feature 756 (0.000000)\n",
      "503. feature 757 (0.000000)\n",
      "504. feature 758 (0.000000)\n",
      "505. feature 759 (0.000000)\n",
      "506. feature 760 (0.000000)\n",
      "507. feature 743 (0.000000)\n",
      "508. feature 741 (0.000000)\n",
      "509. feature 724 (0.000000)\n",
      "510. feature 732 (0.000000)\n",
      "511. feature 8 (0.000000)\n",
      "512. feature 726 (0.000000)\n",
      "513. feature 727 (0.000000)\n",
      "514. feature 7 (0.000000)\n",
      "515. feature 6 (0.000000)\n",
      "516. feature 5 (0.000000)\n",
      "517. feature 731 (0.000000)\n",
      "518. feature 4 (0.000000)\n",
      "519. feature 740 (0.000000)\n",
      "520. feature 3 (0.000000)\n",
      "521. feature 2 (0.000000)\n",
      "522. feature 735 (0.000000)\n",
      "523. feature 736 (0.000000)\n",
      "524. feature 737 (0.000000)\n",
      "525. feature 738 (0.000000)\n",
      "526. feature 739 (0.000000)\n",
      "527. feature 680 (0.000000)\n",
      "528. feature 678 (0.000000)\n",
      "529. feature 595 (0.000000)\n",
      "530. feature 677 (0.000000)\n",
      "531. feature 616 (0.000000)\n",
      "532. feature 617 (0.000000)\n",
      "533. feature 618 (0.000000)\n",
      "534. feature 35 (0.000000)\n",
      "535. feature 620 (0.000000)\n",
      "536. feature 33 (0.000000)\n",
      "537. feature 622 (0.000000)\n",
      "538. feature 32 (0.000000)\n",
      "539. feature 624 (0.000000)\n",
      "540. feature 497 (0.000000)\n",
      "541. feature 627 (0.000000)\n",
      "542. feature 628 (0.000000)\n",
      "543. feature 629 (0.000000)\n",
      "544. feature 29 (0.000000)\n",
      "545. feature 28 (0.000000)\n",
      "546. feature 631 (0.000000)\n",
      "547. feature 27 (0.000000)\n",
      "548. feature 615 (0.000000)\n",
      "549. feature 614 (0.000000)\n",
      "550. feature 613 (0.000000)\n",
      "551. feature 39 (0.000000)\n",
      "552. feature 597 (0.000000)\n",
      "553. feature 598 (0.000000)\n",
      "554. feature 599 (0.000000)\n",
      "555. feature 601 (0.000000)\n",
      "556. feature 602 (0.000000)\n",
      "557. feature 603 (0.000000)\n",
      "558. feature 604 (0.000000)\n",
      "559. feature 38 (0.000000)\n",
      "560. feature 612 (0.000000)\n",
      "561. feature 605 (0.000000)\n",
      "562. feature 606 (0.000000)\n",
      "563. feature 37 (0.000000)\n",
      "564. feature 609 (0.000000)\n",
      "565. feature 36 (0.000000)\n",
      "566. feature 610 (0.000000)\n",
      "567. feature 611 (0.000000)\n",
      "568. feature 635 (0.000000)\n",
      "569. feature 636 (0.000000)\n",
      "570. feature 26 (0.000000)\n",
      "571. feature 666 (0.000000)\n",
      "572. feature 659 (0.000000)\n",
      "573. feature 660 (0.000000)\n",
      "574. feature 661 (0.000000)\n",
      "575. feature 663 (0.000000)\n",
      "576. feature 664 (0.000000)\n",
      "577. feature 20 (0.000000)\n",
      "578. feature 665 (0.000000)\n",
      "579. feature 667 (0.000000)\n",
      "580. feature 656 (0.000000)\n",
      "581. feature 670 (0.000000)\n",
      "582. feature 671 (0.000000)\n",
      "583. feature 19 (0.000000)\n",
      "584. feature 673 (0.000000)\n",
      "585. feature 674 (0.000000)\n",
      "586. feature 675 (0.000000)\n",
      "587. feature 676 (0.000000)\n",
      "588. feature 21 (0.000000)\n",
      "589. feature 655 (0.000000)\n",
      "590. feature 637 (0.000000)\n",
      "591. feature 646 (0.000000)\n",
      "592. feature 24 (0.000000)\n",
      "593. feature 640 (0.000000)\n",
      "594. feature 641 (0.000000)\n",
      "595. feature 642 (0.000000)\n",
      "596. feature 643 (0.000000)\n",
      "597. feature 644 (0.000000)\n",
      "598. feature 645 (0.000000)\n",
      "599. feature 647 (0.000000)\n",
      "600. feature 654 (0.000000)\n",
      "601. feature 648 (0.000000)\n",
      "602. feature 649 (0.000000)\n",
      "603. feature 650 (0.000000)\n",
      "604. feature 23 (0.000000)\n",
      "605. feature 651 (0.000000)\n",
      "606. feature 22 (0.000000)\n",
      "607. feature 653 (0.000000)\n",
      "608. feature 498 (0.000000)\n",
      "609. feature 462 (0.000000)\n",
      "610. feature 70 (0.000000)\n",
      "611. feature 272 (0.000000)\n",
      "612. feature 122 (0.000000)\n",
      "613. feature 251 (0.000000)\n",
      "614. feature 252 (0.000000)\n",
      "615. feature 253 (0.000000)\n",
      "616. feature 254 (0.000000)\n",
      "617. feature 255 (0.000000)\n",
      "618. feature 121 (0.000000)\n",
      "619. feature 258 (0.000000)\n",
      "620. feature 120 (0.000000)\n",
      "621. feature 260 (0.000000)\n",
      "622. feature 119 (0.000000)\n",
      "623. feature 264 (0.000000)\n",
      "624. feature 265 (0.000000)\n",
      "625. feature 266 (0.000000)\n",
      "626. feature 118 (0.000000)\n",
      "627. feature 269 (0.000000)\n",
      "628. feature 270 (0.000000)\n",
      "629. feature 249 (0.000000)\n",
      "630. feature 247 (0.000000)\n",
      "631. feature 246 (0.000000)\n",
      "632. feature 234 (0.000000)\n",
      "633. feature 226 (0.000000)\n",
      "634. feature 227 (0.000000)\n",
      "635. feature 128 (0.000000)\n",
      "636. feature 127 (0.000000)\n",
      "637. feature 231 (0.000000)\n",
      "638. feature 232 (0.000000)\n",
      "639. feature 233 (0.000000)\n",
      "640. feature 236 (0.000000)\n",
      "641. feature 245 (0.000000)\n",
      "642. feature 238 (0.000000)\n",
      "643. feature 239 (0.000000)\n",
      "644. feature 240 (0.000000)\n",
      "645. feature 241 (0.000000)\n",
      "646. feature 125 (0.000000)\n",
      "647. feature 242 (0.000000)\n",
      "648. feature 244 (0.000000)\n",
      "649. feature 271 (0.000000)\n",
      "650. feature 273 (0.000000)\n",
      "651. feature 496 (0.000000)\n",
      "652. feature 274 (0.000000)\n",
      "653. feature 297 (0.000000)\n",
      "654. feature 298 (0.000000)\n",
      "655. feature 299 (0.000000)\n",
      "656. feature 301 (0.000000)\n",
      "657. feature 110 (0.000000)\n",
      "658. feature 304 (0.000000)\n",
      "659. feature 305 (0.000000)\n",
      "660. feature 306 (0.000000)\n",
      "661. feature 307 (0.000000)\n",
      "662. feature 308 (0.000000)\n",
      "663. feature 309 (0.000000)\n",
      "664. feature 109 (0.000000)\n",
      "665. feature 311 (0.000000)\n",
      "666. feature 312 (0.000000)\n",
      "667. feature 313 (0.000000)\n",
      "668. feature 314 (0.000000)\n",
      "669. feature 317 (0.000000)\n",
      "670. feature 111 (0.000000)\n",
      "671. feature 295 (0.000000)\n",
      "672. feature 293 (0.000000)\n",
      "673. feature 283 (0.000000)\n",
      "674. feature 275 (0.000000)\n",
      "675. feature 116 (0.000000)\n",
      "676. feature 277 (0.000000)\n",
      "677. feature 279 (0.000000)\n",
      "678. feature 280 (0.000000)\n",
      "679. feature 281 (0.000000)\n",
      "680. feature 282 (0.000000)\n",
      "681. feature 284 (0.000000)\n",
      "682. feature 292 (0.000000)\n",
      "683. feature 285 (0.000000)\n",
      "684. feature 115 (0.000000)\n",
      "685. feature 287 (0.000000)\n",
      "686. feature 288 (0.000000)\n",
      "687. feature 114 (0.000000)\n",
      "688. feature 112 (0.000000)\n",
      "689. feature 291 (0.000000)\n",
      "690. feature 225 (0.000000)\n",
      "691. feature 129 (0.000000)\n",
      "692. feature 224 (0.000000)\n",
      "693. feature 131 (0.000000)\n",
      "694. feature 170 (0.000000)\n",
      "695. feature 142 (0.000000)\n",
      "696. feature 172 (0.000000)\n",
      "697. feature 173 (0.000000)\n",
      "698. feature 174 (0.000000)\n",
      "699. feature 175 (0.000000)\n",
      "700. feature 176 (0.000000)\n",
      "701. feature 177 (0.000000)\n",
      "702. feature 178 (0.000000)\n",
      "703. feature 141 (0.000000)\n",
      "704. feature 140 (0.000000)\n",
      "705. feature 179 (0.000000)\n",
      "706. feature 139 (0.000000)\n",
      "707. feature 180 (0.000000)\n",
      "708. feature 181 (0.000000)\n",
      "709. feature 138 (0.000000)\n",
      "710. feature 182 (0.000000)\n",
      "711. feature 168 (0.000000)\n",
      "712. feature 167 (0.000000)\n",
      "713. feature 166 (0.000000)\n",
      "714. feature 155 (0.000000)\n",
      "715. feature 147 (0.000000)\n",
      "716. feature 148 (0.000000)\n",
      "717. feature 149 (0.000000)\n",
      "718. feature 150 (0.000000)\n",
      "719. feature 151 (0.000000)\n",
      "720. feature 152 (0.000000)\n",
      "721. feature 153 (0.000000)\n",
      "722. feature 157 (0.000000)\n",
      "723. feature 165 (0.000000)\n",
      "724. feature 158 (0.000000)\n",
      "725. feature 159 (0.000000)\n",
      "726. feature 160 (0.000000)\n",
      "727. feature 161 (0.000000)\n",
      "728. feature 162 (0.000000)\n",
      "729. feature 163 (0.000000)\n",
      "730. feature 164 (0.000000)\n",
      "731. feature 183 (0.000000)\n",
      "732. feature 184 (0.000000)\n",
      "733. feature 185 (0.000000)\n",
      "734. feature 214 (0.000000)\n",
      "735. feature 207 (0.000000)\n",
      "736. feature 208 (0.000000)\n",
      "737. feature 209 (0.000000)\n",
      "738. feature 210 (0.000000)\n",
      "739. feature 211 (0.000000)\n",
      "740. feature 212 (0.000000)\n",
      "741. feature 213 (0.000000)\n",
      "742. feature 215 (0.000000)\n",
      "743. feature 135 (0.000000)\n",
      "744. feature 218 (0.000000)\n",
      "745. feature 219 (0.000000)\n",
      "746. feature 134 (0.000000)\n",
      "747. feature 221 (0.000000)\n",
      "748. feature 222 (0.000000)\n",
      "749. feature 223 (0.000000)\n",
      "750. feature 133 (0.000000)\n",
      "751. feature 206 (0.000000)\n",
      "752. feature 204 (0.000000)\n",
      "753. feature 186 (0.000000)\n",
      "754. feature 193 (0.000000)\n",
      "755. feature 187 (0.000000)\n",
      "756. feature 188 (0.000000)\n",
      "757. feature 189 (0.000000)\n",
      "758. feature 137 (0.000000)\n",
      "759. feature 190 (0.000000)\n",
      "760. feature 191 (0.000000)\n",
      "761. feature 192 (0.000000)\n",
      "762. feature 194 (0.000000)\n",
      "763. feature 203 (0.000000)\n",
      "764. feature 195 (0.000000)\n",
      "765. feature 197 (0.000000)\n",
      "766. feature 198 (0.000000)\n",
      "767. feature 199 (0.000000)\n",
      "768. feature 200 (0.000000)\n",
      "769. feature 201 (0.000000)\n",
      "770. feature 202 (0.000000)\n",
      "771. feature 319 (0.000000)\n",
      "772. feature 320 (0.000000)\n",
      "773. feature 108 (0.000000)\n",
      "774. feature 413 (0.000000)\n",
      "775. feature 439 (0.000000)\n",
      "776. feature 85 (0.000000)\n",
      "777. feature 441 (0.000000)\n",
      "778. feature 84 (0.000000)\n",
      "779. feature 443 (0.000000)\n",
      "780. feature 444 (0.000000)\n",
      "781. feature 445 (0.000000)\n",
      "782. feature 446 (0.000000)\n",
      "783. feature 447 (0.000000)\n",
      "784. feature 83 (0.000000)\n",
      "785. feature 82 (0.000000)\n",
      "786. feature 81 (0.000000)\n",
      "787. feature 450 (0.000000)\n",
      "788. feature 451 (0.000000)\n",
      "789. feature 453 (0.000000)\n",
      "790. feature 454 (0.000000)\n",
      "791. feature 455 (0.000000)\n",
      "792. feature 438 (0.000000)\n",
      "793. feature 86 (0.000000)\n",
      "794. feature 87 (0.000000)\n",
      "795. feature 425 (0.000000)\n",
      "796. feature 92 (0.000000)\n",
      "797. feature 416 (0.000000)\n",
      "798. feature 419 (0.000000)\n",
      "799. feature 90 (0.000000)\n",
      "800. feature 89 (0.000000)\n",
      "801. feature 422 (0.000000)\n",
      "802. feature 424 (0.000000)\n",
      "803. feature 426 (0.000000)\n",
      "804. feature 435 (0.000000)\n",
      "805. feature 427 (0.000000)\n",
      "806. feature 428 (0.000000)\n",
      "807. feature 429 (0.000000)\n",
      "808. feature 430 (0.000000)\n",
      "809. feature 88 (0.000000)\n",
      "810. feature 433 (0.000000)\n",
      "811. feature 434 (0.000000)\n",
      "812. feature 456 (0.000000)\n",
      "813. feature 457 (0.000000)\n",
      "814. feature 458 (0.000000)\n",
      "815. feature 486 (0.000000)\n",
      "816. feature 73 (0.000000)\n",
      "817. feature 480 (0.000000)\n",
      "818. feature 71 (0.000000)\n",
      "819. feature 482 (0.000000)\n",
      "820. feature 483 (0.000000)\n",
      "821. feature 484 (0.000000)\n",
      "822. feature 485 (0.000000)\n",
      "823. feature 488 (0.000000)\n",
      "824. feature 477 (0.000000)\n",
      "825. feature 489 (0.000000)\n",
      "826. feature 490 (0.000000)\n",
      "827. feature 491 (0.000000)\n",
      "828. feature 492 (0.000000)\n",
      "829. feature 493 (0.000000)\n",
      "830. feature 494 (0.000000)\n",
      "831. feature 495 (0.000000)\n",
      "832. feature 478 (0.000000)\n",
      "833. feature 476 (0.000000)\n",
      "834. feature 80 (0.000000)\n",
      "835. feature 467 (0.000000)\n",
      "836. feature 79 (0.000000)\n",
      "837. feature 461 (0.000000)\n",
      "838. feature 146 (0.000000)\n",
      "839. feature 78 (0.000000)\n",
      "840. feature 464 (0.000000)\n",
      "841. feature 465 (0.000000)\n",
      "842. feature 466 (0.000000)\n",
      "843. feature 469 (0.000000)\n",
      "844. feature 475 (0.000000)\n",
      "845. feature 471 (0.000000)\n",
      "846. feature 472 (0.000000)\n",
      "847. feature 473 (0.000000)\n",
      "848. feature 474 (0.000000)\n",
      "849. feature 76 (0.000000)\n",
      "850. feature 75 (0.000000)\n",
      "851. feature 74 (0.000000)\n",
      "852. feature 414 (0.000000)\n",
      "853. feature 412 (0.000000)\n",
      "854. feature 322 (0.000000)\n",
      "855. feature 411 (0.000000)\n",
      "856. feature 345 (0.000000)\n",
      "857. feature 346 (0.000000)\n",
      "858. feature 347 (0.000000)\n",
      "859. feature 103 (0.000000)\n",
      "860. feature 102 (0.000000)\n",
      "861. feature 101 (0.000000)\n",
      "862. feature 350 (0.000000)\n",
      "863. feature 351 (0.000000)\n",
      "864. feature 352 (0.000000)\n",
      "865. feature 353 (0.000000)\n",
      "866. feature 354 (0.000000)\n",
      "867. feature 355 (0.000000)\n",
      "868. feature 356 (0.000000)\n",
      "869. feature 357 (0.000000)\n",
      "870. feature 358 (0.000000)\n",
      "871. feature 359 (0.000000)\n",
      "872. feature 360 (0.000000)\n",
      "873. feature 344 (0.000000)\n",
      "874. feature 104 (0.000000)\n",
      "875. feature 105 (0.000000)\n",
      "876. feature 332 (0.000000)\n",
      "877. feature 323 (0.000000)\n",
      "878. feature 324 (0.000000)\n",
      "879. feature 325 (0.000000)\n",
      "880. feature 326 (0.000000)\n",
      "881. feature 329 (0.000000)\n",
      "882. feature 330 (0.000000)\n",
      "883. feature 331 (0.000000)\n",
      "884. feature 107 (0.000000)\n",
      "885. feature 106 (0.000000)\n",
      "886. feature 333 (0.000000)\n",
      "887. feature 334 (0.000000)\n",
      "888. feature 335 (0.000000)\n",
      "889. feature 336 (0.000000)\n",
      "890. feature 337 (0.000000)\n",
      "891. feature 338 (0.000000)\n",
      "892. feature 340 (0.000000)\n",
      "893. feature 361 (0.000000)\n",
      "894. feature 363 (0.000000)\n",
      "895. feature 364 (0.000000)\n",
      "896. feature 400 (0.000000)\n",
      "897. feature 392 (0.000000)\n",
      "898. feature 393 (0.000000)\n",
      "899. feature 96 (0.000000)\n",
      "900. feature 396 (0.000000)\n",
      "901. feature 397 (0.000000)\n",
      "902. feature 398 (0.000000)\n",
      "903. feature 399 (0.000000)\n",
      "904. feature 401 (0.000000)\n",
      "905. feature 390 (0.000000)\n",
      "906. feature 402 (0.000000)\n",
      "907. feature 95 (0.000000)\n",
      "908. feature 404 (0.000000)\n",
      "909. feature 405 (0.000000)\n",
      "910. feature 406 (0.000000)\n",
      "911. feature 94 (0.000000)\n",
      "912. feature 93 (0.000000)\n",
      "913. feature 391 (0.000000)\n",
      "914. feature 389 (0.000000)\n",
      "915. feature 366 (0.000000)\n",
      "916. feature 376 (0.000000)\n",
      "917. feature 368 (0.000000)\n",
      "918. feature 100 (0.000000)\n",
      "919. feature 371 (0.000000)\n",
      "920. feature 372 (0.000000)\n",
      "921. feature 99 (0.000000)\n",
      "922. feature 374 (0.000000)\n",
      "923. feature 98 (0.000000)\n",
      "924. feature 377 (0.000000)\n",
      "925. feature 388 (0.000000)\n",
      "926. feature 379 (0.000000)\n",
      "927. feature 380 (0.000000)\n",
      "928. feature 97 (0.000000)\n",
      "929. feature 383 (0.000000)\n",
      "930. feature 762 (0.000000)\n",
      "931. feature 386 (0.000000)\n",
      "932. feature 387 (0.000000)\n",
      "933. feature 761 (0.000000)\n",
      "934. feature 1695 (0.000000)\n",
      "935. feature 763 (0.000000)\n",
      "936. feature 1381 (0.000000)\n",
      "937. feature 1370 (0.000000)\n",
      "938. feature 1372 (0.000000)\n",
      "939. feature 1373 (0.000000)\n",
      "940. feature 1374 (0.000000)\n",
      "941. feature 1375 (0.000000)\n",
      "942. feature 1377 (0.000000)\n",
      "943. feature 1378 (0.000000)\n",
      "944. feature 1379 (0.000000)\n",
      "945. feature 1380 (0.000000)\n",
      "946. feature 1383 (0.000000)\n",
      "947. feature 1366 (0.000000)\n",
      "948. feature 1384 (0.000000)\n",
      "949. feature 1385 (0.000000)\n",
      "950. feature 1387 (0.000000)\n",
      "951. feature 1388 (0.000000)\n",
      "952. feature 1389 (0.000000)\n",
      "953. feature 1390 (0.000000)\n",
      "954. feature 1391 (0.000000)\n",
      "955. feature 1393 (0.000000)\n",
      "956. feature 1395 (0.000000)\n",
      "957. feature 1368 (0.000000)\n",
      "958. feature 1364 (0.000000)\n",
      "959. feature 1398 (0.000000)\n",
      "960. feature 1349 (0.000000)\n",
      "961. feature 1340 (0.000000)\n",
      "962. feature 1341 (0.000000)\n",
      "963. feature 1342 (0.000000)\n",
      "964. feature 1343 (0.000000)\n",
      "965. feature 1344 (0.000000)\n",
      "966. feature 1345 (0.000000)\n",
      "967. feature 1346 (0.000000)\n",
      "968. feature 1347 (0.000000)\n",
      "969. feature 1348 (0.000000)\n",
      "970. feature 1350 (0.000000)\n",
      "971. feature 1363 (0.000000)\n",
      "972. feature 1351 (0.000000)\n",
      "973. feature 1353 (0.000000)\n",
      "974. feature 1354 (0.000000)\n",
      "975. feature 1355 (0.000000)\n",
      "976. feature 1357 (0.000000)\n",
      "977. feature 1358 (0.000000)\n",
      "978. feature 1359 (0.000000)\n",
      "979. feature 1360 (0.000000)\n",
      "980. feature 1362 (0.000000)\n",
      "981. feature 1396 (0.000000)\n",
      "982. feature 1399 (0.000000)\n",
      "983. feature 1453 (0.000000)\n",
      "984. feature 1439 (0.000000)\n",
      "985. feature 1430 (0.000000)\n",
      "986. feature 1431 (0.000000)\n",
      "987. feature 1432 (0.000000)\n",
      "988. feature 1433 (0.000000)\n",
      "989. feature 1434 (0.000000)\n",
      "990. feature 1435 (0.000000)\n",
      "991. feature 1436 (0.000000)\n",
      "992. feature 1437 (0.000000)\n",
      "993. feature 1438 (0.000000)\n",
      "994. feature 1441 (0.000000)\n",
      "995. feature 1428 (0.000000)\n",
      "996. feature 1442 (0.000000)\n",
      "997. feature 1443 (0.000000)\n",
      "998. feature 1444 (0.000000)\n",
      "999. feature 1445 (0.000000)\n",
      "1000. feature 1446 (0.000000)\n",
      "1001. feature 1447 (0.000000)\n",
      "1002. feature 1448 (0.000000)\n",
      "1003. feature 1449 (0.000000)\n",
      "1004. feature 1451 (0.000000)\n",
      "1005. feature 1429 (0.000000)\n",
      "1006. feature 1427 (0.000000)\n",
      "1007. feature 1400 (0.000000)\n",
      "1008. feature 1412 (0.000000)\n",
      "1009. feature 1401 (0.000000)\n",
      "1010. feature 1403 (0.000000)\n",
      "1011. feature 1404 (0.000000)\n",
      "1012. feature 1405 (0.000000)\n",
      "1013. feature 1406 (0.000000)\n",
      "1014. feature 1407 (0.000000)\n",
      "1015. feature 1408 (0.000000)\n",
      "1016. feature 1409 (0.000000)\n",
      "1017. feature 1410 (0.000000)\n",
      "1018. feature 1413 (0.000000)\n",
      "1019. feature 1426 (0.000000)\n",
      "1020. feature 1415 (0.000000)\n",
      "1021. feature 1418 (0.000000)\n",
      "1022. feature 1419 (0.000000)\n",
      "1023. feature 1420 (0.000000)\n",
      "1024. feature 1421 (0.000000)\n",
      "1025. feature 1422 (0.000000)\n",
      "1026. feature 1423 (0.000000)\n",
      "1027. feature 1424 (0.000000)\n",
      "1028. feature 1425 (0.000000)\n",
      "1029. feature 1339 (0.000000)\n",
      "1030. feature 1338 (0.000000)\n",
      "1031. feature 1337 (0.000000)\n",
      "1032. feature 1272 (0.000000)\n",
      "1033. feature 1261 (0.000000)\n",
      "1034. feature 1262 (0.000000)\n",
      "1035. feature 1263 (0.000000)\n",
      "1036. feature 1264 (0.000000)\n",
      "1037. feature 1266 (0.000000)\n",
      "1038. feature 1267 (0.000000)\n",
      "1039. feature 1268 (0.000000)\n",
      "1040. feature 1270 (0.000000)\n",
      "1041. feature 1271 (0.000000)\n",
      "1042. feature 1273 (0.000000)\n",
      "1043. feature 1259 (0.000000)\n",
      "1044. feature 1274 (0.000000)\n",
      "1045. feature 1275 (0.000000)\n",
      "1046. feature 1277 (0.000000)\n",
      "1047. feature 1278 (0.000000)\n",
      "1048. feature 1279 (0.000000)\n",
      "1049. feature 1280 (0.000000)\n",
      "1050. feature 1281 (0.000000)\n",
      "1051. feature 1282 (0.000000)\n",
      "1052. feature 1283 (0.000000)\n",
      "1053. feature 1260 (0.000000)\n",
      "1054. feature 1258 (0.000000)\n",
      "1055. feature 1335 (0.000000)\n",
      "1056. feature 1246 (0.000000)\n",
      "1057. feature 1237 (0.000000)\n",
      "1058. feature 1238 (0.000000)\n",
      "1059. feature 1239 (0.000000)\n",
      "1060. feature 1240 (0.000000)\n",
      "1061. feature 1241 (0.000000)\n",
      "1062. feature 1242 (0.000000)\n",
      "1063. feature 1243 (0.000000)\n",
      "1064. feature 1244 (0.000000)\n",
      "1065. feature 1245 (0.000000)\n",
      "1066. feature 1247 (0.000000)\n",
      "1067. feature 1257 (0.000000)\n",
      "1068. feature 1248 (0.000000)\n",
      "1069. feature 1249 (0.000000)\n",
      "1070. feature 1250 (0.000000)\n",
      "1071. feature 1251 (0.000000)\n",
      "1072. feature 1252 (0.000000)\n",
      "1073. feature 1253 (0.000000)\n",
      "1074. feature 1254 (0.000000)\n",
      "1075. feature 1255 (0.000000)\n",
      "1076. feature 1256 (0.000000)\n",
      "1077. feature 1284 (0.000000)\n",
      "1078. feature 1285 (0.000000)\n",
      "1079. feature 1287 (0.000000)\n",
      "1080. feature 1321 (0.000000)\n",
      "1081. feature 1312 (0.000000)\n",
      "1082. feature 1313 (0.000000)\n",
      "1083. feature 1314 (0.000000)\n",
      "1084. feature 1315 (0.000000)\n",
      "1085. feature 1316 (0.000000)\n",
      "1086. feature 1317 (0.000000)\n",
      "1087. feature 1318 (0.000000)\n",
      "1088. feature 1319 (0.000000)\n",
      "1089. feature 1320 (0.000000)\n",
      "1090. feature 1322 (0.000000)\n",
      "1091. feature 1288 (0.000000)\n",
      "1092. feature 1323 (0.000000)\n",
      "1093. feature 1324 (0.000000)\n",
      "1094. feature 1325 (0.000000)\n",
      "1095. feature 1326 (0.000000)\n",
      "1096. feature 1328 (0.000000)\n",
      "1097. feature 1329 (0.000000)\n",
      "1098. feature 1330 (0.000000)\n",
      "1099. feature 1333 (0.000000)\n",
      "1100. feature 1334 (0.000000)\n",
      "1101. feature 1311 (0.000000)\n",
      "1102. feature 1310 (0.000000)\n",
      "1103. feature 1309 (0.000000)\n",
      "1104. feature 1308 (0.000000)\n",
      "1105. feature 1289 (0.000000)\n",
      "1106. feature 1290 (0.000000)\n",
      "1107. feature 1291 (0.000000)\n",
      "1108. feature 1292 (0.000000)\n",
      "1109. feature 1293 (0.000000)\n",
      "1110. feature 1294 (0.000000)\n",
      "1111. feature 1295 (0.000000)\n",
      "1112. feature 1296 (0.000000)\n",
      "1113. feature 1297 (0.000000)\n",
      "1114. feature 1298 (0.000000)\n",
      "1115. feature 1299 (0.000000)\n",
      "1116. feature 1300 (0.000000)\n",
      "1117. feature 1301 (0.000000)\n",
      "1118. feature 1302 (0.000000)\n",
      "1119. feature 1303 (0.000000)\n",
      "1120. feature 1304 (0.000000)\n",
      "1121. feature 1305 (0.000000)\n",
      "1122. feature 1306 (0.000000)\n",
      "1123. feature 1307 (0.000000)\n",
      "1124. feature 1452 (0.000000)\n",
      "1125. feature 1456 (0.000000)\n",
      "1126. feature 764 (0.000000)\n",
      "1127. feature 1614 (0.000000)\n",
      "1128. feature 1605 (0.000000)\n",
      "1129. feature 1606 (0.000000)\n",
      "1130. feature 1607 (0.000000)\n",
      "1131. feature 1608 (0.000000)\n",
      "1132. feature 1609 (0.000000)\n",
      "1133. feature 1610 (0.000000)\n",
      "1134. feature 1611 (0.000000)\n",
      "1135. feature 1612 (0.000000)\n",
      "1136. feature 1613 (0.000000)\n",
      "1137. feature 1615 (0.000000)\n",
      "1138. feature 1601 (0.000000)\n",
      "1139. feature 1616 (0.000000)\n",
      "1140. feature 1619 (0.000000)\n",
      "1141. feature 1620 (0.000000)\n",
      "1142. feature 1621 (0.000000)\n",
      "1143. feature 1622 (0.000000)\n",
      "1144. feature 1623 (0.000000)\n",
      "1145. feature 1624 (0.000000)\n",
      "1146. feature 1625 (0.000000)\n",
      "1147. feature 1626 (0.000000)\n",
      "1148. feature 1602 (0.000000)\n",
      "1149. feature 1600 (0.000000)\n",
      "1150. feature 1629 (0.000000)\n",
      "1151. feature 1586 (0.000000)\n",
      "1152. feature 1573 (0.000000)\n",
      "1153. feature 1574 (0.000000)\n",
      "1154. feature 1577 (0.000000)\n",
      "1155. feature 1578 (0.000000)\n",
      "1156. feature 1580 (0.000000)\n",
      "1157. feature 1581 (0.000000)\n",
      "1158. feature 1582 (0.000000)\n",
      "1159. feature 1584 (0.000000)\n",
      "1160. feature 1585 (0.000000)\n",
      "1161. feature 1587 (0.000000)\n",
      "1162. feature 1598 (0.000000)\n",
      "1163. feature 1588 (0.000000)\n",
      "1164. feature 1589 (0.000000)\n",
      "1165. feature 1590 (0.000000)\n",
      "1166. feature 1591 (0.000000)\n",
      "1167. feature 1592 (0.000000)\n",
      "1168. feature 1593 (0.000000)\n",
      "1169. feature 1594 (0.000000)\n",
      "1170. feature 1595 (0.000000)\n",
      "1171. feature 1596 (0.000000)\n",
      "1172. feature 1628 (0.000000)\n",
      "1173. feature 1630 (0.000000)\n",
      "1174. feature 1458 (0.000000)\n",
      "1175. feature 1679 (0.000000)\n",
      "1176. feature 1666 (0.000000)\n",
      "1177. feature 1668 (0.000000)\n",
      "1178. feature 1672 (0.000000)\n",
      "1179. feature 1673 (0.000000)\n",
      "1180. feature 1674 (0.000000)\n",
      "1181. feature 1675 (0.000000)\n",
      "1182. feature 1676 (0.000000)\n",
      "1183. feature 1677 (0.000000)\n",
      "1184. feature 1678 (0.000000)\n",
      "1185. feature 1680 (0.000000)\n",
      "1186. feature 1664 (0.000000)\n",
      "1187. feature 1683 (0.000000)\n",
      "1188. feature 1684 (0.000000)\n",
      "1189. feature 1686 (0.000000)\n",
      "1190. feature 1687 (0.000000)\n",
      "1191. feature 1688 (0.000000)\n",
      "1192. feature 1689 (0.000000)\n",
      "1193. feature 1690 (0.000000)\n",
      "1194. feature 1692 (0.000000)\n",
      "1195. feature 1693 (0.000000)\n",
      "1196. feature 1665 (0.000000)\n",
      "1197. feature 1662 (0.000000)\n",
      "1198. feature 1631 (0.000000)\n",
      "1199. feature 1642 (0.000000)\n",
      "1200. feature 1632 (0.000000)\n",
      "1201. feature 1633 (0.000000)\n",
      "1202. feature 1634 (0.000000)\n",
      "1203. feature 1635 (0.000000)\n",
      "1204. feature 1636 (0.000000)\n",
      "1205. feature 1638 (0.000000)\n",
      "1206. feature 1639 (0.000000)\n",
      "1207. feature 1640 (0.000000)\n",
      "1208. feature 1641 (0.000000)\n",
      "1209. feature 1643 (0.000000)\n",
      "1210. feature 1661 (0.000000)\n",
      "1211. feature 1644 (0.000000)\n",
      "1212. feature 1647 (0.000000)\n",
      "1213. feature 1648 (0.000000)\n",
      "1214. feature 1652 (0.000000)\n",
      "1215. feature 1653 (0.000000)\n",
      "1216. feature 1654 (0.000000)\n",
      "1217. feature 1657 (0.000000)\n",
      "1218. feature 1658 (0.000000)\n",
      "1219. feature 1659 (0.000000)\n",
      "1220. feature 1570 (0.000000)\n",
      "1221. feature 1569 (0.000000)\n",
      "1222. feature 1568 (0.000000)\n",
      "1223. feature 1499 (0.000000)\n",
      "1224. feature 1490 (0.000000)\n",
      "1225. feature 1491 (0.000000)\n",
      "1226. feature 1492 (0.000000)\n",
      "1227. feature 1493 (0.000000)\n",
      "1228. feature 1494 (0.000000)\n",
      "1229. feature 1495 (0.000000)\n",
      "1230. feature 1496 (0.000000)\n",
      "1231. feature 1497 (0.000000)\n",
      "1232. feature 1498 (0.000000)\n",
      "1233. feature 1500 (0.000000)\n",
      "1234. feature 1488 (0.000000)\n",
      "1235. feature 1501 (0.000000)\n",
      "1236. feature 1502 (0.000000)\n",
      "1237. feature 1503 (0.000000)\n",
      "1238. feature 1504 (0.000000)\n",
      "1239. feature 1505 (0.000000)\n",
      "1240. feature 1506 (0.000000)\n",
      "1241. feature 1507 (0.000000)\n",
      "1242. feature 1508 (0.000000)\n",
      "1243. feature 1509 (0.000000)\n",
      "1244. feature 1489 (0.000000)\n",
      "1245. feature 1486 (0.000000)\n",
      "1246. feature 1567 (0.000000)\n",
      "1247. feature 1473 (0.000000)\n",
      "1248. feature 1459 (0.000000)\n",
      "1249. feature 1460 (0.000000)\n",
      "1250. feature 1461 (0.000000)\n",
      "1251. feature 1462 (0.000000)\n",
      "1252. feature 1463 (0.000000)\n",
      "1253. feature 1464 (0.000000)\n",
      "1254. feature 1467 (0.000000)\n",
      "1255. feature 1468 (0.000000)\n",
      "1256. feature 1470 (0.000000)\n",
      "1257. feature 1474 (0.000000)\n",
      "1258. feature 1485 (0.000000)\n",
      "1259. feature 1475 (0.000000)\n",
      "1260. feature 1476 (0.000000)\n",
      "1261. feature 1477 (0.000000)\n",
      "1262. feature 1478 (0.000000)\n",
      "1263. feature 1479 (0.000000)\n",
      "1264. feature 1480 (0.000000)\n",
      "1265. feature 1481 (0.000000)\n",
      "1266. feature 1483 (0.000000)\n",
      "1267. feature 1484 (0.000000)\n",
      "1268. feature 1510 (0.000000)\n",
      "1269. feature 1511 (0.000000)\n",
      "1270. feature 1512 (0.000000)\n",
      "1271. feature 1553 (0.000000)\n",
      "1272. feature 1543 (0.000000)\n",
      "1273. feature 1544 (0.000000)\n",
      "1274. feature 1545 (0.000000)\n",
      "1275. feature 1546 (0.000000)\n",
      "1276. feature 1547 (0.000000)\n",
      "1277. feature 1548 (0.000000)\n",
      "1278. feature 1549 (0.000000)\n",
      "1279. feature 1550 (0.000000)\n",
      "1280. feature 1552 (0.000000)\n",
      "1281. feature 1555 (0.000000)\n",
      "1282. feature 1513 (0.000000)\n",
      "1283. feature 1556 (0.000000)\n",
      "1284. feature 1557 (0.000000)\n",
      "1285. feature 1558 (0.000000)\n",
      "1286. feature 1559 (0.000000)\n",
      "1287. feature 1560 (0.000000)\n",
      "1288. feature 1562 (0.000000)\n",
      "1289. feature 1564 (0.000000)\n",
      "1290. feature 1565 (0.000000)\n",
      "1291. feature 1566 (0.000000)\n",
      "1292. feature 1541 (0.000000)\n",
      "1293. feature 1539 (0.000000)\n",
      "1294. feature 1538 (0.000000)\n",
      "1295. feature 1536 (0.000000)\n",
      "1296. feature 1514 (0.000000)\n",
      "1297. feature 1515 (0.000000)\n",
      "1298. feature 1516 (0.000000)\n",
      "1299. feature 1517 (0.000000)\n",
      "1300. feature 1518 (0.000000)\n",
      "1301. feature 1519 (0.000000)\n",
      "1302. feature 1520 (0.000000)\n",
      "1303. feature 1521 (0.000000)\n",
      "1304. feature 1523 (0.000000)\n",
      "1305. feature 1524 (0.000000)\n",
      "1306. feature 1525 (0.000000)\n",
      "1307. feature 1527 (0.000000)\n",
      "1308. feature 1528 (0.000000)\n",
      "1309. feature 1529 (0.000000)\n",
      "1310. feature 1530 (0.000000)\n",
      "1311. feature 1531 (0.000000)\n",
      "1312. feature 1532 (0.000000)\n",
      "1313. feature 1533 (0.000000)\n",
      "1314. feature 1535 (0.000000)\n",
      "1315. feature 1235 (0.000000)\n",
      "1316. feature 1234 (0.000000)\n",
      "1317. feature 1232 (0.000000)\n",
      "1318. feature 917 (0.000000)\n",
      "1319. feature 904 (0.000000)\n",
      "1320. feature 905 (0.000000)\n",
      "1321. feature 906 (0.000000)\n",
      "1322. feature 907 (0.000000)\n",
      "1323. feature 911 (0.000000)\n",
      "1324. feature 912 (0.000000)\n",
      "1325. feature 913 (0.000000)\n",
      "1326. feature 914 (0.000000)\n",
      "1327. feature 915 (0.000000)\n",
      "1328. feature 920 (0.000000)\n",
      "1329. feature 902 (0.000000)\n",
      "1330. feature 921 (0.000000)\n",
      "1331. feature 922 (0.000000)\n",
      "1332. feature 923 (0.000000)\n",
      "1333. feature 924 (0.000000)\n",
      "1334. feature 925 (0.000000)\n",
      "1335. feature 926 (0.000000)\n",
      "1336. feature 929 (0.000000)\n",
      "1337. feature 931 (0.000000)\n",
      "1338. feature 932 (0.000000)\n",
      "1339. feature 903 (0.000000)\n",
      "1340. feature 901 (0.000000)\n",
      "1341. feature 935 (0.000000)\n",
      "1342. feature 888 (0.000000)\n",
      "1343. feature 879 (0.000000)\n",
      "1344. feature 880 (0.000000)\n",
      "1345. feature 881 (0.000000)\n",
      "1346. feature 882 (0.000000)\n",
      "1347. feature 883 (0.000000)\n",
      "1348. feature 884 (0.000000)\n",
      "1349. feature 885 (0.000000)\n",
      "1350. feature 886 (0.000000)\n",
      "1351. feature 887 (0.000000)\n",
      "1352. feature 889 (0.000000)\n",
      "1353. feature 900 (0.000000)\n",
      "1354. feature 890 (0.000000)\n",
      "1355. feature 891 (0.000000)\n",
      "1356. feature 892 (0.000000)\n",
      "1357. feature 894 (0.000000)\n",
      "1358. feature 895 (0.000000)\n",
      "1359. feature 896 (0.000000)\n",
      "1360. feature 897 (0.000000)\n",
      "1361. feature 898 (0.000000)\n",
      "1362. feature 899 (0.000000)\n",
      "1363. feature 933 (0.000000)\n",
      "1364. feature 936 (0.000000)\n",
      "1365. feature 1231 (0.000000)\n",
      "1366. feature 979 (0.000000)\n",
      "1367. feature 968 (0.000000)\n",
      "1368. feature 969 (0.000000)\n",
      "1369. feature 970 (0.000000)\n",
      "1370. feature 971 (0.000000)\n",
      "1371. feature 972 (0.000000)\n",
      "1372. feature 973 (0.000000)\n",
      "1373. feature 974 (0.000000)\n",
      "1374. feature 975 (0.000000)\n",
      "1375. feature 976 (0.000000)\n",
      "1376. feature 980 (0.000000)\n",
      "1377. feature 966 (0.000000)\n",
      "1378. feature 981 (0.000000)\n",
      "1379. feature 982 (0.000000)\n",
      "1380. feature 983 (0.000000)\n",
      "1381. feature 984 (0.000000)\n",
      "1382. feature 985 (0.000000)\n",
      "1383. feature 986 (0.000000)\n",
      "1384. feature 987 (0.000000)\n",
      "1385. feature 989 (0.000000)\n",
      "1386. feature 990 (0.000000)\n",
      "1387. feature 967 (0.000000)\n",
      "1388. feature 964 (0.000000)\n",
      "1389. feature 937 (0.000000)\n",
      "1390. feature 950 (0.000000)\n",
      "1391. feature 938 (0.000000)\n",
      "1392. feature 939 (0.000000)\n",
      "1393. feature 942 (0.000000)\n",
      "1394. feature 943 (0.000000)\n",
      "1395. feature 944 (0.000000)\n",
      "1396. feature 945 (0.000000)\n",
      "1397. feature 946 (0.000000)\n",
      "1398. feature 948 (0.000000)\n",
      "1399. feature 949 (0.000000)\n",
      "1400. feature 951 (0.000000)\n",
      "1401. feature 963 (0.000000)\n",
      "1402. feature 952 (0.000000)\n",
      "1403. feature 955 (0.000000)\n",
      "1404. feature 956 (0.000000)\n",
      "1405. feature 957 (0.000000)\n",
      "1406. feature 958 (0.000000)\n",
      "1407. feature 959 (0.000000)\n",
      "1408. feature 960 (0.000000)\n",
      "1409. feature 961 (0.000000)\n",
      "1410. feature 962 (0.000000)\n",
      "1411. feature 877 (0.000000)\n",
      "1412. feature 875 (0.000000)\n",
      "1413. feature 874 (0.000000)\n",
      "1414. feature 801 (0.000000)\n",
      "1415. feature 791 (0.000000)\n",
      "1416. feature 792 (0.000000)\n",
      "1417. feature 793 (0.000000)\n",
      "1418. feature 795 (0.000000)\n",
      "1419. feature 796 (0.000000)\n",
      "1420. feature 797 (0.000000)\n",
      "1421. feature 798 (0.000000)\n",
      "1422. feature 799 (0.000000)\n",
      "1423. feature 800 (0.000000)\n",
      "1424. feature 802 (0.000000)\n",
      "1425. feature 789 (0.000000)\n",
      "1426. feature 803 (0.000000)\n",
      "1427. feature 804 (0.000000)\n",
      "1428. feature 805 (0.000000)\n",
      "1429. feature 806 (0.000000)\n",
      "1430. feature 807 (0.000000)\n",
      "1431. feature 808 (0.000000)\n",
      "1432. feature 809 (0.000000)\n",
      "1433. feature 810 (0.000000)\n",
      "1434. feature 811 (0.000000)\n",
      "1435. feature 790 (0.000000)\n",
      "1436. feature 788 (0.000000)\n",
      "1437. feature 873 (0.000000)\n",
      "1438. feature 775 (0.000000)\n",
      "1439. feature 766 (0.000000)\n",
      "1440. feature 767 (0.000000)\n",
      "1441. feature 768 (0.000000)\n",
      "1442. feature 769 (0.000000)\n",
      "1443. feature 770 (0.000000)\n",
      "1444. feature 771 (0.000000)\n",
      "1445. feature 772 (0.000000)\n",
      "1446. feature 773 (0.000000)\n",
      "1447. feature 774 (0.000000)\n",
      "1448. feature 776 (0.000000)\n",
      "1449. feature 787 (0.000000)\n",
      "1450. feature 777 (0.000000)\n",
      "1451. feature 778 (0.000000)\n",
      "1452. feature 779 (0.000000)\n",
      "1453. feature 780 (0.000000)\n",
      "1454. feature 781 (0.000000)\n",
      "1455. feature 782 (0.000000)\n",
      "1456. feature 783 (0.000000)\n",
      "1457. feature 784 (0.000000)\n",
      "1458. feature 786 (0.000000)\n",
      "1459. feature 813 (0.000000)\n",
      "1460. feature 814 (0.000000)\n",
      "1461. feature 815 (0.000000)\n",
      "1462. feature 858 (0.000000)\n",
      "1463. feature 1694 (0.000000)\n",
      "1464. feature 848 (0.000000)\n",
      "1465. feature 849 (0.000000)\n",
      "1466. feature 850 (0.000000)\n",
      "1467. feature 851 (0.000000)\n",
      "1468. feature 853 (0.000000)\n",
      "1469. feature 854 (0.000000)\n",
      "1470. feature 855 (0.000000)\n",
      "1471. feature 857 (0.000000)\n",
      "1472. feature 860 (0.000000)\n",
      "1473. feature 816 (0.000000)\n",
      "1474. feature 861 (0.000000)\n",
      "1475. feature 862 (0.000000)\n",
      "1476. feature 863 (0.000000)\n",
      "1477. feature 864 (0.000000)\n",
      "1478. feature 865 (0.000000)\n",
      "1479. feature 866 (0.000000)\n",
      "1480. feature 869 (0.000000)\n",
      "1481. feature 870 (0.000000)\n",
      "1482. feature 871 (0.000000)\n",
      "1483. feature 846 (0.000000)\n",
      "1484. feature 845 (0.000000)\n",
      "1485. feature 844 (0.000000)\n",
      "1486. feature 843 (0.000000)\n",
      "1487. feature 817 (0.000000)\n",
      "1488. feature 818 (0.000000)\n",
      "1489. feature 819 (0.000000)\n",
      "1490. feature 820 (0.000000)\n",
      "1491. feature 821 (0.000000)\n",
      "1492. feature 822 (0.000000)\n",
      "1493. feature 823 (0.000000)\n",
      "1494. feature 824 (0.000000)\n",
      "1495. feature 827 (0.000000)\n",
      "1496. feature 828 (0.000000)\n",
      "1497. feature 829 (0.000000)\n",
      "1498. feature 830 (0.000000)\n",
      "1499. feature 831 (0.000000)\n",
      "1500. feature 832 (0.000000)\n",
      "1501. feature 833 (0.000000)\n",
      "1502. feature 834 (0.000000)\n",
      "1503. feature 837 (0.000000)\n",
      "1504. feature 838 (0.000000)\n",
      "1505. feature 840 (0.000000)\n",
      "1506. feature 991 (0.000000)\n",
      "1507. feature 992 (0.000000)\n",
      "1508. feature 993 (0.000000)\n",
      "1509. feature 1162 (0.000000)\n",
      "1510. feature 1149 (0.000000)\n",
      "1511. feature 1151 (0.000000)\n",
      "1512. feature 1153 (0.000000)\n",
      "1513. feature 1154 (0.000000)\n",
      "1514. feature 1156 (0.000000)\n",
      "1515. feature 1158 (0.000000)\n",
      "1516. feature 1159 (0.000000)\n",
      "1517. feature 1160 (0.000000)\n",
      "1518. feature 1161 (0.000000)\n",
      "1519. feature 1163 (0.000000)\n",
      "1520. feature 1147 (0.000000)\n",
      "1521. feature 1166 (0.000000)\n",
      "1522. feature 1167 (0.000000)\n",
      "1523. feature 1168 (0.000000)\n",
      "1524. feature 1169 (0.000000)\n",
      "1525. feature 1171 (0.000000)\n",
      "1526. feature 1172 (0.000000)\n",
      "1527. feature 1173 (0.000000)\n",
      "1528. feature 1174 (0.000000)\n",
      "1529. feature 1175 (0.000000)\n",
      "1530. feature 1148 (0.000000)\n",
      "1531. feature 1146 (0.000000)\n",
      "1532. feature 1118 (0.000000)\n",
      "1533. feature 1133 (0.000000)\n",
      "1534. feature 1123 (0.000000)\n",
      "1535. feature 1124 (0.000000)\n",
      "1536. feature 1125 (0.000000)\n",
      "1537. feature 1126 (0.000000)\n",
      "1538. feature 1127 (0.000000)\n",
      "1539. feature 1128 (0.000000)\n",
      "1540. feature 1129 (0.000000)\n",
      "1541. feature 1131 (0.000000)\n",
      "1542. feature 1132 (0.000000)\n",
      "1543. feature 1134 (0.000000)\n",
      "1544. feature 1145 (0.000000)\n",
      "1545. feature 1135 (0.000000)\n",
      "1546. feature 1136 (0.000000)\n",
      "1547. feature 1137 (0.000000)\n",
      "1548. feature 1138 (0.000000)\n",
      "1549. feature 1140 (0.000000)\n",
      "1550. feature 1141 (0.000000)\n",
      "1551. feature 1142 (0.000000)\n",
      "1552. feature 1143 (0.000000)\n",
      "1553. feature 1144 (0.000000)\n",
      "1554. feature 1176 (0.000000)\n",
      "1555. feature 1177 (0.000000)\n",
      "1556. feature 1178 (0.000000)\n",
      "1557. feature 1219 (0.000000)\n",
      "1558. feature 1208 (0.000000)\n",
      "1559. feature 1209 (0.000000)\n",
      "1560. feature 1211 (0.000000)\n",
      "1561. feature 1213 (0.000000)\n",
      "1562. feature 1214 (0.000000)\n",
      "1563. feature 1215 (0.000000)\n",
      "1564. feature 1216 (0.000000)\n",
      "1565. feature 1217 (0.000000)\n",
      "1566. feature 1218 (0.000000)\n",
      "1567. feature 1220 (0.000000)\n",
      "1568. feature 1180 (0.000000)\n",
      "1569. feature 1222 (0.000000)\n",
      "1570. feature 1223 (0.000000)\n",
      "1571. feature 1224 (0.000000)\n",
      "1572. feature 1225 (0.000000)\n",
      "1573. feature 1226 (0.000000)\n",
      "1574. feature 1227 (0.000000)\n",
      "1575. feature 1228 (0.000000)\n",
      "1576. feature 1229 (0.000000)\n",
      "1577. feature 1230 (0.000000)\n",
      "1578. feature 1207 (0.000000)\n",
      "1579. feature 1206 (0.000000)\n",
      "1580. feature 1205 (0.000000)\n",
      "1581. feature 1204 (0.000000)\n",
      "1582. feature 1181 (0.000000)\n",
      "1583. feature 1182 (0.000000)\n",
      "1584. feature 1183 (0.000000)\n",
      "1585. feature 1184 (0.000000)\n",
      "1586. feature 1186 (0.000000)\n",
      "1587. feature 1187 (0.000000)\n",
      "1588. feature 1188 (0.000000)\n",
      "1589. feature 1189 (0.000000)\n",
      "1590. feature 1190 (0.000000)\n",
      "1591. feature 1191 (0.000000)\n",
      "1592. feature 1192 (0.000000)\n",
      "1593. feature 1193 (0.000000)\n",
      "1594. feature 1194 (0.000000)\n",
      "1595. feature 1195 (0.000000)\n",
      "1596. feature 1196 (0.000000)\n",
      "1597. feature 1197 (0.000000)\n",
      "1598. feature 1198 (0.000000)\n",
      "1599. feature 1199 (0.000000)\n",
      "1600. feature 1202 (0.000000)\n",
      "1601. feature 1122 (0.000000)\n",
      "1602. feature 1117 (0.000000)\n",
      "1603. feature 997 (0.000000)\n",
      "1604. feature 1037 (0.000000)\n",
      "1605. feature 1027 (0.000000)\n",
      "1606. feature 1028 (0.000000)\n",
      "1607. feature 1029 (0.000000)\n",
      "1608. feature 1030 (0.000000)\n",
      "1609. feature 1031 (0.000000)\n",
      "1610. feature 1032 (0.000000)\n",
      "1611. feature 1033 (0.000000)\n",
      "1612. feature 1034 (0.000000)\n",
      "1613. feature 1036 (0.000000)\n",
      "1614. feature 1040 (0.000000)\n",
      "1615. feature 1025 (0.000000)\n",
      "1616. feature 1041 (0.000000)\n",
      "1617. feature 1042 (0.000000)\n",
      "1618. feature 1044 (0.000000)\n",
      "1619. feature 1045 (0.000000)\n",
      "1620. feature 1047 (0.000000)\n",
      "1621. feature 1048 (0.000000)\n",
      "1622. feature 1049 (0.000000)\n",
      "1623. feature 1050 (0.000000)\n",
      "1624. feature 1051 (0.000000)\n",
      "1625. feature 1026 (0.000000)\n",
      "1626. feature 1024 (0.000000)\n",
      "1627. feature 1116 (0.000000)\n",
      "1628. feature 1008 (0.000000)\n",
      "1629. feature 998 (0.000000)\n",
      "1630. feature 999 (0.000000)\n",
      "1631. feature 1000 (0.000000)\n",
      "1632. feature 1001 (0.000000)\n",
      "1633. feature 1002 (0.000000)\n",
      "1634. feature 1003 (0.000000)\n",
      "1635. feature 1005 (0.000000)\n",
      "1636. feature 1006 (0.000000)\n",
      "1637. feature 1007 (0.000000)\n",
      "1638. feature 1009 (0.000000)\n",
      "1639. feature 1022 (0.000000)\n",
      "1640. feature 1010 (0.000000)\n",
      "1641. feature 1012 (0.000000)\n",
      "1642. feature 1013 (0.000000)\n",
      "1643. feature 1014 (0.000000)\n",
      "1644. feature 1015 (0.000000)\n",
      "1645. feature 1016 (0.000000)\n",
      "1646. feature 1017 (0.000000)\n",
      "1647. feature 1019 (0.000000)\n",
      "1648. feature 1020 (0.000000)\n",
      "1649. feature 1052 (0.000000)\n",
      "1650. feature 1053 (0.000000)\n",
      "1651. feature 1054 (0.000000)\n",
      "1652. feature 1097 (0.000000)\n",
      "1653. feature 1087 (0.000000)\n",
      "1654. feature 1089 (0.000000)\n",
      "1655. feature 1090 (0.000000)\n",
      "1656. feature 1091 (0.000000)\n",
      "1657. feature 1092 (0.000000)\n",
      "1658. feature 1093 (0.000000)\n",
      "1659. feature 1094 (0.000000)\n",
      "1660. feature 1095 (0.000000)\n",
      "1661. feature 1096 (0.000000)\n",
      "1662. feature 1098 (0.000000)\n",
      "1663. feature 1055 (0.000000)\n",
      "1664. feature 1099 (0.000000)\n",
      "1665. feature 1100 (0.000000)\n",
      "1666. feature 1102 (0.000000)\n",
      "1667. feature 1103 (0.000000)\n",
      "1668. feature 1104 (0.000000)\n",
      "1669. feature 1107 (0.000000)\n",
      "1670. feature 1108 (0.000000)\n",
      "1671. feature 1111 (0.000000)\n",
      "1672. feature 1114 (0.000000)\n",
      "1673. feature 1086 (0.000000)\n",
      "1674. feature 1085 (0.000000)\n",
      "1675. feature 1084 (0.000000)\n",
      "1676. feature 1082 (0.000000)\n",
      "1677. feature 1056 (0.000000)\n",
      "1678. feature 1058 (0.000000)\n",
      "1679. feature 1059 (0.000000)\n",
      "1680. feature 1060 (0.000000)\n",
      "1681. feature 1061 (0.000000)\n",
      "1682. feature 1062 (0.000000)\n",
      "1683. feature 1063 (0.000000)\n",
      "1684. feature 1064 (0.000000)\n",
      "1685. feature 1065 (0.000000)\n",
      "1686. feature 1066 (0.000000)\n",
      "1687. feature 1067 (0.000000)\n",
      "1688. feature 1069 (0.000000)\n",
      "1689. feature 1071 (0.000000)\n",
      "1690. feature 1074 (0.000000)\n",
      "1691. feature 1075 (0.000000)\n",
      "1692. feature 1076 (0.000000)\n",
      "1693. feature 1078 (0.000000)\n",
      "1694. feature 1079 (0.000000)\n",
      "1695. feature 1080 (0.000000)\n",
      "1696. feature 847 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "importances = rdc.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class RandomizedLasso is deprecated; The class RandomizedLasso is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RandomizedLasso\n",
    "\n",
    "\n",
    "X = Xer(train_data, \"all\")\n",
    "y = Yer(train_label, 10).reshape(train_label.shape[0],)\n",
    "\n",
    "# rdc = RandomForestRegressor(max_depth=100, random_state=0, n_estimators=1000, max_features=None, min_samples_leaf=2)\n",
    "# rdc.fit(X, y)\n",
    "\n",
    "\n",
    " \n",
    "# print \"Features sorted by their score:\"\n",
    "# rlasso.scores_\n",
    "\n",
    "lizfin = []\n",
    "lez = [0.000001,0.0000001,0.00000001,0.000000001][::-1]\n",
    "\n",
    "\n",
    "\n",
    "rlasso = RandomizedLasso(alpha=0.001)\n",
    "rlasso.fit(X, y)\n",
    "\n",
    "lizn = rlasso.scores_\n",
    "lizn = list(lizn)\n",
    "\n",
    "# lizfin.append(np.count_nonzero(lizn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/utils/deprecation.py:58: DeprecationWarning: Class RandomizedLasso is deprecated; The class RandomizedLasso is deprecated in 0.19 and will be removed in 0.21.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(255, 165)"
      ]
     },
     "execution_count": 1166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlasso = RandomizedLasso(alpha=0.001)\n",
    "rlasso.fit(X, y)\n",
    "X_lassod = rlasso.transform(X)\n",
    "X_lassod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.535,\n",
       " 0.17,\n",
       " 0.495,\n",
       " 0.535,\n",
       " 0.395,\n",
       " 0.235,\n",
       " 0.125,\n",
       " 0.08,\n",
       " 0.105,\n",
       " 0.065,\n",
       " 0.095,\n",
       " 0.085,\n",
       " 0.03,\n",
       " 0.055,\n",
       " 0.03,\n",
       " 0.35,\n",
       " 0.05,\n",
       " 0.06,\n",
       " 0.105,\n",
       " 0.15,\n",
       " 0.125,\n",
       " 0.115,\n",
       " 0.03,\n",
       " 0.235,\n",
       " 0.32,\n",
       " 0.08,\n",
       " 0.07,\n",
       " 0.19,\n",
       " 0.185,\n",
       " 0.38,\n",
       " 0.06,\n",
       " 0.14,\n",
       " 0.705,\n",
       " 0.155,\n",
       " 0.37,\n",
       " 0.0,\n",
       " 0.31,\n",
       " 0.51,\n",
       " 0.36,\n",
       " 0.67,\n",
       " 0.55,\n",
       " 0.0,\n",
       " 0.245,\n",
       " 0.0,\n",
       " 0.79,\n",
       " 0.105,\n",
       " 0.485,\n",
       " 0.38,\n",
       " 0.74,\n",
       " 0.405,\n",
       " 0.615,\n",
       " 0.0,\n",
       " 0.545,\n",
       " 0.0,\n",
       " 0.205,\n",
       " 0.17,\n",
       " 0.52,\n",
       " 0.585,\n",
       " 0.315,\n",
       " 0.25,\n",
       " 0.31,\n",
       " 0.815,\n",
       " 0.645,\n",
       " 0.92,\n",
       " 0.0,\n",
       " 0.465,\n",
       " 0.215,\n",
       " 0.595,\n",
       " 0.195,\n",
       " 0.135,\n",
       " 0.415,\n",
       " 0.39,\n",
       " 0.16,\n",
       " 0.0,\n",
       " 0.44,\n",
       " 0.465,\n",
       " 0.48,\n",
       " 0.395,\n",
       " 0.2,\n",
       " 0.115,\n",
       " 0.14,\n",
       " 0.17,\n",
       " 0.09,\n",
       " 0.155,\n",
       " 0.12,\n",
       " 0.14,\n",
       " 0.135,\n",
       " 0.15,\n",
       " 0.08,\n",
       " 0.06,\n",
       " 0.17,\n",
       " 0.28,\n",
       " 0.125,\n",
       " 0.1,\n",
       " 0.235,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.635,\n",
       " 0.24,\n",
       " 0.17,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.49,\n",
       " 0.14,\n",
       " 0.16,\n",
       " 0.26,\n",
       " 0.12,\n",
       " 0.0,\n",
       " 0.045,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.11,\n",
       " 0.0,\n",
       " 0.015,\n",
       " 0.085,\n",
       " 0.065,\n",
       " 0.02,\n",
       " 0.005,\n",
       " 0.025,\n",
       " 0.03,\n",
       " 0.015,\n",
       " 0.075,\n",
       " 0.03,\n",
       " 0.08,\n",
       " 0.495,\n",
       " 0.235,\n",
       " 0.385,\n",
       " 0.15,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.065,\n",
       " 0.175,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.085,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.225,\n",
       " 0.035,\n",
       " 0.27,\n",
       " 0.055,\n",
       " 0.13,\n",
       " 0.05,\n",
       " 0.0,\n",
       " 0.015,\n",
       " 0.0,\n",
       " 0.15,\n",
       " 0.1,\n",
       " 0.02,\n",
       " 0.045,\n",
       " 0.105,\n",
       " 0.055,\n",
       " 0.035,\n",
       " 0.01,\n",
       " 0.03,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.18,\n",
       " 0.13,\n",
       " 0.035,\n",
       " 0.125,\n",
       " 0.03,\n",
       " 0.095,\n",
       " 0.06,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.0,\n",
       " 0.18,\n",
       " 0.015,\n",
       " 0.015,\n",
       " 0.05,\n",
       " 0.08,\n",
       " 0.02,\n",
       " 0.055,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.015,\n",
       " 0.0,\n",
       " 0.17,\n",
       " 0.07,\n",
       " 0.14,\n",
       " 0.135,\n",
       " 0.03,\n",
       " 0.2,\n",
       " 0.065,\n",
       " 0.0,\n",
       " 0.07,\n",
       " 0.0,\n",
       " 0.26,\n",
       " 0.04,\n",
       " 0.025,\n",
       " 0.035,\n",
       " 0.07,\n",
       " 0.03,\n",
       " 0.03,\n",
       " 0.03,\n",
       " 0.005,\n",
       " 0.05,\n",
       " 0.005,\n",
       " 0.085,\n",
       " 0.24,\n",
       " 0.15,\n",
       " 0.22,\n",
       " 0.06,\n",
       " 0.12,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.25,\n",
       " 0.015,\n",
       " 0.025,\n",
       " 0.075,\n",
       " 0.04,\n",
       " 0.05,\n",
       " 0.04,\n",
       " 0.0,\n",
       " 0.09,\n",
       " 0.0,\n",
       " 0.13,\n",
       " 0.225,\n",
       " 0.225,\n",
       " 0.38,\n",
       " 0.07,\n",
       " 0.15,\n",
       " 0.145,\n",
       " 0.005,\n",
       " 0.055,\n",
       " 0.0,\n",
       " 0.26,\n",
       " 0.045,\n",
       " 0.01,\n",
       " 0.075,\n",
       " 0.235,\n",
       " 0.08,\n",
       " 0.035,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.185,\n",
       " 0.005,\n",
       " 0.23,\n",
       " 0.075,\n",
       " 0.34,\n",
       " 0.03,\n",
       " 0.045,\n",
       " 0.115,\n",
       " 0.155,\n",
       " 0.0,\n",
       " 0.035,\n",
       " 0.0,\n",
       " 0.495,\n",
       " 0.01,\n",
       " 0.005,\n",
       " 0.005,\n",
       " 0.08,\n",
       " 0.015,\n",
       " 0.015,\n",
       " 0.005,\n",
       " 0.075,\n",
       " 0.03,\n",
       " 0.005,\n",
       " 0.39,\n",
       " 0.285,\n",
       " 0.11,\n",
       " 0.275,\n",
       " 0.075,\n",
       " 0.365,\n",
       " 0.105,\n",
       " 0.005,\n",
       " 0.075,\n",
       " 0.0,\n",
       " 0.345,\n",
       " 0.085,\n",
       " 0.06,\n",
       " 0.24,\n",
       " 0.04,\n",
       " 0.035,\n",
       " 0.24,\n",
       " 0.015,\n",
       " 0.005,\n",
       " 0.03,\n",
       " 0.0,\n",
       " 0.09,\n",
       " 0.13,\n",
       " 0.3,\n",
       " 0.19,\n",
       " 0.04,\n",
       " 0.085,\n",
       " 0.15,\n",
       " 0.0,\n",
       " 0.085,\n",
       " 0.0,\n",
       " 0.685,\n",
       " 0.03,\n",
       " 0.025,\n",
       " 0.115,\n",
       " 0.105,\n",
       " 0.005,\n",
       " 0.07,\n",
       " 0.025,\n",
       " 0.01,\n",
       " 0.085,\n",
       " 0.005,\n",
       " 0.155,\n",
       " 0.28,\n",
       " 0.395,\n",
       " 0.16,\n",
       " 0.26,\n",
       " 0.13,\n",
       " 0.095,\n",
       " 0.0,\n",
       " 0.09,\n",
       " 0.005,\n",
       " 0.225,\n",
       " 0.02,\n",
       " 0.1,\n",
       " 0.225,\n",
       " 0.16,\n",
       " 0.025,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.005,\n",
       " 0.01,\n",
       " 0.225,\n",
       " 0.16,\n",
       " 0.245,\n",
       " 0.195,\n",
       " 0.065,\n",
       " 0.145,\n",
       " 0.22,\n",
       " 0.06,\n",
       " 0.455,\n",
       " 0.01,\n",
       " 0.31,\n",
       " 0.1,\n",
       " 0.075,\n",
       " 0.08,\n",
       " 0.1,\n",
       " 0.085,\n",
       " 0.05,\n",
       " 0.015,\n",
       " 0.065,\n",
       " 0.045,\n",
       " 0.03,\n",
       " 0.305,\n",
       " 0.21,\n",
       " 0.42,\n",
       " 0.28,\n",
       " 0.125,\n",
       " 0.115,\n",
       " 0.225,\n",
       " 0.0,\n",
       " 0.045,\n",
       " 0.0,\n",
       " 0.185,\n",
       " 0.035,\n",
       " 0.03,\n",
       " 0.09,\n",
       " 0.285,\n",
       " 0.015,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.13,\n",
       " 0.0,\n",
       " 0.19,\n",
       " 0.24,\n",
       " 0.355,\n",
       " 0.155,\n",
       " 0.03,\n",
       " 0.125,\n",
       " 0.14,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.235,\n",
       " 0.115,\n",
       " 0.01,\n",
       " 0.075,\n",
       " 0.17,\n",
       " 0.05,\n",
       " 0.035,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.135,\n",
       " 0.015,\n",
       " 0.175,\n",
       " 0.2,\n",
       " 0.265,\n",
       " 0.405,\n",
       " 0.055,\n",
       " 0.15,\n",
       " 0.155,\n",
       " 0.0,\n",
       " 0.08,\n",
       " 0.0,\n",
       " 0.235,\n",
       " 0.075,\n",
       " 0.02,\n",
       " 0.125,\n",
       " 0.085,\n",
       " 0.065,\n",
       " 0.055,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.36,\n",
       " 0.025,\n",
       " 0.24,\n",
       " 0.135,\n",
       " 0.035,\n",
       " 0.2,\n",
       " 0.38,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.005,\n",
       " 0.575,\n",
       " 0.115,\n",
       " 0.005,\n",
       " 0.285,\n",
       " 0.12,\n",
       " 0.13,\n",
       " 0.185,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.015,\n",
       " 0.1,\n",
       " 0.33,\n",
       " 0.23,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.07,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.015,\n",
       " 0.04,\n",
       " 0.025,\n",
       " 0.015,\n",
       " 0.0,\n",
       " 0.035,\n",
       " 0.0,\n",
       " 0.02,\n",
       " 0.065,\n",
       " 0.125,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.075,\n",
       " 0.005,\n",
       " 0.025,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.03,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.035,\n",
       " 0.12,\n",
       " 0.075,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02,\n",
       " 0.04,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.015,\n",
       " 0.015,\n",
       " 0.055,\n",
       " 0.145,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.02,\n",
       " 0.015,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.01,\n",
       " 0.065,\n",
       " 0.135,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02,\n",
       " 0.18,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.07,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.11,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.12,\n",
       " 0.205,\n",
       " 0.275,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.03,\n",
       " 0.0,\n",
       " 0.03,\n",
       " 0.14,\n",
       " 0.0,\n",
       " 0.055,\n",
       " 0.01,\n",
       " 0.035,\n",
       " 0.39,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.11,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.3,\n",
       " 0.17,\n",
       " 0.06,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.005,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.065,\n",
       " 0.015,\n",
       " 0.035,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.045,\n",
       " 0.0,\n",
       " 0.11,\n",
       " 0.125,\n",
       " 0.085,\n",
       " 0.135,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.015,\n",
       " 0.145,\n",
       " 0.005,\n",
       " 0.015,\n",
       " 0.015,\n",
       " 0.01,\n",
       " 0.16,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.03,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.235,\n",
       " 0.18,\n",
       " 0.165,\n",
       " 0.0,\n",
       " 0.07,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.045,\n",
       " 0.0,\n",
       " 0.12,\n",
       " 0.07,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.16,\n",
       " 0.015,\n",
       " 0.015,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.16,\n",
       " 0.355,\n",
       " 0.565,\n",
       " 0.0,\n",
       " 0.05,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.065,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.16,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.085,\n",
       " 0.06,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08,\n",
       " 0.255,\n",
       " 0.205,\n",
       " 0.16,\n",
       " 0.0,\n",
       " 0.07,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.015,\n",
       " 0.0,\n",
       " 0.15,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.065,\n",
       " 0.16,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.27,\n",
       " 0.085,\n",
       " 0.07,\n",
       " 0.105,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.11,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.09,\n",
       " 0.115,\n",
       " 0.06,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.315,\n",
       " 0.11,\n",
       " 0.1,\n",
       " 0.13,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.065,\n",
       " 0.0,\n",
       " 0.19,\n",
       " 0.12,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.085,\n",
       " 0.015,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.235,\n",
       " 0.185,\n",
       " 0.035,\n",
       " 0.08,\n",
       " 0.0,\n",
       " 0.015,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.085,\n",
       " 0.0,\n",
       " 0.09,\n",
       " 0.04,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.135,\n",
       " 0.015,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.11,\n",
       " 0.03,\n",
       " 0.29,\n",
       " 0.545,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.015,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.15,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.065,\n",
       " 0.02,\n",
       " 0.055,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.055,\n",
       " 0.015,\n",
       " 0.11,\n",
       " 0.27,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.04,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.055,\n",
       " 0.005,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.235,\n",
       " 0.12,\n",
       " 0.305,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.03,\n",
       " 0.085,\n",
       " 0.055,\n",
       " 0.205,\n",
       " 0.0,\n",
       " 0.035,\n",
       " 0.015,\n",
       " 0.07,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.325,\n",
       " 0.49,\n",
       " 0.09,\n",
       " 0.055,\n",
       " 0.015,\n",
       " 0.045,\n",
       " 0.07,\n",
       " 0.14,\n",
       " 0.11,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.035,\n",
       " 0.0,\n",
       " 0.12,\n",
       " 0.03,\n",
       " 0.045,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.105,\n",
       " 0.025,\n",
       " 0.075,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08,\n",
       " 0.415,\n",
       " 0.255,\n",
       " 0.305,\n",
       " 0.08,\n",
       " 0.015,\n",
       " 0.055,\n",
       " 0.0,\n",
       " 0.11,\n",
       " 0.005,\n",
       " 0.34,\n",
       " 0.06,\n",
       " 0.02,\n",
       " 0.185,\n",
       " 0.025,\n",
       " 0.02,\n",
       " 0.06,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.3,\n",
       " 0.165,\n",
       " 0.41,\n",
       " 0.18,\n",
       " 0.01,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.08,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.035,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.025,\n",
       " 0.005,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.17,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.15,\n",
       " 0.235,\n",
       " 0.05,\n",
       " 0.125,\n",
       " 0.055,\n",
       " 0.0,\n",
       " 0.125,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.175,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.1,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.15,\n",
       " 0.14,\n",
       " 0.22,\n",
       " 0.165,\n",
       " 0.115,\n",
       " 0.015,\n",
       " 0.07,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.0,\n",
       " 0.14,\n",
       " 0.225,\n",
       " 0.005,\n",
       " 0.135,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.12,\n",
       " 0.015,\n",
       " 0.035,\n",
       " 0.1,\n",
       " 0.075,\n",
       " 0.205,\n",
       " 0.17,\n",
       " 0.275,\n",
       " 0.055,\n",
       " 0.09,\n",
       " 0.0,\n",
       " 0.055,\n",
       " 0.0,\n",
       " 0.18,\n",
       " 0.07,\n",
       " 0.0,\n",
       " 0.215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.295,\n",
       " 0.27,\n",
       " 0.155,\n",
       " 0.645,\n",
       " 0.41,\n",
       " 0.01,\n",
       " 0.04,\n",
       " 0.0,\n",
       " 0.035,\n",
       " 0.0,\n",
       " 0.46,\n",
       " 0.035,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.02,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.105,\n",
       " 0.1,\n",
       " 0.085,\n",
       " 0.425,\n",
       " 0.49,\n",
       " 0.015,\n",
       " 0.09,\n",
       " 0.0,\n",
       " 0.015,\n",
       " 0.0,\n",
       " 0.28,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.2,\n",
       " 0.0,\n",
       " 0.01,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.14,\n",
       " 0.575,\n",
       " 0.23,\n",
       " 0.225,\n",
       " 0.175,\n",
       " 0.015,\n",
       " 0.105,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.165,\n",
       " 0.01,\n",
       " 0.01,\n",
       " 0.315,\n",
       " 0.0,\n",
       " 0.005,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.305,\n",
       " 0.425,\n",
       " 0.28,\n",
       " 0.16,\n",
       " 0.185,\n",
       " 0.04,\n",
       " 0.235,\n",
       " ...]"
      ]
     },
     "execution_count": 1127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lizn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1191, 1195, 1197, 1214]"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lizn = rlasso.scores_\n",
    "# lizn = list(lizn)\n",
    "# print(np.count_nonzero(lizn))\n",
    "# lizn.sort()\n",
    "# lizn[::-1]\n",
    "lizfin\n",
    "\n",
    "\n",
    "# ran with 0.0001, got 1197\n",
    "\n",
    "# ran with [0.00001,0.001,0.01,0.1,1,10]\n",
    "# got [1209, 1172, 991, 2, 0, 0]\n",
    "\n",
    "# ran with [0.000001,0.0000001,0.00000001,0.000000001][::-1]\n",
    "# got [1191, 1195, 1197, 1214]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature 63 (0.920000)\n",
      "2. feature 61 (0.815000)\n",
      "3. feature 44 (0.790000)\n",
      "4. feature 48 (0.740000)\n",
      "5. feature 1597 (0.725000)\n",
      "6. feature 32 (0.705000)\n",
      "7. feature 310 (0.685000)\n",
      "8. feature 39 (0.670000)\n",
      "9. feature 62 (0.645000)\n",
      "10. feature 933 (0.645000)\n",
      "11. feature 103 (0.635000)\n",
      "12. feature 50 (0.615000)\n",
      "13. feature 1279 (0.615000)\n",
      "14. feature 1269 (0.610000)\n",
      "15. feature 67 (0.595000)\n",
      "16. feature 1558 (0.595000)\n",
      "17. feature 1497 (0.590000)\n",
      "18. feature 57 (0.585000)\n",
      "19. feature 1332 (0.575000)\n",
      "20. feature 973 (0.575000)\n",
      "21. feature 436 (0.575000)\n",
      "22. feature 639 (0.565000)\n",
      "23. feature 1048 (0.560000)\n",
      "24. feature 40 (0.550000)\n",
      "25. feature 1143 (0.545000)\n",
      "26. feature 744 (0.545000)\n",
      "27. feature 52 (0.545000)\n",
      "28. feature 1655 (0.535000)\n",
      "29. feature 0 (0.535000)\n",
      "30. feature 3 (0.535000)\n",
      "31. feature 56 (0.520000)\n",
      "32. feature 1618 (0.520000)\n",
      "33. feature 1121 (0.510000)\n",
      "34. feature 37 (0.510000)\n",
      "35. feature 1108 (0.505000)\n",
      "36. feature 1060 (0.505000)\n",
      "37. feature 268 (0.495000)\n",
      "38. feature 132 (0.495000)\n",
      "39. feature 2 (0.495000)\n",
      "40. feature 110 (0.490000)\n",
      "41. feature 799 (0.490000)\n",
      "42. feature 955 (0.490000)\n",
      "43. feature 46 (0.485000)\n",
      "44. feature 76 (0.480000)\n",
      "45. feature 75 (0.465000)\n",
      "46. feature 65 (0.465000)\n",
      "47. feature 940 (0.460000)\n",
      "48. feature 350 (0.455000)\n",
      "49. feature 74 (0.440000)\n",
      "50. feature 954 (0.425000)\n",
      "51. feature 994 (0.425000)\n",
      "52. feature 365 (0.420000)\n",
      "53. feature 826 (0.415000)\n",
      "54. feature 1129 (0.415000)\n",
      "55. feature 70 (0.415000)\n",
      "56. feature 1083 (0.415000)\n",
      "57. feature 848 (0.410000)\n",
      "58. feature 934 (0.410000)\n",
      "59. feature 49 (0.405000)\n",
      "60. feature 408 (0.405000)\n",
      "61. feature 4 (0.395000)\n",
      "62. feature 77 (0.395000)\n",
      "63. feature 323 (0.395000)\n",
      "64. feature 279 (0.390000)\n",
      "65. feature 568 (0.390000)\n",
      "66. feature 71 (0.390000)\n",
      "67. feature 1036 (0.385000)\n",
      "68. feature 134 (0.385000)\n",
      "69. feature 240 (0.380000)\n",
      "70. feature 47 (0.380000)\n",
      "71. feature 29 (0.380000)\n",
      "72. feature 432 (0.380000)\n",
      "73. feature 1101 (0.375000)\n",
      "74. feature 34 (0.370000)\n",
      "75. feature 1150 (0.370000)\n",
      "76. feature 1511 (0.365000)\n",
      "77. feature 284 (0.365000)\n",
      "78. feature 426 (0.360000)\n",
      "79. feature 1351 (0.360000)\n",
      "80. feature 1045 (0.360000)\n",
      "81. feature 1311 (0.360000)\n",
      "82. feature 38 (0.360000)\n",
      "83. feature 1674 (0.360000)\n",
      "84. feature 386 (0.355000)\n",
      "85. feature 638 (0.355000)\n",
      "86. feature 1599 (0.350000)\n",
      "87. feature 1561 (0.350000)\n",
      "88. feature 15 (0.350000)\n",
      "89. feature 1457 (0.350000)\n",
      "90. feature 289 (0.345000)\n",
      "91. feature 1559 (0.345000)\n",
      "92. feature 260 (0.340000)\n",
      "93. feature 1119 (0.340000)\n",
      "94. feature 835 (0.340000)\n",
      "95. feature 1018 (0.335000)\n",
      "96. feature 448 (0.330000)\n",
      "97. feature 1027 (0.330000)\n",
      "98. feature 1195 (0.330000)\n",
      "99. feature 798 (0.325000)\n",
      "100. feature 24 (0.320000)\n",
      "101. feature 58 (0.315000)\n",
      "102. feature 1458 (0.315000)\n",
      "103. feature 699 (0.315000)\n",
      "104. feature 985 (0.315000)\n",
      "105. feature 1342 (0.310000)\n",
      "106. feature 352 (0.310000)\n",
      "107. feature 60 (0.310000)\n",
      "108. feature 36 (0.310000)\n",
      "109. feature 363 (0.305000)\n",
      "110. feature 828 (0.305000)\n",
      "111. feature 785 (0.305000)\n",
      "112. feature 993 (0.305000)\n",
      "113. feature 302 (0.300000)\n",
      "114. feature 1656 (0.300000)\n",
      "115. feature 574 (0.300000)\n",
      "116. feature 1087 (0.300000)\n",
      "117. feature 1041 (0.300000)\n",
      "118. feature 846 (0.300000)\n",
      "119. feature 1225 (0.300000)\n",
      "120. feature 930 (0.295000)\n",
      "121. feature 1648 (0.295000)\n",
      "122. feature 1058 (0.295000)\n",
      "123. feature 1090 (0.290000)\n",
      "124. feature 1435 (0.290000)\n",
      "125. feature 1358 (0.290000)\n",
      "126. feature 1468 (0.290000)\n",
      "127. feature 743 (0.290000)\n",
      "128. feature 1540 (0.285000)\n",
      "129. feature 1066 (0.285000)\n",
      "130. feature 439 (0.285000)\n",
      "131. feature 377 (0.285000)\n",
      "132. feature 280 (0.285000)\n",
      "133. feature 961 (0.280000)\n",
      "134. feature 322 (0.280000)\n",
      "135. feature 1360 (0.280000)\n",
      "136. feature 91 (0.280000)\n",
      "137. feature 995 (0.280000)\n",
      "138. feature 366 (0.280000)\n",
      "139. feature 913 (0.275000)\n",
      "140. feature 282 (0.275000)\n",
      "141. feature 555 (0.275000)\n",
      "142. feature 1436 (0.275000)\n",
      "143. feature 931 (0.270000)\n",
      "144. feature 765 (0.270000)\n",
      "145. feature 678 (0.270000)\n",
      "146. feature 156 (0.270000)\n",
      "147. feature 1434 (0.265000)\n",
      "148. feature 1415 (0.265000)\n",
      "149. feature 407 (0.265000)\n",
      "150. feature 1185 (0.265000)\n",
      "151. feature 113 (0.260000)\n",
      "152. feature 205 (0.260000)\n",
      "153. feature 1258 (0.260000)\n",
      "154. feature 1142 (0.260000)\n",
      "155. feature 1413 (0.260000)\n",
      "156. feature 1637 (0.260000)\n",
      "157. feature 1353 (0.260000)\n",
      "158. feature 247 (0.260000)\n",
      "159. feature 325 (0.260000)\n",
      "160. feature 1059 (0.255000)\n",
      "161. feature 658 (0.255000)\n",
      "162. feature 827 (0.255000)\n",
      "163. feature 1405 (0.255000)\n",
      "164. feature 1509 (0.255000)\n",
      "165. feature 59 (0.250000)\n",
      "166. feature 227 (0.250000)\n",
      "167. feature 1625 (0.250000)\n",
      "168. feature 1217 (0.250000)\n",
      "169. feature 1103 (0.245000)\n",
      "170. feature 1120 (0.245000)\n",
      "171. feature 1321 (0.245000)\n",
      "172. feature 344 (0.245000)\n",
      "173. feature 1079 (0.245000)\n",
      "174. feature 1003 (0.245000)\n",
      "175. feature 42 (0.245000)\n",
      "176. feature 1015 (0.240000)\n",
      "177. feature 428 (0.240000)\n",
      "178. feature 104 (0.240000)\n",
      "179. feature 1456 (0.240000)\n",
      "180. feature 1111 (0.240000)\n",
      "181. feature 217 (0.240000)\n",
      "182. feature 292 (0.240000)\n",
      "183. feature 385 (0.240000)\n",
      "184. feature 295 (0.240000)\n",
      "185. feature 720 (0.235000)\n",
      "186. feature 1381 (0.235000)\n",
      "187. feature 999 (0.235000)\n",
      "188. feature 1628 (0.235000)\n",
      "189. feature 394 (0.235000)\n",
      "190. feature 783 (0.235000)\n",
      "191. feature 868 (0.235000)\n",
      "192. feature 1016 (0.235000)\n",
      "193. feature 251 (0.235000)\n",
      "194. feature 94 (0.235000)\n",
      "195. feature 23 (0.235000)\n",
      "196. feature 1184 (0.235000)\n",
      "197. feature 133 (0.235000)\n",
      "198. feature 1164 (0.235000)\n",
      "199. feature 415 (0.235000)\n",
      "200. feature 616 (0.235000)\n",
      "201. feature 5 (0.235000)\n",
      "202. feature 1081 (0.235000)\n",
      "203. feature 974 (0.230000)\n",
      "204. feature 1371 (0.230000)\n",
      "205. feature 258 (0.230000)\n",
      "206. feature 1598 (0.230000)\n",
      "207. feature 1437 (0.230000)\n",
      "208. feature 1099 (0.230000)\n",
      "209. feature 449 (0.230000)\n",
      "210. feature 334 (0.225000)\n",
      "211. feature 1309 (0.225000)\n",
      "212. feature 369 (0.225000)\n",
      "213. feature 154 (0.225000)\n",
      "214. feature 342 (0.225000)\n",
      "215. feature 899 (0.225000)\n",
      "216. feature 1424 (0.225000)\n",
      "217. feature 238 (0.225000)\n",
      "218. feature 331 (0.225000)\n",
      "219. feature 975 (0.225000)\n",
      "220. feature 239 (0.225000)\n",
      "221. feature 1174 (0.220000)\n",
      "222. feature 1339 (0.220000)\n",
      "223. feature 1205 (0.220000)\n",
      "224. feature 1489 (0.220000)\n",
      "225. feature 219 (0.220000)\n",
      "226. feature 348 (0.220000)\n",
      "227. feature 1384 (0.220000)\n",
      "228. feature 890 (0.220000)\n",
      "229. feature 1331 (0.215000)\n",
      "230. feature 922 (0.215000)\n",
      "231. feature 66 (0.215000)\n",
      "232. feature 1675 (0.215000)\n",
      "233. feature 1663 (0.210000)\n",
      "234. feature 1100 (0.210000)\n",
      "235. feature 1132 (0.210000)\n",
      "236. feature 1340 (0.210000)\n",
      "237. feature 364 (0.210000)\n",
      "238. feature 1423 (0.210000)\n",
      "239. feature 659 (0.205000)\n",
      "240. feature 1580 (0.205000)\n",
      "241. feature 1589 (0.205000)\n",
      "242. feature 554 (0.205000)\n",
      "243. feature 1520 (0.205000)\n",
      "244. feature 791 (0.205000)\n",
      "245. feature 1333 (0.205000)\n",
      "246. feature 54 (0.205000)\n",
      "247. feature 1056 (0.205000)\n",
      "248. feature 911 (0.205000)\n",
      "249. feature 1402 (0.205000)\n",
      "250. feature 1153 (0.200000)\n",
      "251. feature 431 (0.200000)\n",
      "252. feature 1694 (0.200000)\n",
      "253. feature 964 (0.200000)\n",
      "254. feature 1393 (0.200000)\n",
      "255. feature 1104 (0.200000)\n",
      "256. feature 78 (0.200000)\n",
      "257. feature 406 (0.200000)\n",
      "258. feature 943 (0.200000)\n",
      "259. feature 200 (0.200000)\n",
      "260. feature 1477 (0.195000)\n",
      "261. feature 1682 (0.195000)\n",
      "262. feature 1569 (0.195000)\n",
      "263. feature 1426 (0.195000)\n",
      "264. feature 1692 (0.195000)\n",
      "265. feature 1476 (0.195000)\n",
      "266. feature 68 (0.195000)\n",
      "267. feature 1554 (0.195000)\n",
      "268. feature 345 (0.195000)\n",
      "269. feature 1455 (0.195000)\n",
      "270. feature 1077 (0.195000)\n",
      "271. feature 1123 (0.190000)\n",
      "272. feature 1162 (0.190000)\n",
      "273. feature 27 (0.190000)\n",
      "274. feature 384 (0.190000)\n",
      "275. feature 1513 (0.190000)\n",
      "276. feature 709 (0.190000)\n",
      "277. feature 303 (0.190000)\n",
      "278. feature 838 (0.185000)\n",
      "279. feature 28 (0.185000)\n",
      "280. feature 721 (0.185000)\n",
      "281. feature 373 (0.185000)\n",
      "282. feature 442 (0.185000)\n",
      "283. feature 1337 (0.185000)\n",
      "284. feature 997 (0.185000)\n",
      "285. feature 256 (0.185000)\n",
      "286. feature 184 (0.180000)\n",
      "287. feature 919 (0.180000)\n",
      "288. feature 849 (0.180000)\n",
      "289. feature 1024 (0.180000)\n",
      "290. feature 617 (0.180000)\n",
      "291. feature 1508 (0.180000)\n",
      "292. feature 1671 (0.180000)\n",
      "293. feature 1375 (0.180000)\n",
      "294. feature 1141 (0.180000)\n",
      "295. feature 1080 (0.180000)\n",
      "296. feature 1658 (0.180000)\n",
      "297. feature 542 (0.180000)\n",
      "298. feature 174 (0.180000)\n",
      "299. feature 1363 (0.175000)\n",
      "300. feature 1122 (0.175000)\n",
      "301. feature 1651 (0.175000)\n",
      "302. feature 976 (0.175000)\n",
      "303. feature 877 (0.175000)\n",
      "304. feature 143 (0.175000)\n",
      "305. feature 1140 (0.175000)\n",
      "306. feature 405 (0.175000)\n",
      "307. feature 1224 (0.170000)\n",
      "308. feature 398 (0.170000)\n",
      "309. feature 1 (0.170000)\n",
      "310. feature 55 (0.170000)\n",
      "311. feature 81 (0.170000)\n",
      "312. feature 90 (0.170000)\n",
      "313. feature 105 (0.170000)\n",
      "314. feature 195 (0.170000)\n",
      "315. feature 1521 (0.170000)\n",
      "316. feature 575 (0.170000)\n",
      "317. feature 864 (0.170000)\n",
      "318. feature 912 (0.170000)\n",
      "319. feature 1466 (0.170000)\n",
      "320. feature 1248 (0.170000)\n",
      "321. feature 891 (0.165000)\n",
      "322. feature 982 (0.165000)\n",
      "323. feature 1017 (0.165000)\n",
      "324. feature 618 (0.165000)\n",
      "325. feature 847 (0.165000)\n",
      "326. feature 1644 (0.165000)\n",
      "327. feature 1592 (0.165000)\n",
      "328. feature 1145 (0.160000)\n",
      "329. feature 343 (0.160000)\n",
      "330. feature 1329 (0.160000)\n",
      "331. feature 631 (0.160000)\n",
      "332. feature 1082 (0.160000)\n",
      "333. feature 112 (0.160000)\n",
      "334. feature 637 (0.160000)\n",
      "335. feature 996 (0.160000)\n",
      "336. feature 335 (0.160000)\n",
      "337. feature 673 (0.160000)\n",
      "338. feature 1354 (0.160000)\n",
      "339. feature 1247 (0.160000)\n",
      "340. feature 647 (0.160000)\n",
      "341. feature 1685 (0.160000)\n",
      "342. feature 72 (0.160000)\n",
      "343. feature 660 (0.160000)\n",
      "344. feature 610 (0.160000)\n",
      "345. feature 324 (0.160000)\n",
      "346. feature 33 (0.155000)\n",
      "347. feature 411 (0.155000)\n",
      "348. feature 321 (0.155000)\n",
      "349. feature 932 (0.155000)\n",
      "350. feature 387 (0.155000)\n",
      "351. feature 264 (0.155000)\n",
      "352. feature 1069 (0.155000)\n",
      "353. feature 83 (0.155000)\n",
      "354. feature 1305 (0.150000)\n",
      "355. feature 19 (0.150000)\n",
      "356. feature 1392 (0.150000)\n",
      "357. feature 1267 (0.150000)\n",
      "358. feature 306 (0.150000)\n",
      "359. feature 163 (0.150000)\n",
      "360. feature 135 (0.150000)\n",
      "361. feature 410 (0.150000)\n",
      "362. feature 242 (0.150000)\n",
      "363. feature 667 (0.150000)\n",
      "364. feature 1057 (0.150000)\n",
      "365. feature 218 (0.150000)\n",
      "366. feature 867 (0.150000)\n",
      "367. feature 87 (0.150000)\n",
      "368. feature 752 (0.150000)\n",
      "369. feature 888 (0.150000)\n",
      "370. feature 1501 (0.145000)\n",
      "371. feature 1414 (0.145000)\n",
      "372. feature 513 (0.145000)\n",
      "373. feature 605 (0.145000)\n",
      "374. feature 1161 (0.145000)\n",
      "375. feature 1037 (0.145000)\n",
      "376. feature 1522 (0.145000)\n",
      "377. feature 347 (0.145000)\n",
      "378. feature 1465 (0.145000)\n",
      "379. feature 243 (0.145000)\n",
      "380. feature 1668 (0.145000)\n",
      "381. feature 1531 (0.145000)\n",
      "382. feature 80 (0.140000)\n",
      "383. feature 1583 (0.140000)\n",
      "384. feature 85 (0.140000)\n",
      "385. feature 390 (0.140000)\n",
      "386. feature 1040 (0.140000)\n",
      "387. feature 31 (0.140000)\n",
      "388. feature 1109 (0.140000)\n",
      "389. feature 1102 (0.140000)\n",
      "390. feature 111 (0.140000)\n",
      "391. feature 1541 (0.140000)\n",
      "392. feature 563 (0.140000)\n",
      "393. feature 972 (0.140000)\n",
      "394. feature 889 (0.140000)\n",
      "395. feature 197 (0.140000)\n",
      "396. feature 1612 (0.140000)\n",
      "397. feature 1310 (0.140000)\n",
      "398. feature 1579 (0.140000)\n",
      "399. feature 805 (0.140000)\n",
      "400. feature 898 (0.140000)\n",
      "401. feature 429 (0.135000)\n",
      "402. feature 86 (0.135000)\n",
      "403. feature 735 (0.135000)\n",
      "404. feature 1582 (0.135000)\n",
      "405. feature 1650 (0.135000)\n",
      "406. feature 69 (0.135000)\n",
      "407. feature 901 (0.135000)\n",
      "408. feature 403 (0.135000)\n",
      "409. feature 597 (0.135000)\n",
      "410. feature 198 (0.135000)\n",
      "411. feature 1219 (0.135000)\n",
      "412. feature 534 (0.135000)\n",
      "413. feature 158 (0.130000)\n",
      "414. feature 1004 (0.130000)\n",
      "415. feature 1006 (0.130000)\n",
      "416. feature 702 (0.130000)\n",
      "417. feature 1447 (0.130000)\n",
      "418. feature 326 (0.130000)\n",
      "419. feature 301 (0.130000)\n",
      "420. feature 1604 (0.130000)\n",
      "421. feature 1038 (0.130000)\n",
      "422. feature 1666 (0.130000)\n",
      "423. feature 237 (0.130000)\n",
      "424. feature 1416 (0.130000)\n",
      "425. feature 441 (0.130000)\n",
      "426. feature 1163 (0.130000)\n",
      "427. feature 382 (0.130000)\n",
      "428. feature 175 (0.130000)\n",
      "429. feature 1649 (0.125000)\n",
      "430. feature 20 (0.125000)\n",
      "431. feature 177 (0.125000)\n",
      "432. feature 873 (0.125000)\n",
      "433. feature 6 (0.125000)\n",
      "434. feature 1078 (0.125000)\n",
      "435. feature 870 (0.125000)\n",
      "436. feature 1289 (0.125000)\n",
      "437. feature 807 (0.125000)\n",
      "438. feature 595 (0.125000)\n",
      "439. feature 470 (0.125000)\n",
      "440. feature 389 (0.125000)\n",
      "441. feature 636 (0.125000)\n",
      "442. feature 1557 (0.125000)\n",
      "443. feature 226 (0.125000)\n",
      "444. feature 367 (0.125000)\n",
      "445. feature 1039 (0.125000)\n",
      "446. feature 92 (0.125000)\n",
      "447. feature 1227 (0.125000)\n",
      "448. feature 418 (0.125000)\n",
      "449. feature 84 (0.120000)\n",
      "450. feature 1206 (0.120000)\n",
      "451. feature 440 (0.120000)\n",
      "452. feature 553 (0.120000)\n",
      "453. feature 221 (0.120000)\n",
      "454. feature 1636 (0.120000)\n",
      "455. feature 1372 (0.120000)\n",
      "456. feature 114 (0.120000)\n",
      "457. feature 814 (0.120000)\n",
      "458. feature 1528 (0.120000)\n",
      "459. feature 1098 (0.120000)\n",
      "460. feature 784 (0.120000)\n",
      "461. feature 625 (0.120000)\n",
      "462. feature 491 (0.120000)\n",
      "463. feature 1295 (0.120000)\n",
      "464. feature 1330 (0.120000)\n",
      "465. feature 1542 (0.120000)\n",
      "466. feature 906 (0.120000)\n",
      "467. feature 710 (0.120000)\n",
      "468. feature 1617 (0.120000)\n",
      "469. feature 79 (0.115000)\n",
      "470. feature 368 (0.115000)\n",
      "471. feature 1106 (0.115000)\n",
      "472. feature 313 (0.115000)\n",
      "473. feature 1183 (0.115000)\n",
      "474. feature 1586 (0.115000)\n",
      "475. feature 263 (0.115000)\n",
      "476. feature 892 (0.115000)\n",
      "477. feature 437 (0.115000)\n",
      "478. feature 694 (0.115000)\n",
      "479. feature 1335 (0.115000)\n",
      "480. feature 1130 (0.115000)\n",
      "481. feature 395 (0.115000)\n",
      "482. feature 21 (0.115000)\n",
      "483. feature 1395 (0.115000)\n",
      "484. feature 1246 (0.110000)\n",
      "485. feature 741 (0.110000)\n",
      "486. feature 550 (0.110000)\n",
      "487. feature 1290 (0.110000)\n",
      "488. feature 764 (0.110000)\n",
      "489. feature 571 (0.110000)\n",
      "490. feature 594 (0.110000)\n",
      "491. feature 281 (0.110000)\n",
      "492. feature 833 (0.110000)\n",
      "493. feature 688 (0.110000)\n",
      "494. feature 119 (0.110000)\n",
      "495. feature 700 (0.110000)\n",
      "496. feature 1175 (0.110000)\n",
      "497. feature 806 (0.110000)\n",
      "498. feature 1417 (0.105000)\n",
      "499. feature 820 (0.105000)\n",
      "500. feature 18 (0.105000)\n",
      "501. feature 978 (0.105000)\n",
      "502. feature 45 (0.105000)\n",
      "503. feature 1268 (0.105000)\n",
      "504. feature 681 (0.105000)\n",
      "505. feature 314 (0.105000)\n",
      "506. feature 1146 (0.105000)\n",
      "507. feature 167 (0.105000)\n",
      "508. feature 8 (0.105000)\n",
      "509. feature 1445 (0.105000)\n",
      "510. feature 1601 (0.105000)\n",
      "511. feature 951 (0.105000)\n",
      "512. feature 285 (0.105000)\n",
      "513. feature 1550 (0.105000)\n",
      "514. feature 668 (0.100000)\n",
      "515. feature 1691 (0.100000)\n",
      "516. feature 880 (0.100000)\n",
      "517. feature 471 (0.100000)\n",
      "518. feature 353 (0.100000)\n",
      "519. feature 819 (0.100000)\n",
      "520. feature 1512 (0.100000)\n",
      "521. feature 1560 (0.100000)\n",
      "522. feature 701 (0.100000)\n",
      "523. feature 952 (0.100000)\n",
      "524. feature 646 (0.100000)\n",
      "525. feature 164 (0.100000)\n",
      "526. feature 1564 (0.100000)\n",
      "527. feature 93 (0.100000)\n",
      "528. feature 909 (0.100000)\n",
      "529. feature 447 (0.100000)\n",
      "530. feature 450 (0.100000)\n",
      "531. feature 1014 (0.100000)\n",
      "532. feature 356 (0.100000)\n",
      "533. feature 333 (0.100000)\n",
      "534. feature 117 (0.100000)\n",
      "535. feature 1607 (0.095000)\n",
      "536. feature 179 (0.095000)\n",
      "537. feature 1596 (0.095000)\n",
      "538. feature 327 (0.095000)\n",
      "539. feature 1500 (0.095000)\n",
      "540. feature 1573 (0.095000)\n",
      "541. feature 10 (0.095000)\n",
      "542. feature 1226 (0.095000)\n",
      "543. feature 957 (0.090000)\n",
      "544. feature 1125 (0.090000)\n",
      "545. feature 915 (0.090000)\n",
      "546. feature 376 (0.090000)\n",
      "547. feature 730 (0.090000)\n",
      "548. feature 235 (0.090000)\n",
      "549. feature 693 (0.090000)\n",
      "550. feature 329 (0.090000)\n",
      "551. feature 1574 (0.090000)\n",
      "552. feature 800 (0.090000)\n",
      "553. feature 1672 (0.090000)\n",
      "554. feature 1300 (0.090000)\n",
      "555. feature 1172 (0.090000)\n",
      "556. feature 1373 (0.090000)\n",
      "557. feature 1350 (0.090000)\n",
      "558. feature 1665 (0.090000)\n",
      "559. feature 300 (0.090000)\n",
      "560. feature 82 (0.090000)\n",
      "561. feature 419 (0.085000)\n",
      "562. feature 596 (0.085000)\n",
      "563. feature 1646 (0.085000)\n",
      "564. feature 1453 (0.085000)\n",
      "565. feature 789 (0.085000)\n",
      "566. feature 1352 (0.085000)\n",
      "567. feature 1570 (0.085000)\n",
      "568. feature 308 (0.085000)\n",
      "569. feature 1652 (0.085000)\n",
      "570. feature 1444 (0.085000)\n",
      "571. feature 679 (0.085000)\n",
      "572. feature 1680 (0.085000)\n",
      "573. feature 122 (0.085000)\n",
      "574. feature 11 (0.085000)\n",
      "575. feature 953 (0.085000)\n",
      "576. feature 305 (0.085000)\n",
      "577. feature 148 (0.085000)\n",
      "578. feature 319 (0.085000)\n",
      "579. feature 290 (0.085000)\n",
      "580. feature 651 (0.085000)\n",
      "581. feature 714 (0.085000)\n",
      "582. feature 728 (0.085000)\n",
      "583. feature 357 (0.085000)\n",
      "584. feature 216 (0.085000)\n",
      "585. feature 854 (0.080000)\n",
      "586. feature 252 (0.080000)\n",
      "587. feature 188 (0.080000)\n",
      "588. feature 657 (0.080000)\n",
      "589. feature 1197 (0.080000)\n",
      "590. feature 355 (0.080000)\n",
      "591. feature 88 (0.080000)\n",
      "592. feature 131 (0.080000)\n",
      "593. feature 272 (0.080000)\n",
      "594. feature 1584 (0.080000)\n",
      "595. feature 723 (0.080000)\n",
      "596. feature 7 (0.080000)\n",
      "597. feature 1588 (0.080000)\n",
      "598. feature 413 (0.080000)\n",
      "599. feature 825 (0.080000)\n",
      "600. feature 829 (0.080000)\n",
      "601. feature 1308 (0.080000)\n",
      "602. feature 1374 (0.080000)\n",
      "603. feature 1182 (0.080000)\n",
      "604. feature 1549 (0.080000)\n",
      "605. feature 25 (0.080000)\n",
      "606. feature 479 (0.075000)\n",
      "607. feature 1277 (0.075000)\n",
      "608. feature 416 (0.075000)\n",
      "609. feature 230 (0.075000)\n",
      "610. feature 259 (0.075000)\n",
      "611. feature 354 (0.075000)\n",
      "612. feature 250 (0.075000)\n",
      "613. feature 1498 (0.075000)\n",
      "614. feature 1274 (0.075000)\n",
      "615. feature 910 (0.075000)\n",
      "616. feature 129 (0.075000)\n",
      "617. feature 1394 (0.075000)\n",
      "618. feature 492 (0.075000)\n",
      "619. feature 1144 (0.075000)\n",
      "620. feature 822 (0.075000)\n",
      "621. feature 276 (0.075000)\n",
      "622. feature 1514 (0.075000)\n",
      "623. feature 397 (0.075000)\n",
      "624. feature 287 (0.075000)\n",
      "625. feature 283 (0.075000)\n",
      "626. feature 458 (0.070000)\n",
      "627. feature 26 (0.070000)\n",
      "628. feature 626 (0.070000)\n",
      "629. feature 547 (0.070000)\n",
      "630. feature 209 (0.070000)\n",
      "631. feature 241 (0.070000)\n",
      "632. feature 1631 (0.070000)\n",
      "633. feature 920 (0.070000)\n",
      "634. feature 1020 (0.070000)\n",
      "635. feature 1645 (0.070000)\n",
      "636. feature 1507 (0.070000)\n",
      "637. feature 1238 (0.070000)\n",
      "638. feature 196 (0.070000)\n",
      "639. feature 1615 (0.070000)\n",
      "640. feature 620 (0.070000)\n",
      "641. feature 203 (0.070000)\n",
      "642. feature 804 (0.070000)\n",
      "643. feature 795 (0.070000)\n",
      "644. feature 1683 (0.070000)\n",
      "645. feature 316 (0.070000)\n",
      "646. feature 662 (0.070000)\n",
      "647. feature 680 (0.070000)\n",
      "648. feature 1602 (0.070000)\n",
      "649. feature 894 (0.070000)\n",
      "650. feature 1064 (0.070000)\n",
      "651. feature 1547 (0.065000)\n",
      "652. feature 756 (0.065000)\n",
      "653. feature 1635 (0.065000)\n",
      "654. feature 644 (0.065000)\n",
      "655. feature 1595 (0.065000)\n",
      "656. feature 360 (0.065000)\n",
      "657. feature 707 (0.065000)\n",
      "658. feature 142 (0.065000)\n",
      "659. feature 1555 (0.065000)\n",
      "660. feature 1544 (0.065000)\n",
      "661. feature 1442 (0.065000)\n",
      "662. feature 587 (0.065000)\n",
      "663. feature 201 (0.065000)\n",
      "664. feature 533 (0.065000)\n",
      "665. feature 1035 (0.065000)\n",
      "666. feature 469 (0.065000)\n",
      "667. feature 1568 (0.065000)\n",
      "668. feature 9 (0.065000)\n",
      "669. feature 1216 (0.065000)\n",
      "670. feature 123 (0.065000)\n",
      "671. feature 672 (0.065000)\n",
      "672. feature 346 (0.065000)\n",
      "673. feature 420 (0.065000)\n",
      "674. feature 1653 (0.060000)\n",
      "675. feature 652 (0.060000)\n",
      "676. feature 30 (0.060000)\n",
      "677. feature 1377 (0.060000)\n",
      "678. feature 89 (0.060000)\n",
      "679. feature 1397 (0.060000)\n",
      "680. feature 576 (0.060000)\n",
      "681. feature 1515 (0.060000)\n",
      "682. feature 1664 (0.060000)\n",
      "683. feature 1061 (0.060000)\n",
      "684. feature 291 (0.060000)\n",
      "685. feature 220 (0.060000)\n",
      "686. feature 695 (0.060000)\n",
      "687. feature 1298 (0.060000)\n",
      "688. feature 1693 (0.060000)\n",
      "689. feature 1266 (0.060000)\n",
      "690. feature 17 (0.060000)\n",
      "691. feature 1530 (0.060000)\n",
      "692. feature 1025 (0.060000)\n",
      "693. feature 841 (0.060000)\n",
      "694. feature 180 (0.060000)\n",
      "695. feature 349 (0.060000)\n",
      "696. feature 836 (0.060000)\n",
      "697. feature 1240 (0.060000)\n",
      "698. feature 190 (0.055000)\n",
      "699. feature 157 (0.055000)\n",
      "700. feature 871 (0.055000)\n",
      "701. feature 1614 (0.055000)\n",
      "702. feature 13 (0.055000)\n",
      "703. feature 777 (0.055000)\n",
      "704. feature 1479 (0.055000)\n",
      "705. feature 565 (0.055000)\n",
      "706. feature 245 (0.055000)\n",
      "707. feature 758 (0.055000)\n",
      "708. feature 762 (0.055000)\n",
      "709. feature 168 (0.055000)\n",
      "710. feature 1576 (0.055000)\n",
      "711. feature 914 (0.055000)\n",
      "712. feature 917 (0.055000)\n",
      "713. feature 831 (0.055000)\n",
      "714. feature 790 (0.055000)\n",
      "715. feature 512 (0.055000)\n",
      "716. feature 409 (0.055000)\n",
      "717. feature 421 (0.055000)\n",
      "718. feature 801 (0.055000)\n",
      "719. feature 1419 (0.055000)\n",
      "720. feature 1263 (0.055000)\n",
      "721. feature 1676 (0.055000)\n",
      "722. feature 1259 (0.055000)\n",
      "723. feature 1062 (0.055000)\n",
      "724. feature 1673 (0.050000)\n",
      "725. feature 399 (0.050000)\n",
      "726. feature 641 (0.050000)\n",
      "727. feature 232 (0.050000)\n",
      "728. feature 1575 (0.050000)\n",
      "729. feature 1690 (0.050000)\n",
      "730. feature 214 (0.050000)\n",
      "731. feature 1518 (0.050000)\n",
      "732. feature 1516 (0.050000)\n",
      "733. feature 16 (0.050000)\n",
      "734. feature 1647 (0.050000)\n",
      "735. feature 869 (0.050000)\n",
      "736. feature 1418 (0.050000)\n",
      "737. feature 187 (0.050000)\n",
      "738. feature 182 (0.050000)\n",
      "739. feature 358 (0.050000)\n",
      "740. feature 159 (0.050000)\n",
      "741. feature 1623 (0.050000)\n",
      "742. feature 1237 (0.050000)\n",
      "743. feature 1256 (0.045000)\n",
      "744. feature 371 (0.045000)\n",
      "745. feature 1688 (0.045000)\n",
      "746. feature 803 (0.045000)\n",
      "747. feature 623 (0.045000)\n",
      "748. feature 1593 (0.045000)\n",
      "749. feature 1301 (0.045000)\n",
      "750. feature 1214 (0.045000)\n",
      "751. feature 1529 (0.045000)\n",
      "752. feature 1379 (0.045000)\n",
      "753. feature 262 (0.045000)\n",
      "754. feature 1532 (0.045000)\n",
      "755. feature 248 (0.045000)\n",
      "756. feature 592 (0.045000)\n",
      "757. feature 1478 (0.045000)\n",
      "758. feature 1556 (0.045000)\n",
      "759. feature 166 (0.045000)\n",
      "760. feature 1611 (0.045000)\n",
      "761. feature 116 (0.045000)\n",
      "762. feature 361 (0.045000)\n",
      "763. feature 1361 (0.045000)\n",
      "764. feature 816 (0.045000)\n",
      "765. feature 1533 (0.045000)\n",
      "766. feature 1571 (0.045000)\n",
      "767. feature 462 (0.040000)\n",
      "768. feature 1088 (0.040000)\n",
      "769. feature 856 (0.040000)\n",
      "770. feature 224 (0.040000)\n",
      "771. feature 773 (0.040000)\n",
      "772. feature 1396 (0.040000)\n",
      "773. feature 1398 (0.040000)\n",
      "774. feature 1613 (0.040000)\n",
      "775. feature 552 (0.040000)\n",
      "776. feature 859 (0.040000)\n",
      "777. feature 1616 (0.040000)\n",
      "778. feature 1502 (0.040000)\n",
      "779. feature 293 (0.040000)\n",
      "780. feature 504 (0.040000)\n",
      "781. feature 731 (0.040000)\n",
      "782. feature 147 (0.040000)\n",
      "783. feature 998 (0.040000)\n",
      "784. feature 1365 (0.040000)\n",
      "785. feature 1563 (0.040000)\n",
      "786. feature 1362 (0.040000)\n",
      "787. feature 304 (0.040000)\n",
      "788. feature 1686 (0.040000)\n",
      "789. feature 231 (0.040000)\n",
      "790. feature 683 (0.040000)\n",
      "791. feature 751 (0.040000)\n",
      "792. feature 233 (0.040000)\n",
      "793. feature 936 (0.040000)\n",
      "794. feature 1307 (0.040000)\n",
      "795. feature 1369 (0.040000)\n",
      "796. feature 1382 (0.040000)\n",
      "797. feature 206 (0.040000)\n",
      "798. feature 1376 (0.040000)\n",
      "799. feature 896 (0.040000)\n",
      "800. feature 294 (0.035000)\n",
      "801. feature 1505 (0.035000)\n",
      "802. feature 1659 (0.035000)\n",
      "803. feature 857 (0.035000)\n",
      "804. feature 1204 (0.035000)\n",
      "805. feature 1503 (0.035000)\n",
      "806. feature 430 (0.035000)\n",
      "807. feature 1629 (0.035000)\n",
      "808. feature 374 (0.035000)\n",
      "809. feature 1630 (0.035000)\n",
      "810. feature 208 (0.035000)\n",
      "811. feature 793 (0.035000)\n",
      "812. feature 1421 (0.035000)\n",
      "813. feature 812 (0.035000)\n",
      "814. feature 1171 (0.035000)\n",
      "815. feature 1661 (0.035000)\n",
      "816. feature 253 (0.035000)\n",
      "817. feature 176 (0.035000)\n",
      "818. feature 589 (0.035000)\n",
      "819. feature 722 (0.035000)\n",
      "820. feature 1005 (0.035000)\n",
      "821. feature 490 (0.035000)\n",
      "822. feature 567 (0.035000)\n",
      "823. feature 169 (0.035000)\n",
      "824. feature 941 (0.035000)\n",
      "825. feature 938 (0.035000)\n",
      "826. feature 1306 (0.035000)\n",
      "827. feature 1608 (0.035000)\n",
      "828. feature 266 (0.035000)\n",
      "829. feature 1491 (0.035000)\n",
      "830. feature 908 (0.035000)\n",
      "831. feature 1319 (0.035000)\n",
      "832. feature 1577 (0.035000)\n",
      "833. feature 400 (0.035000)\n",
      "834. feature 155 (0.035000)\n",
      "835. feature 466 (0.035000)\n",
      "836. feature 298 (0.030000)\n",
      "837. feature 1594 (0.030000)\n",
      "838. feature 562 (0.030000)\n",
      "839. feature 375 (0.030000)\n",
      "840. feature 362 (0.030000)\n",
      "841. feature 1400 (0.030000)\n",
      "842. feature 22 (0.030000)\n",
      "843. feature 1177 (0.030000)\n",
      "844. feature 1117 (0.030000)\n",
      "845. feature 210 (0.030000)\n",
      "846. feature 211 (0.030000)\n",
      "847. feature 261 (0.030000)\n",
      "848. feature 1578 (0.030000)\n",
      "849. feature 742 (0.030000)\n",
      "850. feature 212 (0.030000)\n",
      "851. feature 521 (0.030000)\n",
      "852. feature 815 (0.030000)\n",
      "853. feature 613 (0.030000)\n",
      "854. feature 388 (0.030000)\n",
      "855. feature 1585 (0.030000)\n",
      "856. feature 560 (0.030000)\n",
      "857. feature 1639 (0.030000)\n",
      "858. feature 1463 (0.030000)\n",
      "859. feature 1276 (0.030000)\n",
      "860. feature 1634 (0.030000)\n",
      "861. feature 199 (0.030000)\n",
      "862. feature 171 (0.030000)\n",
      "863. feature 1019 (0.030000)\n",
      "864. feature 1572 (0.030000)\n",
      "865. feature 1348 (0.030000)\n",
      "866. feature 14 (0.030000)\n",
      "867. feature 12 (0.030000)\n",
      "868. feature 1495 (0.030000)\n",
      "869. feature 127 (0.030000)\n",
      "870. feature 1316 (0.030000)\n",
      "871. feature 1654 (0.030000)\n",
      "872. feature 130 (0.030000)\n",
      "873. feature 1499 (0.030000)\n",
      "874. feature 1032 (0.030000)\n",
      "875. feature 277 (0.030000)\n",
      "876. feature 788 (0.030000)\n",
      "877. feature 178 (0.030000)\n",
      "878. feature 1193 (0.030000)\n",
      "879. feature 311 (0.030000)\n",
      "880. feature 484 (0.030000)\n",
      "881. feature 689 (0.025000)\n",
      "882. feature 254 (0.025000)\n",
      "883. feature 229 (0.025000)\n",
      "884. feature 586 (0.025000)\n",
      "885. feature 544 (0.025000)\n",
      "886. feature 336 (0.025000)\n",
      "887. feature 1523 (0.025000)\n",
      "888. feature 1587 (0.025000)\n",
      "889. feature 312 (0.025000)\n",
      "890. feature 772 (0.025000)\n",
      "891. feature 317 (0.025000)\n",
      "892. feature 222 (0.025000)\n",
      "893. feature 584 (0.025000)\n",
      "894. feature 630 (0.025000)\n",
      "895. feature 1370 (0.025000)\n",
      "896. feature 427 (0.025000)\n",
      "897. feature 1473 (0.025000)\n",
      "898. feature 860 (0.025000)\n",
      "899. feature 500 (0.025000)\n",
      "900. feature 445 (0.025000)\n",
      "901. feature 1280 (0.025000)\n",
      "902. feature 392 (0.025000)\n",
      "903. feature 1138 (0.025000)\n",
      "904. feature 1390 (0.025000)\n",
      "905. feature 505 (0.025000)\n",
      "906. feature 1318 (0.025000)\n",
      "907. feature 1232 (0.025000)\n",
      "908. feature 1641 (0.025000)\n",
      "909. feature 1642 (0.025000)\n",
      "910. feature 424 (0.025000)\n",
      "911. feature 1487 (0.025000)\n",
      "912. feature 1486 (0.025000)\n",
      "913. feature 1176 (0.025000)\n",
      "914. feature 1620 (0.025000)\n",
      "915. feature 1124 (0.025000)\n",
      "916. feature 126 (0.025000)\n",
      "917. feature 207 (0.025000)\n",
      "918. feature 1349 (0.025000)\n",
      "919. feature 1326 (0.025000)\n",
      "920. feature 821 (0.025000)\n",
      "921. feature 1356 (0.025000)\n",
      "922. feature 839 (0.025000)\n",
      "923. feature 463 (0.025000)\n",
      "924. feature 481 (0.025000)\n",
      "925. feature 1368 (0.025000)\n",
      "926. feature 1460 (0.025000)\n",
      "927. feature 1438 (0.025000)\n",
      "928. feature 339 (0.025000)\n",
      "929. feature 541 (0.020000)\n",
      "930. feature 165 (0.020000)\n",
      "931. feature 468 (0.020000)\n",
      "932. feature 1480 (0.020000)\n",
      "933. feature 1203 (0.020000)\n",
      "934. feature 1173 (0.020000)\n",
      "935. feature 332 (0.020000)\n",
      "936. feature 1526 (0.020000)\n",
      "937. feature 482 (0.020000)\n",
      "938. feature 1151 (0.020000)\n",
      "939. feature 151 (0.020000)\n",
      "940. feature 1545 (0.020000)\n",
      "941. feature 485 (0.020000)\n",
      "942. feature 503 (0.020000)\n",
      "943. feature 1454 (0.020000)\n",
      "944. feature 1632 (0.020000)\n",
      "945. feature 948 (0.020000)\n",
      "946. feature 545 (0.020000)\n",
      "947. feature 716 (0.020000)\n",
      "948. feature 1626 (0.020000)\n",
      "949. feature 1148 (0.020000)\n",
      "950. feature 1537 (0.020000)\n",
      "951. feature 1551 (0.020000)\n",
      "952. feature 417 (0.020000)\n",
      "953. feature 796 (0.020000)\n",
      "954. feature 1345 (0.020000)\n",
      "955. feature 837 (0.020000)\n",
      "956. feature 840 (0.020000)\n",
      "957. feature 844 (0.020000)\n",
      "958. feature 1303 (0.020000)\n",
      "959. feature 1292 (0.020000)\n",
      "960. feature 525 (0.020000)\n",
      "961. feature 1288 (0.020000)\n",
      "962. feature 878 (0.020000)\n",
      "963. feature 189 (0.020000)\n",
      "964. feature 1494 (0.020000)\n",
      "965. feature 1287 (0.020000)\n",
      "966. feature 124 (0.020000)\n",
      "967. feature 1606 (0.020000)\n",
      "968. feature 757 (0.020000)\n",
      "969. feature 1229 (0.020000)\n",
      "970. feature 632 (0.015000)\n",
      "971. feature 1687 (0.015000)\n",
      "972. feature 274 (0.015000)\n",
      "973. feature 1211 (0.015000)\n",
      "974. feature 404 (0.015000)\n",
      "975. feature 273 (0.015000)\n",
      "976. feature 588 (0.015000)\n",
      "977. feature 1440 (0.015000)\n",
      "978. feature 1312 (0.015000)\n",
      "979. feature 1643 (0.015000)\n",
      "980. feature 1386 (0.015000)\n",
      "981. feature 464 (0.015000)\n",
      "982. feature 1200 (0.015000)\n",
      "983. feature 121 (0.015000)\n",
      "984. feature 1539 (0.015000)\n",
      "985. feature 1245 (0.015000)\n",
      "986. feature 1684 (0.015000)\n",
      "987. feature 1591 (0.015000)\n",
      "988. feature 296 (0.015000)\n",
      "989. feature 378 (0.015000)\n",
      "990. feature 665 (0.015000)\n",
      "991. feature 1228 (0.015000)\n",
      "992. feature 446 (0.015000)\n",
      "993. feature 633 (0.015000)\n",
      "994. feature 1284 (0.015000)\n",
      "995. feature 1127 (0.015000)\n",
      "996. feature 604 (0.015000)\n",
      "997. feature 607 (0.015000)\n",
      "998. feature 608 (0.015000)\n",
      "999. feature 1640 (0.015000)\n",
      "1000. feature 1638 (0.015000)\n",
      "1001. feature 461 (0.015000)\n",
      "1002. feature 802 (0.015000)\n",
      "1003. feature 128 (0.015000)\n",
      "1004. feature 511 (0.015000)\n",
      "1005. feature 977 (0.015000)\n",
      "1006. feature 161 (0.015000)\n",
      "1007. feature 736 (0.015000)\n",
      "1008. feature 959 (0.015000)\n",
      "1009. feature 956 (0.015000)\n",
      "1010. feature 1481 (0.015000)\n",
      "1011. feature 1482 (0.015000)\n",
      "1012. feature 510 (0.015000)\n",
      "1013. feature 1484 (0.015000)\n",
      "1014. feature 907 (0.015000)\n",
      "1015. feature 715 (0.015000)\n",
      "1016. feature 185 (0.015000)\n",
      "1017. feature 186 (0.015000)\n",
      "1018. feature 763 (0.015000)\n",
      "1019. feature 893 (0.015000)\n",
      "1020. feature 193 (0.015000)\n",
      "1021. feature 228 (0.015000)\n",
      "1022. feature 526 (0.015000)\n",
      "1023. feature 830 (0.015000)\n",
      "1024. feature 794 (0.015000)\n",
      "1025. feature 725 (0.015000)\n",
      "1026. feature 749 (0.015000)\n",
      "1027. feature 1074 (0.015000)\n",
      "1028. feature 359 (0.015000)\n",
      "1029. feature 1461 (0.015000)\n",
      "1030. feature 1011 (0.015000)\n",
      "1031. feature 1022 (0.015000)\n",
      "1032. feature 1459 (0.015000)\n",
      "1033. feature 569 (0.010000)\n",
      "1034. feature 1253 (0.010000)\n",
      "1035. feature 269 (0.010000)\n",
      "1036. feature 885 (0.010000)\n",
      "1037. feature 609 (0.010000)\n",
      "1038. feature 686 (0.010000)\n",
      "1039. feature 396 (0.010000)\n",
      "1040. feature 318 (0.010000)\n",
      "1041. feature 1271 (0.010000)\n",
      "1042. feature 615 (0.010000)\n",
      "1043. feature 1343 (0.010000)\n",
      "1044. feature 487 (0.010000)\n",
      "1045. feature 1534 (0.010000)\n",
      "1046. feature 1439 (0.010000)\n",
      "1047. feature 1243 (0.010000)\n",
      "1048. feature 1242 (0.010000)\n",
      "1049. feature 1359 (0.010000)\n",
      "1050. feature 1235 (0.010000)\n",
      "1051. feature 809 (0.010000)\n",
      "1052. feature 1281 (0.010000)\n",
      "1053. feature 1609 (0.010000)\n",
      "1054. feature 1282 (0.010000)\n",
      "1055. feature 401 (0.010000)\n",
      "1056. feature 1610 (0.010000)\n",
      "1057. feature 532 (0.010000)\n",
      "1058. feature 843 (0.010000)\n",
      "1059. feature 1504 (0.010000)\n",
      "1060. feature 850 (0.010000)\n",
      "1061. feature 1071 (0.010000)\n",
      "1062. feature 1389 (0.010000)\n",
      "1063. feature 1304 (0.010000)\n",
      "1064. feature 779 (0.010000)\n",
      "1065. feature 1302 (0.010000)\n",
      "1066. feature 524 (0.010000)\n",
      "1067. feature 341 (0.010000)\n",
      "1068. feature 862 (0.010000)\n",
      "1069. feature 1667 (0.010000)\n",
      "1070. feature 566 (0.010000)\n",
      "1071. feature 1427 (0.010000)\n",
      "1072. feature 1492 (0.010000)\n",
      "1073. feature 1567 (0.010000)\n",
      "1074. feature 351 (0.010000)\n",
      "1075. feature 966 (0.010000)\n",
      "1076. feature 1488 (0.010000)\n",
      "1077. feature 1404 (0.010000)\n",
      "1078. feature 249 (0.010000)\n",
      "1079. feature 1159 (0.010000)\n",
      "1080. feature 1475 (0.010000)\n",
      "1081. feature 457 (0.010000)\n",
      "1082. feature 962 (0.010000)\n",
      "1083. feature 1619 (0.010000)\n",
      "1084. feature 983 (0.010000)\n",
      "1085. feature 984 (0.010000)\n",
      "1086. feature 1524 (0.010000)\n",
      "1087. feature 719 (0.010000)\n",
      "1088. feature 379 (0.010000)\n",
      "1089. feature 1627 (0.010000)\n",
      "1090. feature 1403 (0.010000)\n",
      "1091. feature 1241 (0.010000)\n",
      "1092. feature 1432 (0.010000)\n",
      "1093. feature 1198 (0.010000)\n",
      "1094. feature 1662 (0.010000)\n",
      "1095. feature 170 (0.010000)\n",
      "1096. feature 1085 (0.010000)\n",
      "1097. feature 1433 (0.010000)\n",
      "1098. feature 935 (0.010000)\n",
      "1099. feature 1196 (0.010000)\n",
      "1100. feature 434 (0.010000)\n",
      "1101. feature 1008 (0.010000)\n",
      "1102. feature 1178 (0.010000)\n",
      "1103. feature 422 (0.005000)\n",
      "1104. feature 529 (0.005000)\n",
      "1105. feature 320 (0.005000)\n",
      "1106. feature 330 (0.005000)\n",
      "1107. feature 531 (0.005000)\n",
      "1108. feature 480 (0.005000)\n",
      "1109. feature 1566 (0.005000)\n",
      "1110. feature 548 (0.005000)\n",
      "1111. feature 476 (0.005000)\n",
      "1112. feature 340 (0.005000)\n",
      "1113. feature 381 (0.005000)\n",
      "1114. feature 499 (0.005000)\n",
      "1115. feature 1552 (0.005000)\n",
      "1116. feature 573 (0.005000)\n",
      "1117. feature 438 (0.005000)\n",
      "1118. feature 460 (0.005000)\n",
      "1119. feature 508 (0.005000)\n",
      "1120. feature 337 (0.005000)\n",
      "1121. feature 435 (0.005000)\n",
      "1122. feature 1538 (0.005000)\n",
      "1123. feature 1562 (0.005000)\n",
      "1124. feature 579 (0.005000)\n",
      "1125. feature 1218 (0.005000)\n",
      "1126. feature 1220 (0.005000)\n",
      "1127. feature 1467 (0.005000)\n",
      "1128. feature 1469 (0.005000)\n",
      "1129. feature 149 (0.005000)\n",
      "1130. feature 1221 (0.005000)\n",
      "1131. feature 1234 (0.005000)\n",
      "1132. feature 987 (0.005000)\n",
      "1133. feature 153 (0.005000)\n",
      "1134. feature 1470 (0.005000)\n",
      "1135. feature 980 (0.005000)\n",
      "1136. feature 1474 (0.005000)\n",
      "1137. feature 969 (0.005000)\n",
      "1138. feature 1669 (0.005000)\n",
      "1139. feature 1429 (0.005000)\n",
      "1140. feature 949 (0.005000)\n",
      "1141. feature 172 (0.005000)\n",
      "1142. feature 1411 (0.005000)\n",
      "1143. feature 925 (0.005000)\n",
      "1144. feature 1139 (0.005000)\n",
      "1145. feature 1490 (0.005000)\n",
      "1146. feature 1677 (0.005000)\n",
      "1147. feature 1493 (0.005000)\n",
      "1148. feature 904 (0.005000)\n",
      "1149. feature 1001 (0.005000)\n",
      "1150. feature 1410 (0.005000)\n",
      "1151. feature 1165 (0.005000)\n",
      "1152. feature 1009 (0.005000)\n",
      "1153. feature 1070 (0.005000)\n",
      "1154. feature 1068 (0.005000)\n",
      "1155. feature 1067 (0.005000)\n",
      "1156. feature 1406 (0.005000)\n",
      "1157. feature 125 (0.005000)\n",
      "1158. feature 1049 (0.005000)\n",
      "1159. feature 1092 (0.005000)\n",
      "1160. feature 1448 (0.005000)\n",
      "1161. feature 1047 (0.005000)\n",
      "1162. feature 1046 (0.005000)\n",
      "1163. feature 1449 (0.005000)\n",
      "1164. feature 1450 (0.005000)\n",
      "1165. feature 1190 (0.005000)\n",
      "1166. feature 1034 (0.005000)\n",
      "1167. feature 1033 (0.005000)\n",
      "1168. feature 1192 (0.005000)\n",
      "1169. feature 1194 (0.005000)\n",
      "1170. feature 580 (0.005000)\n",
      "1171. feature 1452 (0.005000)\n",
      "1172. feature 1407 (0.005000)\n",
      "1173. feature 140 (0.005000)\n",
      "1174. feature 1408 (0.005000)\n",
      "1175. feature 1010 (0.005000)\n",
      "1176. feature 900 (0.005000)\n",
      "1177. feature 1496 (0.005000)\n",
      "1178. feature 191 (0.005000)\n",
      "1179. feature 1255 (0.005000)\n",
      "1180. feature 1327 (0.005000)\n",
      "1181. feature 698 (0.005000)\n",
      "1182. feature 1112 (0.005000)\n",
      "1183. feature 1383 (0.005000)\n",
      "1184. feature 1336 (0.005000)\n",
      "1185. feature 270 (0.005000)\n",
      "1186. feature 271 (0.005000)\n",
      "1187. feature 275 (0.005000)\n",
      "1188. feature 674 (0.005000)\n",
      "1189. feature 278 (0.005000)\n",
      "1190. feature 286 (0.005000)\n",
      "1191. feature 653 (0.005000)\n",
      "1192. feature 1338 (0.005000)\n",
      "1193. feature 1689 (0.005000)\n",
      "1194. feature 297 (0.005000)\n",
      "1195. feature 1347 (0.005000)\n",
      "1196. feature 611 (0.005000)\n",
      "1197. feature 606 (0.005000)\n",
      "1198. feature 1355 (0.005000)\n",
      "1199. feature 1364 (0.005000)\n",
      "1200. feature 599 (0.005000)\n",
      "1201. feature 315 (0.005000)\n",
      "1202. feature 581 (0.005000)\n",
      "1203. feature 706 (0.005000)\n",
      "1204. feature 257 (0.005000)\n",
      "1205. feature 1323 (0.005000)\n",
      "1206. feature 787 (0.005000)\n",
      "1207. feature 875 (0.005000)\n",
      "1208. feature 866 (0.005000)\n",
      "1209. feature 861 (0.005000)\n",
      "1210. feature 1678 (0.005000)\n",
      "1211. feature 1260 (0.005000)\n",
      "1212. feature 851 (0.005000)\n",
      "1213. feature 1506 (0.005000)\n",
      "1214. feature 834 (0.005000)\n",
      "1215. feature 213 (0.005000)\n",
      "1216. feature 1261 (0.005000)\n",
      "1217. feature 215 (0.005000)\n",
      "1218. feature 1517 (0.005000)\n",
      "1219. feature 1320 (0.005000)\n",
      "1220. feature 1137 (0.005000)\n",
      "1221. feature 778 (0.005000)\n",
      "1222. feature 1633 (0.005000)\n",
      "1223. feature 1283 (0.005000)\n",
      "1224. feature 1291 (0.005000)\n",
      "1225. feature 244 (0.005000)\n",
      "1226. feature 1313 (0.005000)\n",
      "1227. feature 1605 (0.005000)\n",
      "1228. feature 737 (0.005000)\n",
      "1229. feature 1186 (0.005000)\n",
      "1230. feature 95 (0.000000)\n",
      "1231. feature 383 (0.000000)\n",
      "1232. feature 1670 (0.000000)\n",
      "1233. feature 73 (0.000000)\n",
      "1234. feature 1565 (0.000000)\n",
      "1235. feature 64 (0.000000)\n",
      "1236. feature 1657 (0.000000)\n",
      "1237. feature 1660 (0.000000)\n",
      "1238. feature 53 (0.000000)\n",
      "1239. feature 51 (0.000000)\n",
      "1240. feature 43 (0.000000)\n",
      "1241. feature 97 (0.000000)\n",
      "1242. feature 41 (0.000000)\n",
      "1243. feature 35 (0.000000)\n",
      "1244. feature 1679 (0.000000)\n",
      "1245. feature 1681 (0.000000)\n",
      "1246. feature 391 (0.000000)\n",
      "1247. feature 96 (0.000000)\n",
      "1248. feature 137 (0.000000)\n",
      "1249. feature 380 (0.000000)\n",
      "1250. feature 236 (0.000000)\n",
      "1251. feature 173 (0.000000)\n",
      "1252. feature 181 (0.000000)\n",
      "1253. feature 183 (0.000000)\n",
      "1254. feature 192 (0.000000)\n",
      "1255. feature 202 (0.000000)\n",
      "1256. feature 204 (0.000000)\n",
      "1257. feature 338 (0.000000)\n",
      "1258. feature 223 (0.000000)\n",
      "1259. feature 225 (0.000000)\n",
      "1260. feature 234 (0.000000)\n",
      "1261. feature 1581 (0.000000)\n",
      "1262. feature 246 (0.000000)\n",
      "1263. feature 98 (0.000000)\n",
      "1264. feature 255 (0.000000)\n",
      "1265. feature 1603 (0.000000)\n",
      "1266. feature 265 (0.000000)\n",
      "1267. feature 267 (0.000000)\n",
      "1268. feature 328 (0.000000)\n",
      "1269. feature 1600 (0.000000)\n",
      "1270. feature 288 (0.000000)\n",
      "1271. feature 1590 (0.000000)\n",
      "1272. feature 299 (0.000000)\n",
      "1273. feature 307 (0.000000)\n",
      "1274. feature 309 (0.000000)\n",
      "1275. feature 162 (0.000000)\n",
      "1276. feature 160 (0.000000)\n",
      "1277. feature 152 (0.000000)\n",
      "1278. feature 150 (0.000000)\n",
      "1279. feature 99 (0.000000)\n",
      "1280. feature 100 (0.000000)\n",
      "1281. feature 101 (0.000000)\n",
      "1282. feature 102 (0.000000)\n",
      "1283. feature 106 (0.000000)\n",
      "1284. feature 107 (0.000000)\n",
      "1285. feature 108 (0.000000)\n",
      "1286. feature 109 (0.000000)\n",
      "1287. feature 1624 (0.000000)\n",
      "1288. feature 115 (0.000000)\n",
      "1289. feature 118 (0.000000)\n",
      "1290. feature 372 (0.000000)\n",
      "1291. feature 120 (0.000000)\n",
      "1292. feature 370 (0.000000)\n",
      "1293. feature 1622 (0.000000)\n",
      "1294. feature 1621 (0.000000)\n",
      "1295. feature 136 (0.000000)\n",
      "1296. feature 138 (0.000000)\n",
      "1297. feature 139 (0.000000)\n",
      "1298. feature 141 (0.000000)\n",
      "1299. feature 144 (0.000000)\n",
      "1300. feature 145 (0.000000)\n",
      "1301. feature 146 (0.000000)\n",
      "1302. feature 194 (0.000000)\n",
      "1303. feature 685 (0.000000)\n",
      "1304. feature 393 (0.000000)\n",
      "1305. feature 1063 (0.000000)\n",
      "1306. feature 1055 (0.000000)\n",
      "1307. feature 1054 (0.000000)\n",
      "1308. feature 1053 (0.000000)\n",
      "1309. feature 1052 (0.000000)\n",
      "1310. feature 1051 (0.000000)\n",
      "1311. feature 1050 (0.000000)\n",
      "1312. feature 1044 (0.000000)\n",
      "1313. feature 1043 (0.000000)\n",
      "1314. feature 1042 (0.000000)\n",
      "1315. feature 1451 (0.000000)\n",
      "1316. feature 1031 (0.000000)\n",
      "1317. feature 1030 (0.000000)\n",
      "1318. feature 1029 (0.000000)\n",
      "1319. feature 1028 (0.000000)\n",
      "1320. feature 1026 (0.000000)\n",
      "1321. feature 1023 (0.000000)\n",
      "1322. feature 1021 (0.000000)\n",
      "1323. feature 1013 (0.000000)\n",
      "1324. feature 1012 (0.000000)\n",
      "1325. feature 1007 (0.000000)\n",
      "1326. feature 1462 (0.000000)\n",
      "1327. feature 1446 (0.000000)\n",
      "1328. feature 1065 (0.000000)\n",
      "1329. feature 881 (0.000000)\n",
      "1330. feature 1443 (0.000000)\n",
      "1331. feature 1110 (0.000000)\n",
      "1332. feature 1425 (0.000000)\n",
      "1333. feature 1107 (0.000000)\n",
      "1334. feature 1105 (0.000000)\n",
      "1335. feature 1428 (0.000000)\n",
      "1336. feature 1430 (0.000000)\n",
      "1337. feature 1431 (0.000000)\n",
      "1338. feature 1097 (0.000000)\n",
      "1339. feature 1096 (0.000000)\n",
      "1340. feature 1095 (0.000000)\n",
      "1341. feature 1094 (0.000000)\n",
      "1342. feature 1093 (0.000000)\n",
      "1343. feature 1091 (0.000000)\n",
      "1344. feature 1089 (0.000000)\n",
      "1345. feature 1086 (0.000000)\n",
      "1346. feature 1084 (0.000000)\n",
      "1347. feature 1441 (0.000000)\n",
      "1348. feature 1076 (0.000000)\n",
      "1349. feature 1075 (0.000000)\n",
      "1350. feature 1073 (0.000000)\n",
      "1351. feature 1072 (0.000000)\n",
      "1352. feature 1002 (0.000000)\n",
      "1353. feature 1000 (0.000000)\n",
      "1354. feature 1464 (0.000000)\n",
      "1355. feature 992 (0.000000)\n",
      "1356. feature 939 (0.000000)\n",
      "1357. feature 937 (0.000000)\n",
      "1358. feature 1485 (0.000000)\n",
      "1359. feature 929 (0.000000)\n",
      "1360. feature 928 (0.000000)\n",
      "1361. feature 927 (0.000000)\n",
      "1362. feature 926 (0.000000)\n",
      "1363. feature 924 (0.000000)\n",
      "1364. feature 923 (0.000000)\n",
      "1365. feature 921 (0.000000)\n",
      "1366. feature 918 (0.000000)\n",
      "1367. feature 916 (0.000000)\n",
      "1368. feature 905 (0.000000)\n",
      "1369. feature 903 (0.000000)\n",
      "1370. feature 902 (0.000000)\n",
      "1371. feature 897 (0.000000)\n",
      "1372. feature 895 (0.000000)\n",
      "1373. feature 887 (0.000000)\n",
      "1374. feature 886 (0.000000)\n",
      "1375. feature 884 (0.000000)\n",
      "1376. feature 883 (0.000000)\n",
      "1377. feature 1483 (0.000000)\n",
      "1378. feature 942 (0.000000)\n",
      "1379. feature 944 (0.000000)\n",
      "1380. feature 971 (0.000000)\n",
      "1381. feature 991 (0.000000)\n",
      "1382. feature 990 (0.000000)\n",
      "1383. feature 989 (0.000000)\n",
      "1384. feature 988 (0.000000)\n",
      "1385. feature 986 (0.000000)\n",
      "1386. feature 1471 (0.000000)\n",
      "1387. feature 981 (0.000000)\n",
      "1388. feature 979 (0.000000)\n",
      "1389. feature 1472 (0.000000)\n",
      "1390. feature 970 (0.000000)\n",
      "1391. feature 945 (0.000000)\n",
      "1392. feature 968 (0.000000)\n",
      "1393. feature 967 (0.000000)\n",
      "1394. feature 965 (0.000000)\n",
      "1395. feature 963 (0.000000)\n",
      "1396. feature 960 (0.000000)\n",
      "1397. feature 958 (0.000000)\n",
      "1398. feature 950 (0.000000)\n",
      "1399. feature 947 (0.000000)\n",
      "1400. feature 946 (0.000000)\n",
      "1401. feature 1422 (0.000000)\n",
      "1402. feature 1113 (0.000000)\n",
      "1403. feature 1114 (0.000000)\n",
      "1404. feature 1230 (0.000000)\n",
      "1405. feature 1286 (0.000000)\n",
      "1406. feature 1285 (0.000000)\n",
      "1407. feature 1278 (0.000000)\n",
      "1408. feature 1275 (0.000000)\n",
      "1409. feature 1273 (0.000000)\n",
      "1410. feature 1272 (0.000000)\n",
      "1411. feature 1270 (0.000000)\n",
      "1412. feature 1391 (0.000000)\n",
      "1413. feature 1265 (0.000000)\n",
      "1414. feature 1264 (0.000000)\n",
      "1415. feature 1262 (0.000000)\n",
      "1416. feature 1257 (0.000000)\n",
      "1417. feature 1254 (0.000000)\n",
      "1418. feature 1252 (0.000000)\n",
      "1419. feature 1251 (0.000000)\n",
      "1420. feature 1250 (0.000000)\n",
      "1421. feature 1249 (0.000000)\n",
      "1422. feature 1244 (0.000000)\n",
      "1423. feature 1239 (0.000000)\n",
      "1424. feature 1236 (0.000000)\n",
      "1425. feature 1233 (0.000000)\n",
      "1426. feature 1293 (0.000000)\n",
      "1427. feature 1294 (0.000000)\n",
      "1428. feature 1296 (0.000000)\n",
      "1429. feature 1385 (0.000000)\n",
      "1430. feature 1367 (0.000000)\n",
      "1431. feature 1366 (0.000000)\n",
      "1432. feature 1357 (0.000000)\n",
      "1433. feature 1346 (0.000000)\n",
      "1434. feature 1344 (0.000000)\n",
      "1435. feature 1378 (0.000000)\n",
      "1436. feature 1341 (0.000000)\n",
      "1437. feature 1380 (0.000000)\n",
      "1438. feature 1334 (0.000000)\n",
      "1439. feature 1328 (0.000000)\n",
      "1440. feature 1297 (0.000000)\n",
      "1441. feature 1325 (0.000000)\n",
      "1442. feature 1324 (0.000000)\n",
      "1443. feature 1322 (0.000000)\n",
      "1444. feature 1387 (0.000000)\n",
      "1445. feature 1317 (0.000000)\n",
      "1446. feature 1315 (0.000000)\n",
      "1447. feature 1314 (0.000000)\n",
      "1448. feature 1388 (0.000000)\n",
      "1449. feature 1299 (0.000000)\n",
      "1450. feature 1231 (0.000000)\n",
      "1451. feature 1223 (0.000000)\n",
      "1452. feature 1115 (0.000000)\n",
      "1453. feature 1222 (0.000000)\n",
      "1454. feature 1160 (0.000000)\n",
      "1455. feature 1158 (0.000000)\n",
      "1456. feature 1157 (0.000000)\n",
      "1457. feature 1156 (0.000000)\n",
      "1458. feature 1155 (0.000000)\n",
      "1459. feature 1154 (0.000000)\n",
      "1460. feature 1152 (0.000000)\n",
      "1461. feature 1149 (0.000000)\n",
      "1462. feature 1147 (0.000000)\n",
      "1463. feature 1409 (0.000000)\n",
      "1464. feature 1412 (0.000000)\n",
      "1465. feature 1136 (0.000000)\n",
      "1466. feature 1135 (0.000000)\n",
      "1467. feature 1134 (0.000000)\n",
      "1468. feature 1133 (0.000000)\n",
      "1469. feature 1131 (0.000000)\n",
      "1470. feature 1128 (0.000000)\n",
      "1471. feature 1126 (0.000000)\n",
      "1472. feature 1420 (0.000000)\n",
      "1473. feature 1118 (0.000000)\n",
      "1474. feature 1116 (0.000000)\n",
      "1475. feature 1166 (0.000000)\n",
      "1476. feature 1167 (0.000000)\n",
      "1477. feature 1168 (0.000000)\n",
      "1478. feature 1199 (0.000000)\n",
      "1479. feature 1215 (0.000000)\n",
      "1480. feature 1213 (0.000000)\n",
      "1481. feature 1212 (0.000000)\n",
      "1482. feature 1210 (0.000000)\n",
      "1483. feature 1209 (0.000000)\n",
      "1484. feature 1208 (0.000000)\n",
      "1485. feature 1207 (0.000000)\n",
      "1486. feature 1202 (0.000000)\n",
      "1487. feature 1201 (0.000000)\n",
      "1488. feature 1399 (0.000000)\n",
      "1489. feature 1169 (0.000000)\n",
      "1490. feature 1191 (0.000000)\n",
      "1491. feature 1189 (0.000000)\n",
      "1492. feature 1188 (0.000000)\n",
      "1493. feature 1187 (0.000000)\n",
      "1494. feature 1401 (0.000000)\n",
      "1495. feature 1181 (0.000000)\n",
      "1496. feature 1180 (0.000000)\n",
      "1497. feature 1179 (0.000000)\n",
      "1498. feature 1170 (0.000000)\n",
      "1499. feature 882 (0.000000)\n",
      "1500. feature 879 (0.000000)\n",
      "1501. feature 402 (0.000000)\n",
      "1502. feature 1536 (0.000000)\n",
      "1503. feature 570 (0.000000)\n",
      "1504. feature 564 (0.000000)\n",
      "1505. feature 561 (0.000000)\n",
      "1506. feature 559 (0.000000)\n",
      "1507. feature 558 (0.000000)\n",
      "1508. feature 557 (0.000000)\n",
      "1509. feature 556 (0.000000)\n",
      "1510. feature 551 (0.000000)\n",
      "1511. feature 549 (0.000000)\n",
      "1512. feature 546 (0.000000)\n",
      "1513. feature 543 (0.000000)\n",
      "1514. feature 540 (0.000000)\n",
      "1515. feature 539 (0.000000)\n",
      "1516. feature 538 (0.000000)\n",
      "1517. feature 537 (0.000000)\n",
      "1518. feature 536 (0.000000)\n",
      "1519. feature 535 (0.000000)\n",
      "1520. feature 530 (0.000000)\n",
      "1521. feature 528 (0.000000)\n",
      "1522. feature 527 (0.000000)\n",
      "1523. feature 523 (0.000000)\n",
      "1524. feature 572 (0.000000)\n",
      "1525. feature 577 (0.000000)\n",
      "1526. feature 876 (0.000000)\n",
      "1527. feature 578 (0.000000)\n",
      "1528. feature 629 (0.000000)\n",
      "1529. feature 628 (0.000000)\n",
      "1530. feature 627 (0.000000)\n",
      "1531. feature 624 (0.000000)\n",
      "1532. feature 622 (0.000000)\n",
      "1533. feature 621 (0.000000)\n",
      "1534. feature 619 (0.000000)\n",
      "1535. feature 1535 (0.000000)\n",
      "1536. feature 614 (0.000000)\n",
      "1537. feature 612 (0.000000)\n",
      "1538. feature 603 (0.000000)\n",
      "1539. feature 602 (0.000000)\n",
      "1540. feature 601 (0.000000)\n",
      "1541. feature 600 (0.000000)\n",
      "1542. feature 598 (0.000000)\n",
      "1543. feature 593 (0.000000)\n",
      "1544. feature 591 (0.000000)\n",
      "1545. feature 590 (0.000000)\n",
      "1546. feature 585 (0.000000)\n",
      "1547. feature 583 (0.000000)\n",
      "1548. feature 582 (0.000000)\n",
      "1549. feature 522 (0.000000)\n",
      "1550. feature 520 (0.000000)\n",
      "1551. feature 519 (0.000000)\n",
      "1552. feature 518 (0.000000)\n",
      "1553. feature 472 (0.000000)\n",
      "1554. feature 467 (0.000000)\n",
      "1555. feature 465 (0.000000)\n",
      "1556. feature 459 (0.000000)\n",
      "1557. feature 456 (0.000000)\n",
      "1558. feature 455 (0.000000)\n",
      "1559. feature 454 (0.000000)\n",
      "1560. feature 453 (0.000000)\n",
      "1561. feature 452 (0.000000)\n",
      "1562. feature 451 (0.000000)\n",
      "1563. feature 1543 (0.000000)\n",
      "1564. feature 444 (0.000000)\n",
      "1565. feature 443 (0.000000)\n",
      "1566. feature 1546 (0.000000)\n",
      "1567. feature 433 (0.000000)\n",
      "1568. feature 1548 (0.000000)\n",
      "1569. feature 425 (0.000000)\n",
      "1570. feature 423 (0.000000)\n",
      "1571. feature 414 (0.000000)\n",
      "1572. feature 412 (0.000000)\n",
      "1573. feature 1553 (0.000000)\n",
      "1574. feature 473 (0.000000)\n",
      "1575. feature 474 (0.000000)\n",
      "1576. feature 475 (0.000000)\n",
      "1577. feature 498 (0.000000)\n",
      "1578. feature 517 (0.000000)\n",
      "1579. feature 516 (0.000000)\n",
      "1580. feature 515 (0.000000)\n",
      "1581. feature 514 (0.000000)\n",
      "1582. feature 509 (0.000000)\n",
      "1583. feature 507 (0.000000)\n",
      "1584. feature 506 (0.000000)\n",
      "1585. feature 502 (0.000000)\n",
      "1586. feature 501 (0.000000)\n",
      "1587. feature 497 (0.000000)\n",
      "1588. feature 477 (0.000000)\n",
      "1589. feature 496 (0.000000)\n",
      "1590. feature 495 (0.000000)\n",
      "1591. feature 494 (0.000000)\n",
      "1592. feature 493 (0.000000)\n",
      "1593. feature 489 (0.000000)\n",
      "1594. feature 488 (0.000000)\n",
      "1595. feature 486 (0.000000)\n",
      "1596. feature 483 (0.000000)\n",
      "1597. feature 478 (0.000000)\n",
      "1598. feature 634 (0.000000)\n",
      "1599. feature 635 (0.000000)\n",
      "1600. feature 640 (0.000000)\n",
      "1601. feature 745 (0.000000)\n",
      "1602. feature 781 (0.000000)\n",
      "1603. feature 780 (0.000000)\n",
      "1604. feature 776 (0.000000)\n",
      "1605. feature 775 (0.000000)\n",
      "1606. feature 774 (0.000000)\n",
      "1607. feature 771 (0.000000)\n",
      "1608. feature 770 (0.000000)\n",
      "1609. feature 769 (0.000000)\n",
      "1610. feature 768 (0.000000)\n",
      "1611. feature 767 (0.000000)\n",
      "1612. feature 766 (0.000000)\n",
      "1613. feature 1519 (0.000000)\n",
      "1614. feature 761 (0.000000)\n",
      "1615. feature 760 (0.000000)\n",
      "1616. feature 759 (0.000000)\n",
      "1617. feature 755 (0.000000)\n",
      "1618. feature 754 (0.000000)\n",
      "1619. feature 753 (0.000000)\n",
      "1620. feature 750 (0.000000)\n",
      "1621. feature 748 (0.000000)\n",
      "1622. feature 747 (0.000000)\n",
      "1623. feature 782 (0.000000)\n",
      "1624. feature 786 (0.000000)\n",
      "1625. feature 792 (0.000000)\n",
      "1626. feature 842 (0.000000)\n",
      "1627. feature 874 (0.000000)\n",
      "1628. feature 872 (0.000000)\n",
      "1629. feature 865 (0.000000)\n",
      "1630. feature 863 (0.000000)\n",
      "1631. feature 858 (0.000000)\n",
      "1632. feature 855 (0.000000)\n",
      "1633. feature 853 (0.000000)\n",
      "1634. feature 852 (0.000000)\n",
      "1635. feature 845 (0.000000)\n",
      "1636. feature 832 (0.000000)\n",
      "1637. feature 797 (0.000000)\n",
      "1638. feature 1510 (0.000000)\n",
      "1639. feature 824 (0.000000)\n",
      "1640. feature 823 (0.000000)\n",
      "1641. feature 818 (0.000000)\n",
      "1642. feature 817 (0.000000)\n",
      "1643. feature 813 (0.000000)\n",
      "1644. feature 811 (0.000000)\n",
      "1645. feature 810 (0.000000)\n",
      "1646. feature 808 (0.000000)\n",
      "1647. feature 746 (0.000000)\n",
      "1648. feature 740 (0.000000)\n",
      "1649. feature 642 (0.000000)\n",
      "1650. feature 739 (0.000000)\n",
      "1651. feature 687 (0.000000)\n",
      "1652. feature 684 (0.000000)\n",
      "1653. feature 682 (0.000000)\n",
      "1654. feature 677 (0.000000)\n",
      "1655. feature 676 (0.000000)\n",
      "1656. feature 675 (0.000000)\n",
      "1657. feature 671 (0.000000)\n",
      "1658. feature 670 (0.000000)\n",
      "1659. feature 669 (0.000000)\n",
      "1660. feature 666 (0.000000)\n",
      "1661. feature 664 (0.000000)\n",
      "1662. feature 663 (0.000000)\n",
      "1663. feature 661 (0.000000)\n",
      "1664. feature 656 (0.000000)\n",
      "1665. feature 655 (0.000000)\n",
      "1666. feature 654 (0.000000)\n",
      "1667. feature 650 (0.000000)\n",
      "1668. feature 649 (0.000000)\n",
      "1669. feature 648 (0.000000)\n",
      "1670. feature 645 (0.000000)\n",
      "1671. feature 643 (0.000000)\n",
      "1672. feature 690 (0.000000)\n",
      "1673. feature 691 (0.000000)\n",
      "1674. feature 692 (0.000000)\n",
      "1675. feature 718 (0.000000)\n",
      "1676. feature 738 (0.000000)\n",
      "1677. feature 734 (0.000000)\n",
      "1678. feature 733 (0.000000)\n",
      "1679. feature 732 (0.000000)\n",
      "1680. feature 729 (0.000000)\n",
      "1681. feature 727 (0.000000)\n",
      "1682. feature 726 (0.000000)\n",
      "1683. feature 724 (0.000000)\n",
      "1684. feature 1525 (0.000000)\n",
      "1685. feature 717 (0.000000)\n",
      "1686. feature 696 (0.000000)\n",
      "1687. feature 713 (0.000000)\n",
      "1688. feature 712 (0.000000)\n",
      "1689. feature 711 (0.000000)\n",
      "1690. feature 708 (0.000000)\n",
      "1691. feature 705 (0.000000)\n",
      "1692. feature 704 (0.000000)\n",
      "1693. feature 703 (0.000000)\n",
      "1694. feature 1527 (0.000000)\n",
      "1695. feature 697 (0.000000)\n",
      "1696. feature 1695 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "importances = rlasso.scores_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# grid.grid_scores_\n",
    "# grid.best_estimator_.get_params()\n",
    "# predictions = grid.predict(X)\n",
    "# print(classification_report(y, predictions))\n",
    "# print(confusion_matrix(y, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdc = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=30, max_features=None, min_sample_leaf)\n",
    "rdc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " 0.11627149754552901,\n",
       " 0.01572881829659685,\n",
       " 0.013287905070717922,\n",
       " 0.003814437877377511,\n",
       " 0.014746722152514408,\n",
       " 0.012846208111377216,\n",
       " 0.01700181448735638,\n",
       " 0.008440539565972578,\n",
       " 0.01646300919568276,\n",
       " 0.006119025104474881,\n",
       " 0.012187551160899932,\n",
       " 0.017514474772539302,\n",
       " 0.007308303587712682,\n",
       " 0.0163051821156014,\n",
       " 0.012682088292643669,\n",
       " 0.011002951579630561,\n",
       " 0.009372385516908556,\n",
       " 0.016500127141716017,\n",
       " 0.031362588352020014,\n",
       " 0.013258810877037678,\n",
       " 0.017977591345531087,\n",
       " 0.004570895522388057,\n",
       " 0.011777550399038529,\n",
       " 0.01581885856079403,\n",
       " 0.012210705474717089,\n",
       " 0.012535975445642075,\n",
       " 0.01383747858472072,\n",
       " 0.037317288226583166,\n",
       " 0.01693709947786218,\n",
       " 0.009811948797861375,\n",
       " 0.031180563807597674,\n",
       " 0.014368955174830745,\n",
       " 0.016047079153030646,\n",
       " 0.012518432955841395,\n",
       " 0.01639569545047978,\n",
       " 0.01568559694024159,\n",
       " 0.016396233855471152,\n",
       " 0.009222391681396259,\n",
       " 0.015391880124392552,\n",
       " 0.0037027149004875467,\n",
       " 0.013258709852198297,\n",
       " 0.012531698373192656,\n",
       " 0.005718690868315924,\n",
       " 0.016937637882853553,\n",
       " 0.004027443303265012,\n",
       " 0.0222682070752672,\n",
       " 0.016566467813838675,\n",
       " 0.011579243278744418,\n",
       " 0.015207609783376493,\n",
       " 0.011773551073739107,\n",
       " 0.015149940281125079,\n",
       " 0.011652790253349777,\n",
       " 0.01304447012579434,\n",
       " 0.015398397076961065,\n",
       " 0.009673683219586495,\n",
       " 0.01616848518416652,\n",
       " 0.008713574148217013,\n",
       " 0.006786718410417114,\n",
       " 0.012216245388056345,\n",
       " 0.01687610827339214,\n",
       " 0.012308157563074998,\n",
       " 0.030332662461547494,\n",
       " 0.009019807488652729,\n",
       " 0.016870324137650574)"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "values, labels = zip(*Counter(rdc.feature_importances_).items())\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEk9P8BJKKrOrfHCgoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb7ee136898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [0,5,9,10,15]\n",
    "y = [0,1,2,3,4]\n",
    "plt.plot(x,y)\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=2.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features : 2\n"
     ]
    }
   ],
   "source": [
    "############################ LINEAR REGRESSION #############################\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "X = Xer(train_data, \"all\")\n",
    "y = Yer(train_label, 10).reshape(train_label.shape[0],)\n",
    "\n",
    "# parameters = {}\n",
    "\n",
    "# grid = GridSearchCV(regr, parameters, cv=2, scoring='explained_variance')\n",
    "# grid.fit(X, y)\n",
    "\n",
    "rfecv = RFECV(estimator=regr, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='neg_mean_squared_error')\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X = Xer(train_data, \"all\")\n",
    "X = np.hstack((X[:,1254:1255],X[:,1257:1258]))\n",
    "\n",
    "y = Yer(train_label, 10).reshape(train_label.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False),\n",
       "       fit_params={}, iid=True, n_jobs=1, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {}\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "grid = GridSearchCV(regr, parameters, cv=2, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ NAIVE BAYES #############################\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn import naive_bayes\n",
    "nb = naive_bayes.GaussianNB()\n",
    "\n",
    "X = Xer(train_data, \"all\")\n",
    "y = YerCutOff(train_label, 15).reshape(train_label.shape[0],)\n",
    "\n",
    "# parameters = {}\n",
    "\n",
    "# grid = GridSearchCV(regr, parameters, cv=2, scoring='explained_variance')\n",
    "# grid.fit(X, y)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "liznb = []\n",
    "\n",
    "for i in range(2,1000):\n",
    "    X_new = SelectKBest(chi2, k=i).fit_transform(X, y)\n",
    "    X_new.shape\n",
    "    \n",
    "    parameters = {}\n",
    "\n",
    "    grid = GridSearchCV(nb, parameters, cv=2, scoring='accuracy')\n",
    "    grid.fit(X_new, y)\n",
    "    \n",
    "    liznb.append(grid.grid_scores_[0].mean_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5176470588235295"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# liznb.index(max(liznb))\n",
    "max(liznb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.40      0.54       178\n",
      "        1.0       0.37      0.81      0.50        77\n",
      "\n",
      "avg / total       0.69      0.52      0.53       255\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_new = SelectKBest(chi2, k=257).fit_transform(X, y)\n",
    "X_new.shape\n",
    "\n",
    "parameters = {}\n",
    "\n",
    "grid = GridSearchCV(nb, parameters, cv=2, scoring='accuracy')\n",
    "grid.fit(X_new, y)\n",
    "\n",
    "predictions = grid.predict(X_new)\n",
    "print(classification_report(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise', estimator=GaussianNB(priors=None),\n",
       "       fit_params={}, iid=True, n_jobs=1, param_grid={},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {}\n",
    "\n",
    "grid = GridSearchCV(nb, parameters, cv=2, scoring='accuracy')\n",
    "grid.fit(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5137254901960784"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_[0].mean_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ NEURAL NET CLASSIFICATION #############################\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1)\n",
    "\n",
    "X = Xer(train_data, \"all\")\n",
    "y = YerCutOff(train_label, 15).reshape(train_label.shape[0],)\n",
    "\n",
    "# parameters = {}\n",
    "\n",
    "# grid = GridSearchCV(regr, parameters, cv=2, scoring='explained_variance')\n",
    "# grid.fit(X, y)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "liznb = []\n",
    "\n",
    "for i in range(2,1000):\n",
    "    X_new = SelectKBest(chi2, k=i).fit_transform(X, y)\n",
    "    X_new.shape\n",
    "    \n",
    "    parameters = {}\n",
    "\n",
    "    grid = GridSearchCV(clf, parameters, cv=2, scoring='accuracy')\n",
    "    grid.fit(X_new, y)\n",
    "    \n",
    "    liznb.append(grid.grid_scores_[0].mean_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ NEURAL NET REGRESSION #############################\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "nnr = MLPRegressor(solver='lbfgs', alpha=1e-5, random_state=1)\n",
    "\n",
    "X = Xer(train_data, \"all\")\n",
    "y = YerCutOff(train_label, 15).reshape(train_label.shape[0],)\n",
    "\n",
    "# parameters = {}\n",
    "\n",
    "# grid = GridSearchCV(regr, parameters, cv=2, scoring='explained_variance')\n",
    "# grid.fit(X, y)\n",
    "\n",
    "rfecv = RFECV(estimator=nnr, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy')\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IDENTIFY BEST FEATURES\n",
    "liz = list(rfecv.support_)\n",
    "liz\n",
    "lizn = [i for i, x in enumerate(liz) if x]\n",
    "\n",
    "lizn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ SVC #############################\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "X = Xer(train_data, \"all\")\n",
    "y = YerCutOff(train_label, 15).reshape(train_label.shape[0],)\n",
    "\n",
    "\n",
    "\n",
    "## SVM Classify\n",
    "svc = SVC(kernel=\"linear\")\n",
    "\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy')\n",
    "\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################ SVR #############################\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "X = Xer(train_data, \"all\")\n",
    "y = YerCutOff(train_label, 15).reshape(train_label.shape[0],)\n",
    "\n",
    "\n",
    "c_range = list(range(1, 10))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "svr = SVR()\n",
    "\n",
    "grid = GridSearchCV(svr, parameters, cv=2, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "## SVM Classify\n",
    "svc = SVC(kernel=\"linear\")\n",
    "\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy')\n",
    "\n",
    "\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "r2_score' is not a valid scoring value. Valid options are ['accuracy',\n",
    "'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', \n",
    "'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', \n",
    "'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', \n",
    "'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', \n",
    "'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score',\n",
    "'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted',\n",
    "'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -46.27092, std: 1.84876, params: {}]"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions = grid.predict(X)\n",
    "# print(classification_report(y, predictions))\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-88.29630716437244"
      ]
     },
     "execution_count": 1190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Xer(train_data, \"all\")\n",
    "y = Yer(train_label, 10).reshape(255,)\n",
    "\n",
    "c_range = list(range(1, 10))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "svr = svm.SVR()\n",
    "\n",
    "grid = GridSearchCV(svr, parameters, cv=2, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "\n",
    "grid.grid_scores_[0].mean_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -80.85133, std: 2.28793, params: {'kernel': 'linear', 'C': 1},\n",
       " mean: -45.75654, std: 2.34488, params: {'kernel': 'rbf', 'C': 1},\n",
       " mean: -45.66917, std: 2.25271, params: {'kernel': 'poly', 'C': 1},\n",
       " mean: -45.97473, std: 2.16432, params: {'kernel': 'sigmoid', 'C': 1},\n",
       " mean: -95.42148, std: 4.78920, params: {'kernel': 'linear', 'C': 2},\n",
       " mean: -46.17972, std: 2.21070, params: {'kernel': 'rbf', 'C': 2},\n",
       " mean: -45.60982, std: 2.13676, params: {'kernel': 'poly', 'C': 2},\n",
       " mean: -46.68628, std: 1.87176, params: {'kernel': 'sigmoid', 'C': 2},\n",
       " mean: -104.28943, std: 5.26622, params: {'kernel': 'linear', 'C': 3},\n",
       " mean: -46.64696, std: 2.14350, params: {'kernel': 'rbf', 'C': 3},\n",
       " mean: -45.60144, std: 2.02948, params: {'kernel': 'poly', 'C': 3},\n",
       " mean: -47.34204, std: 1.74626, params: {'kernel': 'sigmoid', 'C': 3},\n",
       " mean: -112.52939, std: 16.44643, params: {'kernel': 'linear', 'C': 4},\n",
       " mean: -47.10370, std: 2.11196, params: {'kernel': 'rbf', 'C': 4},\n",
       " mean: -45.55242, std: 1.95335, params: {'kernel': 'poly', 'C': 4},\n",
       " mean: -48.01315, std: 1.76247, params: {'kernel': 'sigmoid', 'C': 4},\n",
       " mean: -120.17037, std: 26.67124, params: {'kernel': 'linear', 'C': 5},\n",
       " mean: -47.49661, std: 2.07436, params: {'kernel': 'rbf', 'C': 5},\n",
       " mean: -45.52575, std: 1.90537, params: {'kernel': 'poly', 'C': 5},\n",
       " mean: -48.61020, std: 1.90760, params: {'kernel': 'sigmoid', 'C': 5},\n",
       " mean: -124.82039, std: 33.30400, params: {'kernel': 'linear', 'C': 6},\n",
       " mean: -47.78832, std: 2.05806, params: {'kernel': 'rbf', 'C': 6},\n",
       " mean: -45.53236, std: 1.85497, params: {'kernel': 'poly', 'C': 6},\n",
       " mean: -49.24573, std: 1.99140, params: {'kernel': 'sigmoid', 'C': 6},\n",
       " mean: -125.89658, std: 36.16282, params: {'kernel': 'linear', 'C': 7},\n",
       " mean: -48.04864, std: 2.08296, params: {'kernel': 'rbf', 'C': 7},\n",
       " mean: -45.56056, std: 1.80029, params: {'kernel': 'poly', 'C': 7},\n",
       " mean: -49.82064, std: 2.02650, params: {'kernel': 'sigmoid', 'C': 7},\n",
       " mean: -127.77979, std: 39.79371, params: {'kernel': 'linear', 'C': 8},\n",
       " mean: -48.31941, std: 2.12281, params: {'kernel': 'rbf', 'C': 8},\n",
       " mean: -45.60739, std: 1.76554, params: {'kernel': 'poly', 'C': 8},\n",
       " mean: -50.34477, std: 2.10624, params: {'kernel': 'sigmoid', 'C': 8},\n",
       " mean: -130.05790, std: 43.71114, params: {'kernel': 'linear', 'C': 9},\n",
       " mean: -48.60254, std: 2.16469, params: {'kernel': 'rbf', 'C': 9},\n",
       " mean: -45.67561, std: 1.75287, params: {'kernel': 'poly', 'C': 9},\n",
       " mean: -50.89793, std: 2.18779, params: {'kernel': 'sigmoid', 'C': 9}]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'ds' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-394-baf71eb0da36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mHyperTunerSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-394-baf71eb0da36>\u001b[0m in \u001b[0;36mHyperTunerSVM\u001b[0;34m(trainX, trainy)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    274\u001b[0m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# Heuristic to ensure user has not passed a metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36mget_scorer\u001b[0;34m(scoring)\u001b[0m\n\u001b[1;32m    234\u001b[0m             raise ValueError('%r is not a valid scoring value. '\n\u001b[1;32m    235\u001b[0m                              \u001b[0;34m'Valid options are %s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                              % (scoring, sorted(scorers)))\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'ds' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']"
     ]
    }
   ],
   "source": [
    "# HYPERTUNER WITH SVR\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "results = np.zeros((10,7))\n",
    "\n",
    "def HyperTunerSVM(trainX,trainy):\n",
    "    \n",
    "    ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"] # and \"all\"\n",
    "    phqscores = [1,2,3,4,5,6,7,8,9,10] # just for readability; 10 is sum\n",
    "    for i in range(0,len(ftypes)):\n",
    "        for j in range(0,len(phqscores)):\n",
    "            \n",
    "            X = Xer(trainX, ftypes[i])\n",
    "            y = Yer(trainy, phqscores[j]).reshape(221,)\n",
    "            \n",
    "            c_range = list(range(1, 10))\n",
    "            parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "            svr = svm.SVR()\n",
    "\n",
    "            grid = GridSearchCV(svr, parameters, cv=2, scoring='neg_mean_squared_error')\n",
    "            grid.fit(X, y)\n",
    "\n",
    "            results[j][i] = grid.grid_scores_[0].mean_validation_score\n",
    "\n",
    "            \n",
    "HyperTunerSVM(train_data, train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# ZE HYPATUNA\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "results = np.zeros((10,7))\n",
    "\n",
    "def HyperTunerSVM(trainX,trainy):\n",
    "    \n",
    "    ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"] # and \"all\"\n",
    "    phqscores = [1,2,3,4,5,6,7,8,9,10] # just for readability; 10 is sum\n",
    "    for i in range(0,len(ftypes)):\n",
    "        for j in range(0,len(phqscores)):\n",
    "            \n",
    "            X = Xer(trainX, ftypes[i])\n",
    "            y = Yer(trainy, phqscores[j]).reshape(221,)\n",
    "            \n",
    "            c_range = list(range(1, 10))\n",
    "            parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "            svc = svm.SVC()\n",
    "\n",
    "            grid = GridSearchCV(svc, parameters, cv=2, scoring='accuracy')\n",
    "            grid.fit(X, y)\n",
    "\n",
    "            results[j][i] = grid.grid_scores_[0].mean_validation_score\n",
    "\n",
    "            \n",
    "HyperTunerSVM(train_data, train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>au</th>\n",
       "      <th>ig</th>\n",
       "      <th>txt</th>\n",
       "      <th>con</th>\n",
       "      <th>tw</th>\n",
       "      <th>call</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.375566</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.257919</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.289593</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.280543</td>\n",
       "      <td>0.266968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.285068</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.262443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.312217</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.280543</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.289593</td>\n",
       "      <td>0.203620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.280543</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.375566</td>\n",
       "      <td>0.271493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.533937</td>\n",
       "      <td>0.592760</td>\n",
       "      <td>0.592760</td>\n",
       "      <td>0.592760</td>\n",
       "      <td>0.592760</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.488688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.502262</td>\n",
       "      <td>0.561086</td>\n",
       "      <td>0.561086</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.561086</td>\n",
       "      <td>0.556561</td>\n",
       "      <td>0.493213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.040724</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.067873</td>\n",
       "      <td>0.049774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         au        ig       txt       con        tw      call       all\n",
       "0  0.298643  0.343891  0.343891  0.334842  0.343891  0.343891  0.294118\n",
       "1  0.235294  0.357466  0.357466  0.357466  0.357466  0.375566  0.230769\n",
       "2  0.257919  0.294118  0.294118  0.289593  0.294118  0.280543  0.266968\n",
       "3  0.285068  0.334842  0.334842  0.307692  0.334842  0.321267  0.262443\n",
       "4  0.303167  0.330317  0.330317  0.312217  0.330317  0.298643  0.307692\n",
       "5  0.280543  0.298643  0.298643  0.303167  0.298643  0.289593  0.203620\n",
       "6  0.280543  0.357466  0.357466  0.339367  0.357466  0.375566  0.271493\n",
       "7  0.533937  0.592760  0.592760  0.592760  0.592760  0.588235  0.488688\n",
       "8  0.502262  0.561086  0.561086  0.547511  0.561086  0.556561  0.493213\n",
       "9  0.040724  0.063348  0.063348  0.063348  0.063348  0.067873  0.049774"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n",
    "df = pd.DataFrame(results, columns=[\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2cddc2b41c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mHyperTunerNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# LOG REG\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "results2 = np.zeros((10,7))\n",
    "\n",
    "def HyperTunerNB(trainX,trainy):\n",
    "    \n",
    "    ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"] # and \"all\"\n",
    "    phqscores = [1,2,3,4,5,6,7,8,9,10] # just for readability; 10 is sum\n",
    "    for i in range(0,len(ftypes)):\n",
    "        for j in range(0,len(phqscores)):\n",
    "            \n",
    "            X = Xer(trainX, ftypes[i])\n",
    "            y = Yer(trainy, phqscores[j]).reshape(221,)\n",
    "            \n",
    "            c_range = [0.001,0.01,0.1,0.2,0.3,0.5,0.7,1,2,3,4,5,10,50,100,500,1000,10000]\n",
    "            parameters = {'C':c_range}\n",
    "\n",
    "            logistic = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "\n",
    "\n",
    "            grid = GridSearchCV(logistic, parameters, cv=2, scoring='accuracy')\n",
    "            grid.fit(X, y)\n",
    "\n",
    "            results2[j][i] = grid.grid_scores_[0].mean_validation_score\n",
    "\n",
    "            \n",
    "# HyperTunerNB(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28506787, 0.33936652, 0.33936652, 0.33936652, 0.33936652,\n",
       "        0.35294118, 0.29411765],\n",
       "       [0.2760181 , 0.36199095, 0.36199095, 0.36199095, 0.36199095,\n",
       "        0.36651584, 0.26696833],\n",
       "       [0.27149321, 0.29864253, 0.29864253, 0.29864253, 0.29864253,\n",
       "        0.29411765, 0.28959276],\n",
       "       [0.30769231, 0.32126697, 0.32126697, 0.32126697, 0.32126697,\n",
       "        0.32126697, 0.29864253],\n",
       "       [0.25791855, 0.30316742, 0.30316742, 0.30316742, 0.30316742,\n",
       "        0.30769231, 0.24886878],\n",
       "       [0.35294118, 0.32579186, 0.32579186, 0.32579186, 0.32579186,\n",
       "        0.32579186, 0.3438914 ],\n",
       "       [0.30769231, 0.36651584, 0.36651584, 0.36651584, 0.36651584,\n",
       "        0.36651584, 0.29864253],\n",
       "       [0.58823529, 0.58371041, 0.58371041, 0.58371041, 0.58371041,\n",
       "        0.58371041, 0.58823529],\n",
       "       [0.54298643, 0.54751131, 0.54751131, 0.54751131, 0.54751131,\n",
       "        0.54751131, 0.54298643],\n",
       "       [0.09049774, 0.07692308, 0.07692308, 0.07692308, 0.07692308,\n",
       "        0.07692308, 0.09049774]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>au</th>\n",
       "      <th>ig</th>\n",
       "      <th>txt</th>\n",
       "      <th>con</th>\n",
       "      <th>tw</th>\n",
       "      <th>call</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.285068</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276018</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.266968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.271493</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.289593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.298643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.257919</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.248869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.343891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.298643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.542986</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.542986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.090498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         au        ig       txt       con        tw      call       all\n",
       "0  0.285068  0.339367  0.339367  0.339367  0.339367  0.352941  0.294118\n",
       "1  0.276018  0.361991  0.361991  0.361991  0.361991  0.366516  0.266968\n",
       "2  0.271493  0.298643  0.298643  0.298643  0.298643  0.294118  0.289593\n",
       "3  0.307692  0.321267  0.321267  0.321267  0.321267  0.321267  0.298643\n",
       "4  0.257919  0.303167  0.303167  0.303167  0.303167  0.307692  0.248869\n",
       "5  0.352941  0.325792  0.325792  0.325792  0.325792  0.325792  0.343891\n",
       "6  0.307692  0.366516  0.366516  0.366516  0.366516  0.366516  0.298643\n",
       "7  0.588235  0.583710  0.583710  0.583710  0.583710  0.583710  0.588235\n",
       "8  0.542986  0.547511  0.547511  0.547511  0.547511  0.547511  0.542986\n",
       "9  0.090498  0.076923  0.076923  0.076923  0.076923  0.076923  0.090498"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results2, columns=[\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## some plotting business\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "dataframe = pd.DataFrame(mtr)\n",
    "import seaborn as sns\n",
    "corr = dataframe.corr()\n",
    "\n",
    "plot = sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('iamge.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Xer(trainX, ftypes[i])\n",
    "y = Yer(trainy, phqscores[j]).reshape(221,)\n",
    "\n",
    "c_range = list(range(1, 10))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "grid = GridSearchCV(svc, parameters, cv=2, scoring='accuracy')\n",
    "grid.fit(X, y)\n",
    "\n",
    "results[j][i] = grid.grid_scores_[0].mean_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "c_range = list(range(1, 100))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "parameters['kernel']\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "grid = GridSearchCV(svc, parameters, cv=2, scoring='accuracy')\n",
    "grid.fit(X, y)\n",
    "\n",
    "grid.grid_scores_\n",
    "\n",
    "print(grid.grid_scores_[0].parameters)\n",
    "print(grid.grid_scores_[0].cv_validation_scores)\n",
    "print(grid.grid_scores_[0].mean_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save matrix as csv. \n",
    "import pandas as pd\n",
    "# np.savetxt(\"foo.csv\", g.featureMatrix , delimiter=\",\")\n",
    "\n",
    "# another way\n",
    "\n",
    "dff = pd.DataFrame(g.featureMatrix)\n",
    "dff.to_csv(\"foo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 2244)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = pd.read_csv(\"foo.csv\")\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"Q0\":\"3\",\"Q1\":\"1\",\"Q2\":\"0\",\"Q3\":\"2\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}', '{\"Q0\":\"1\",\"Q1\":\"1\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']\n",
      "[]\n",
      "['{\"Q0\":\"0\",\"Q1\":\"0\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']\n",
      "['{\"Q0\":\"2\",\"Q1\":\"1\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']\n",
      "['{\"Q0\":\"3\",\"Q1\":\"1\",\"Q2\":\"3\",\"Q3\":\"2\",\"Q4\":\"1\",\"Q5\":\"3\",\"Q6\":\"3\",\"Q7\":\"1\",\"Q8\":\"0\"}']\n",
      "['{\"Q0\":\"1\",\"Q1\":\"1\",\"Q2\":\"3\",\"Q3\":\"3\",\"Q4\":\"2\",\"Q5\":\"2\",\"Q6\":\"1\",\"Q7\":\"2\",\"Q8\":\"1\"}']\n",
      "['{\"Q0\":\"2\",\"Q1\":\"1\",\"Q2\":\"1\",\"Q3\":\"2\",\"Q4\":\"0\",\"Q5\":\"2\",\"Q6\":\"2\",\"Q7\":\"1\",\"Q8\":\"2\"}']\n"
     ]
    }
   ],
   "source": [
    "for i in l.lids()[23:30]:\n",
    "    b1 = pickle.load( open( \"datafor15:20\" + \"/DP\" + i +  \"phq\" + \".p\", \"rb\" )) \n",
    "    print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7276', '6830', '7664']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.lids()[23:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539833 is your Amazon security code.\n"
     ]
    }
   ],
   "source": [
    "b1 = pickle.load( open( \"datafor16:13\" + \"/DP\" + \"6578\" +  \"text\" + \".p\", \"rb\" )) \n",
    "\n",
    "print(json.loads(b1[0])[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('7664', 'phq', '{\"Q0\":\"0\",\"Q1\":\"0\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{\"Q0\":\"0\",\"Q1\":\"0\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apickle = []\n",
    "exampleLookup = (\"7664\", \"phq\")\n",
    "for row in c.execute('SELECT DISTINCT * FROM data WHERE id=? AND type=? ', exampleLookup):\n",
    "            \n",
    "    apickle.append(row[2])\n",
    "    \n",
    "    print(row)\n",
    "    \n",
    "apickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-04 to 2018-01-04 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-03 to 2018-01-03 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-02 to 2018-01-02 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-01 to 2018-01-01 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-31 to 2017-12-31 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-30 to 2017-12-30 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-29 to 2017-12-29 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-28 to 2017-12-28 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-27 to 2017-12-27 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-26 to 2017-12-26 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-25 to 2017-12-25 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-24 to 2017-12-24 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-23 to 2017-12-23 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-22 to 2017-12-22 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n']"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load( open( \"datafor16:13\" + \"/DP\" + \"0660\" +  \"gps\" + \".p\", \"rb\" ))\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "filedir = './datafor' + '288'\n",
    "\n",
    "## access example\n",
    "\n",
    "# instagram = pickle.load( open( filedir + \"/DP\" + str(int(l.lids()[4])) +  \"tweets\" + \".p\", \"rb\" )) \n",
    "\n",
    "a3 = pickle.load( open( filedir + \"/DP\" + '1995377' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "# a3 = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "#instagram = pickle.load( open( filedir + \"/DP\" + '11852603' +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "\n",
    "\n",
    "\n",
    "# json.loads(a3.json()[0])\n",
    "\n",
    "a3.json()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"_id\":\"4\",\"thread_id\":\"1\",\"address\":\" 15085301734\",\"person\":\"null\",\"date\":\"1460240309687\",\"date_sent\":\"0\",\"protocol\":\"null\",\"read\":\"1\",\"status\":\"-1\",\"type\":\"2\",\"reply_path_present\":\"null\",\"subject\":\"null\",\"body\":\"Your face\",\"service_center\":\"null\",\"locked\":\"0\",\"error_code\":\"0\",\"seen\":\"1\",\"deletable\":\"0\",\"sim_slot\":\"0\",\"sim_imsi\":\"null\",\"hidden\":\"0\",\"group_id\":\"null\",\"group_type\":\"null\",\"delivery_date\":\"null\",\"app_id\":\"0\",\"msg_id\":\"0\",\"callback_number\":\"null\",\"reserved\":\"0\",\"pri\":\"0\",\"teleservice_id\":\"0\",\"link_url\":\"null\",\"svc_cmd\":\"0\",\"svc_cmd_content\":\"null\",\"roam_pending\":\"0\",\"spam_report\":\"0\",\"safe_message\":\"0\",\"sub_id\":\"-1\",\"creator\":\"com.android.mms\",\"secret_mode\":\"0\",\"favorite\":\"0\",\"d_rpt_cnt\":\"0\",\"using_mode\":\"0\",\"from_address\":\"null\",\"announcements_subtype\":\"0\",\"announcements_scenario_id\":\"null\",\"device_name\":\"null\"}'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# temp = requests.get('http://depressionmqp.wpi.edu:8080/getdata?id=' + str(98945548) + '&type=' + \"text\")\n",
    "# fintemp = json.loads(temp.text)[\"data\"]\n",
    "\n",
    "# while(json.loads(temp.text)[\"nextURL\"] != ''):\n",
    "#     temp = requests.get('http://depressionmqp.wpi.edu:8080' + json.loads(temp.text)[\"nextURL\"])\n",
    "#     fintemp += json.loads(temp.text)[\"data\"]\n",
    "\n",
    "    \n",
    "fintemp[8817]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kelsey = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "\n",
    "# for fil in kelsey:\n",
    "#     print(kelsey[fil])\n",
    "\n",
    "'Valencia' in kelsey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagramMedia = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "filters = {'Lark':0,'Slumber':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "filtervec = np.ones((8,))\n",
    "\n",
    "for i in range(0,3):\n",
    "    filt = json.loads(a[i])['filter']\n",
    "    if(filt in filters):\n",
    "        filters[filt] += 1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i in range(0,8):\n",
    "    filtervec[i] = filters[list(filters)[i]]\n",
    "    \n",
    "filtervec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtervec = np.ones((8,))\n",
    "filtervec/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = embeddingToMastersum(a3)\n",
    "w.reshape((-1, 1)).T.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1511765588'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timenow  = str(int(time.time())) # for temporal congruency\n",
    "timenow1 = timenow\n",
    "timenow1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
