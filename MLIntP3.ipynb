{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is its own cell because it takes a while to load this thing\n",
    "from gensim import models\n",
    "\n",
    "# takes a little bit. increase limit at own risk.\n",
    "# model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  \n",
    "model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and list of scrape times.\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import requests\n",
    "import json\n",
    "import _pickle as pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "import cv2\n",
    "import urllib.request\n",
    "import sqlite3 as sql\n",
    "import base64\n",
    "import ffmpy3\n",
    "from file_read_backwards import FileReadBackwards\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from nltk.data import load\n",
    "import nltk\n",
    "\n",
    "\n",
    "list_of_scrape_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## class Loader\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, list_of_ids = [], list_of_times = []):\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    filedir = \"\"\n",
    "\n",
    "    ## 'file' omitted because it's not used in generating features\n",
    "    list_of_types = ['text','log','contact','calender','gps','tweets','Instagram', 'Instagram media', 'audio','phq']\n",
    "    \n",
    "    def deletdatboi(self, nombre):\n",
    "        shutil.rmtree('./datafor' + list_of_scrape_times[nombre])\n",
    "        \n",
    "    def lids(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        list_of_approved_ids = []\n",
    "        text_file = open(\"codes.txt\", \"r\")\n",
    "        list_of_approved_ids = text_file.read().split('\\n')\n",
    "        del list_of_approved_ids[-1]\n",
    "        \n",
    "        self.list_of_ids = []\n",
    "        \n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            if (row[0] in list_of_approved_ids):\n",
    "                self.list_of_ids.append(row[0])\n",
    "        \n",
    "        return self.list_of_ids\n",
    "    \n",
    "    def lits(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        self.list_of_times = []\n",
    "        \n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            self.list_of_times.append(row[1])\n",
    "        return self.list_of_times\n",
    "    \n",
    "    def downloadAndLabel(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        self.list_of_ids = []\n",
    "        \n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            self.list_of_ids.append(row[0])\n",
    "            self.list_of_times.append(row[1])\n",
    "\n",
    "        number_cols = len(self.list_of_types)\n",
    "        number_rows = len(self.list_of_ids)\n",
    "\n",
    "        ### create directory for this particular scrape/pull\n",
    "        timenow  = str(int(time.time())) # for temporal congruency\n",
    "        timereadable = datetime.datetime.fromtimestamp(int(timenow)).strftime('%H:%M')\n",
    "        list_of_scrape_times.append(timereadable)\n",
    "        os.mkdir('./datafor' + timereadable)\n",
    "        Loader.filedir = './datafor' + timereadable\n",
    "\n",
    "        # alex gave me a list of approved mturk ids\n",
    "        list_of_approved_ids = []\n",
    "        text_file = open(\"codes.txt\", \"r\")\n",
    "        list_of_approved_ids = text_file.read().split('\\n')\n",
    "        del list_of_approved_ids[-1]\n",
    "        \n",
    "        for i in range(0,number_rows):\n",
    "            for j in range(0,number_cols):\n",
    "                \n",
    "                if (self.list_of_ids[i] not in list_of_approved_ids):\n",
    "                    break\n",
    "                \n",
    "                apickle = []\n",
    "\n",
    "                exampleLookup = (self.list_of_ids[i], self.list_of_types[j])\n",
    "                for row in c.execute('SELECT DISTINCT* FROM data WHERE id=? AND type=?', exampleLookup):\n",
    "                    # the row with the ID, type, and content\n",
    "                    apickle.append(row[2])\n",
    "                    \n",
    "                    # WEIRD: duplicate ids in data table with different data\n",
    "                    if exampleLookup[1] == \"phq\" or exampleLookup[1] == \"audio\":\n",
    "                        break\n",
    "\n",
    "                # print(apickle)    \n",
    "                \n",
    "                pickle.dump(apickle, open( Loader.filedir  + \"/DP\" + self.list_of_ids[i] +  self.list_of_types[j] + \".p\", \"wb\" ))\n",
    "        \n",
    "        ## OLD CODE FOR INTERFACING THROUGH THE WEBSERVER\n",
    "        ## CODE ABOVE ACCESSES A .DB FILE LOCALLY\n",
    "        '''\n",
    "        \n",
    "        r = requests.get('http://depressionmqp.wpi.edu:8080/getids')\n",
    "        list_of_idtime = r.json()\n",
    "        \n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_ids.append( list_of_idtime[i]['id'].encode('ascii','ignore') )\n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_times.append( list_of_idtime[i]['date'] )\n",
    "\n",
    "        number_cols = len(self.list_of_types)\n",
    "        number_rows = len(self.list_of_ids)\n",
    "        \n",
    "        ### create directory for this particular scrape/pull\n",
    "        timenow  = str(int(time.time())) # for temporal congruency\n",
    "        timereadable = datetime.datetime.fromtimestamp(int(timenow)).strftime('%H:%M')\n",
    "        list_of_scrape_times.append(timereadable)\n",
    "        os.mkdir('./datafor' + timereadable)\n",
    "        Loader.filedir = './datafor' + timereadable\n",
    "\n",
    "\n",
    "        for i in range(0,number_rows):\n",
    "            for j in range(0,number_cols):\n",
    "                \n",
    "                temp = requests.get('http://depressionmqp.wpi.edu:8080/getdata?id=' + str(int(self.list_of_ids[i])) + '&type=' + self.list_of_types[j])\n",
    "                fintemp = json.loads(temp.text)[\"data\"]\n",
    "                \n",
    "                while(json.loads(temp.text)[\"nextURL\"] != ''):\n",
    "                    temp = requests.get('http://depressionmqp.wpi.edu:8080' + json.loads(temp.text)[\"nextURL\"])\n",
    "                    fintemp += json.loads(temp.text)[\"data\"]\n",
    "                \n",
    "                pickle.dump(fintemp, open( Loader.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  self.list_of_types[j] + \".p\", \"wb\" ))\n",
    "\n",
    "                # loads it into memory. i will not use this for now\n",
    "                # for sake of architectural sanity\n",
    "                # list_of_jsons[i][j] = temp\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Featurizer featurizes methods that convert json objects of the appropriate type into features\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.name = name\n",
    "\n",
    "    # takes in text pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of texts sent in the 24 hours before the scrape\n",
    "    # the last element is count of texts sent on the 24 hour window 14 days before the scrapedate\n",
    "    def textFreqVec14(self, text, scrapedate):\n",
    "        \n",
    "        textFreqVec = np.zeros((14,))\n",
    "        \n",
    "#         if(len(text)==0):\n",
    "#             textFreqVec[:] = np.nan\n",
    "#             return textFreqVec\n",
    "        \n",
    "        # moving average index\n",
    "        n = 10\n",
    "    \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # assuming unordered texts (WHICH TURNS OUT IS THE CASE)\n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+n)*mseconds_in_day)\n",
    "            for t in range(0,len(text)):\n",
    "                text_date = int(json.loads(text[t])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > text_date) and (text_date > time_during_week_lb)):\n",
    "                    textFreqVec[day] += 1.0\n",
    "                    \n",
    "        return textFreqVec\n",
    "    \n",
    "\n",
    "    # takes in call pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of calls sent in the 24 hours before the scrape\n",
    "    # the last element is count of calls sent on the 24 hour window 14 days before the scrapedate\n",
    "    def callFreqVec14(self, call, scrapedate):\n",
    "        \n",
    "        callFreqVec = np.zeros((14,))\n",
    "        \n",
    "#         if(len(call)==0):\n",
    "#             callFreqVec[:] = np.nan\n",
    "#             return callFreqVec\n",
    "\n",
    "        # moving average index\n",
    "        n = 5\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+n)*mseconds_in_day)\n",
    "            for c in range(0,len(call)):\n",
    "                call_date = int(json.loads(call[c])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > call_date) and (call_date > time_during_week_lb)):\n",
    "                    callFreqVec[day] += 1.0\n",
    "                    \n",
    "        return callFreqVec\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, return master vector\n",
    "    def embeddingToMastersum(self, tweets):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(tweets)):\n",
    "            try:\n",
    "                masterSum += self.tweetToEmbedding(tweets[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is one single tweet, returns vector embedding of entire tweet.\n",
    "    # eg: responseobject.json()[0]\n",
    "    def tweetToEmbedding(self, tweet):\n",
    "\n",
    "        q = tweet['text'].split()\n",
    "        \n",
    "        sumVector = np.zeros((300,))\n",
    "        \n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "        \n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, returns follow count\n",
    "    def followerCount(self, tweets):\n",
    "\n",
    "        followerCount = 0\n",
    "\n",
    "#         if(len(tweets)==0):\n",
    "#             followerCount = np.nan\n",
    "#             return followerCount\n",
    "        \n",
    "        # get\n",
    "        try:\n",
    "            followerCount = json.loads(tweets[0])['user']['followers_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followerCount\n",
    "\n",
    "    # input is tweets pickle, returns friend count\n",
    "    def followingCount(self, tweets):\n",
    "\n",
    "        followingCount = 0\n",
    "\n",
    "#         if(len(tweets)==0):\n",
    "#             followingCount = np.nan\n",
    "#             return followingCount\n",
    "        \n",
    "        try:\n",
    "            followingCount = json.loads(tweets[0])['user']['friends_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followingCount\n",
    "\n",
    "    # input is tweets pickle, return avg likes per post for the last 2 weeks\n",
    "    def twitterLikeFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitLikeVec = np.zeros((1,))\n",
    "        \n",
    "#         if(len(tweets)==0):\n",
    "#             twitLikeVec[:] = np.nan\n",
    "#             return twitLikeVec\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitLikeVec[0] += 1.0\n",
    "                    \n",
    "        if(twitLikeVec[0] == 0):\n",
    "            return np.zeros((1,))\n",
    "                    \n",
    "        return twitLikeVec/14\n",
    "\n",
    "\n",
    "    # input is tweets pickle, return avg retweets per post for the last 2 weeks\n",
    "    def twitterRetweetFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitRTVec = np.zeros((1,))\n",
    "        \n",
    "#         if(len(tweets)==0):\n",
    "#             twitRTVec[:] = np.nan\n",
    "#             return twitRTVec\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitRTVec[0] += json.loads(tweets[t])['favorite_count']\n",
    "                    \n",
    "        return twitRTVec/14\n",
    "    \n",
    "    # input is contacts pickle, returns number of contacts\n",
    "    def numOfContacts(self, contacts):\n",
    "        \n",
    "        return len(contacts)\n",
    "    \n",
    "    # input is instagram pickle, return two features: follows count, followed by count  \n",
    "    def instagramThings(self, instagram):\n",
    "        \n",
    "        followsFollowed = np.zeros((2,))\n",
    "        \n",
    "        try:\n",
    "            if(type(json.loads(instagram[0])) == str):\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            else: # its a dictionary like it's supposed to be\n",
    "                followsFollowed[0] = json.loads(instagram[0])['data']['counts']['follows']\n",
    "                followsFollowed[1] = json.loads(instagram[0])['data']['counts']['followed_by']\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        return followsFollowed\n",
    "    \n",
    "    # takes in instagramMedia pickle scrape date, spits out filter usage frequency for the past 2 weeks\n",
    "    def instagramFilterFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        instaFiltVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                instaFiltVec[0] += 1.0\n",
    "                    \n",
    "       \n",
    "        nofilter_count = 0\n",
    "\n",
    "        for i in range(0,int(instaFiltVec[0])):\n",
    "            if(json.loads(instagramMedia[i])['filter'] == \"Normal\"):\n",
    "                nofilter_count += 1\n",
    "        \n",
    "        instaFiltVec[0] -= nofilter_count\n",
    "        \n",
    "        return instaFiltVec\n",
    "        \n",
    "       \n",
    "    \n",
    "    # takes in instagramMedia pickle and scrape date, returns a vector that\n",
    "    # contains a normalized percentage (0-1) for the usage of the filters\n",
    "    # listed below for the past 2 weeks: \n",
    "    # 'Amaro': 0,\n",
    "    #  'Crema': 0,\n",
    "    #  'Hefe': 5,\n",
    "    #  'Inkwell': 0,\n",
    "    #  'Rise': 0,\n",
    "    #  'Valencia': 0,\n",
    "    #  'Willow': 0,\n",
    "    #  'X-Pro II': 0}\n",
    "    \n",
    "    # output example: [0.5,0,0,0,0,0.5,0,0] \n",
    "    # interpretation: user used valencia half the time, Willow the other half\n",
    "    # of time, for the past 2 weeks of Instagram posts.1`sas\n",
    "    def instagramFilterVector(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        filters = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "        filtervec = np.zeros((8,))\n",
    "        \n",
    "        numposts = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                numposts += 1\n",
    "                \n",
    "        if(numposts == 0):\n",
    "            return np.zeros((8,))\n",
    "                \n",
    "        for i in range(0,numposts):\n",
    "            filt = json.loads(instagramMedia[i])['filter']\n",
    "            if(filt in filters):\n",
    "                filters[filt] += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for i in range(0,8):\n",
    "            filtervec[i] = filters[list(filters)[i]]\n",
    "\n",
    "        # percentage of how much the filters are used as a normalized vector\n",
    "        return filtervec/(numposts)\n",
    "    \n",
    "    # takes in InstagramMedia, returns comment and like frequency for the \n",
    "    # past 2 weeks.\n",
    "    def instagramLikeComFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        counts = np.zeros((2,))\n",
    "        \n",
    "        postcount = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount += 1\n",
    "        \n",
    "        if(postcount == 0):\n",
    "            return np.zeros((2,))        \n",
    "        \n",
    "        for i in range(0,postcount):\n",
    "            counts[0] += json.loads(instagramMedia[i])['likes']['count']\n",
    "            counts[1] += json.loads(instagramMedia[i])['comments']['count']\n",
    "        \n",
    "        if (counts[0] + counts[1] == 0):\n",
    "            return np.zeros((2,))\n",
    "        else:\n",
    "            # likes per post for past 2 weeks\n",
    "            return counts/postcount\n",
    "\n",
    "\n",
    "    # takes in instagramMedia pickle and scrapedate, returns IG post \n",
    "    # frequency for the past 2 weeks\n",
    "    def instagramPostFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        postcount = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount[0] += 1.0\n",
    "                \n",
    "        return postcount/14\n",
    "        \n",
    "        \n",
    "    # takes instagramMedia, returns pixelwise average values for [H,S,V]\n",
    "    # (Hue, Saturation, Value) for all posts in the past 2 weeks, the \n",
    "    # count of faces as a frequency of faces per picture, for the past\n",
    "    # 2 weeks as well.\n",
    "    \n",
    "    # testvariable:\n",
    "    # empty string: \"\" if not testing\n",
    "    # \n",
    "    def averageHSVF(self, instagramMedia, scrapedate):\n",
    "\n",
    "\n",
    "        counts = np.zeros((2,))\n",
    "        \n",
    "        postcount = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount += 1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if(os.path.exists(\"./ILLSTOPBLINKINGSOON\")):\n",
    "            shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        os.mkdir('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        avgs = np.zeros((4,))\n",
    "\n",
    "        if(postcount == 0):\n",
    "            return avgs\n",
    "\n",
    "        for i in range(0,postcount):\n",
    "            url = json.loads(instagramMedia[i])['images']['thumbnail']['url']\n",
    "            urllib.request.urlretrieve(url, './ILLSTOPBLINKINGSOON/' + str(i) + '.jpg')\n",
    "\n",
    "        # face_cascade here is a pre trained classifier for frontal faces \n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "            \n",
    "        avgHue = 0\n",
    "        avgSatur = 0\n",
    "        avgVal = 0\n",
    "        avgFaces = 0\n",
    "\n",
    "        ## GODS OF PROGRAMMING, FORGIVE ME FOR THIS TRIPLE NEST\n",
    "\n",
    "        for k in range(0, postcount):\n",
    "            ## BGR and not RGB because imread reads in BGR\n",
    "            img = cv2.imread('./ILLSTOPBLINKINGSOON/' + str(k) + '.jpg')\n",
    "            \n",
    "            hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "            \n",
    "            avgFaces += len(faces)\n",
    "            \n",
    "            for i in range(0,hsv.shape[0]):\n",
    "                for j in range(0,hsv.shape[1]):\n",
    "                    avgHue += hsv[i,j,0]\n",
    "                    avgSatur += hsv[i,j,1]\n",
    "                    avgVal += hsv[i,j,2]\n",
    "                    \n",
    "        \n",
    "\n",
    "        sums = [avgHue,avgSatur,avgVal]\n",
    "\n",
    "        ## 22500 = 150x150 = instagram photo thumbnail shape\n",
    "        avgs = list(map(lambda x: x/(22500*postcount), sums)).append(avgFaces/postcount)\n",
    "\n",
    "        shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        return avgs      \n",
    "\n",
    "    \n",
    "    #input is texts pickle, return master vector \n",
    "    def embeddingToMastersumText(self, texts):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(texts)):\n",
    "            try:\n",
    "                masterSum += self.textToEmbedding(texts[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is single text, returns vector embedding of entire text.\n",
    "    def textToEmbedding(self, text):\n",
    "        \n",
    "        q = json.loads(text)[\"body\"].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # returns [q1,q2,q3,q4,q5,q6,q7,q8,q9] and sum of all these scores\n",
    "    def labelGenerator(self, phq):\n",
    "        \n",
    "        labelVector = np.zeros((10,))\n",
    "        sumOfScores = 0\n",
    "        \n",
    "        # print(phq)\n",
    "        \n",
    "        for i in range(0,9):\n",
    "            temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "            labelVector[i] = temp\n",
    "            labelVector[9] += temp\n",
    "            \n",
    "        return labelVector\n",
    "\n",
    "        \n",
    "    def voiceFeaturizer(self, voice):\n",
    "\n",
    "        audiofeaturevec = np.zeros((1583,))\n",
    "\n",
    "        if(len(voice) == 0):\n",
    "            return audiofeaturevec\n",
    "\n",
    "        # base64 string -> bitstring -> bitstream -> write into 3gp file\n",
    "        bytestream = base64.b64decode(voice[0])\n",
    "        fh = open(\"audio.3gp\",\"wb\")\n",
    "        fh.write(bytestream)\n",
    "        fh.close()\n",
    "\n",
    "        # wav -> 3gp\n",
    "        ff = ffmpy3.FFmpeg( inputs={'audio.3gp': None}, outputs={'audio.wav': None})\n",
    "        ff.run()\n",
    "\n",
    "        os.remove(\"audio.3gp\")\n",
    "\n",
    "        # call to openSMILE\n",
    "        os.system( os.getcwd() + '/openSMILE-2.1.0/bin/linux_x64_standalone_static/SMILExtract -C ' + os.getcwd() + '/openSMILE-2.1.0/config/emobase2010.conf -I audio.wav -O \"out.csv\"')\n",
    "\n",
    "        os.remove(\"audio.wav\")\n",
    "\n",
    "        # csv file has a giant header. last line contains the features we want\n",
    "        # so we read the last line, cut out more useless string with 'l[9:]\n",
    "        with FileReadBackwards(\"out.csv\", encoding=\"utf-8\") as frb:\n",
    "            for l in frb:\n",
    "                b = l[9:].split(',')\n",
    "                break\n",
    "\n",
    "        os.remove(\"out.csv\")\n",
    "\n",
    "        # make list of string into list of floats\n",
    "        a = list(map(float, b))\n",
    "\n",
    "        # make sklearn happy\n",
    "        for i in range(0,len(a)):\n",
    "            audiofeaturevec[i] = a[i]\n",
    "\n",
    "        return audiofeaturevec\n",
    "    \n",
    "    ## give it text pickle, gives you back a 45 element vector of how many \n",
    "    ## Parts Of Speech that person has used.\n",
    "    def POSTagger(self, text):\n",
    "\n",
    "        if(len(text) == 0):\n",
    "            return np.zeros((45,))\n",
    "\n",
    "        POSFreqVec = np.zeros((45,))\n",
    "\n",
    "        tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "        \n",
    "        newtagdic = {}\n",
    "        for key in tagdict.keys():\n",
    "            newtagdic[key] = 0\n",
    "\n",
    "        def text_process(mess):\n",
    "            # Remove all punctuation, stopwords\n",
    "            nopunc = [char for char in mess if char not in string.punctuation]\n",
    "            nopunc = ''.join(nopunc)\n",
    "            return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "        \n",
    "        for i in range(0,len(text)):\n",
    "            body = json.loads(text[i])[\"body\"]\n",
    "            q = text_process(body)\n",
    "            txt = nltk.Text(q)\n",
    "            tags = nltk.pos_tag(txt)\n",
    "\n",
    "            for tag in tags:\n",
    "                pos = tag[1]\n",
    "                newtagdic[pos] += 1\n",
    "\n",
    "        for i in range(0,len(newtagdic)):\n",
    "            POSFreqVec[i] = newtagdic[list(newtagdic)[i]]\n",
    "\n",
    "        return POSFreqVec/len(text)\n",
    "    \n",
    "    def SentAnalysis(self, text, scrapedate):\n",
    "\n",
    "        if(len(text) == 0):\n",
    "            return np.zeros((14,))\n",
    "\n",
    "        SentFreqVec = np.zeros((14,))\n",
    "\n",
    "        def text_process(mess):\n",
    "            # Remove all punctuation, stopwords\n",
    "            nopunc = [char for char in mess if char not in string.punctuation]\n",
    "            nopunc = ''.join(nopunc)\n",
    "            return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "        # moving average index\n",
    "        n = 10\n",
    "\n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "\n",
    "        # assuming unordered texts (WHICH TURNS OUT IS THE CASE)\n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+n)*mseconds_in_day)\n",
    "            for t in range(0,len(text)):\n",
    "                text_date = int(json.loads(text[t])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > text_date) and (text_date > time_during_week_lb)):\n",
    "                    body = json.loads(text[t])[\"body\"]\n",
    "                    blob = TextBlob(\" \".join(text_process(body)))\n",
    "\n",
    "                    SentFreqVec[day] += blob.sentiment.polarity\n",
    "\n",
    "        # normalize\n",
    "        return SentFreqVec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(self, filedir, list_of_ids, list_of_times):\n",
    "        self.filedir = filedir\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.f = Featurizer()\n",
    "        self.num_of_people = len(list_of_ids)\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    BigMatrix = None\n",
    "    audioless = None\n",
    "    l = Loader()\n",
    "    num_of_people = len(l.lids())\n",
    "    \n",
    "    \n",
    "    LabelMtr = None\n",
    "    AudioMtr = None\n",
    "    IGMtr = None\n",
    "    ContactsMtr = None\n",
    "    TwitterMtr = None\n",
    "    TextMtr = None\n",
    "    CallMtr = None\n",
    "    \n",
    "    \n",
    "    ### BEHOLD, FEATURE COLUMNS ###\n",
    "    \n",
    "    # call frequency\n",
    "    featureVectorCF = np.zeros((num_of_people,14))\n",
    "    # follower count\n",
    "    featureVectorFC = np.zeros((num_of_people,1))\n",
    "    # following count\n",
    "    featureVectorFC2 = np.zeros((num_of_people,1))\n",
    "    # twitter like frequency\n",
    "    featureVectorTWL = np.zeros((num_of_people,1))\n",
    "    # twitter retweet frequency\n",
    "    featureVectorTWRT = np.zeros((num_of_people,1))\n",
    "    # num of contacts\n",
    "    featureVectorNC = np.zeros((num_of_people,1))\n",
    "    # instagram follows, followed by\n",
    "    featureVectorIG1 = np.zeros((num_of_people,2))\n",
    "    # instagram filter usage freq\n",
    "    featureVectorIG2 = np.zeros((num_of_people,1))\n",
    "    # instagram filter vec: Valencia, X-Pro II, Hefe, Amaro, Rise, Willow, Crema, Inkwell\n",
    "    featureVectorIGFV = np.zeros((num_of_people,8))\n",
    "    # instagram like freq, comment freq\n",
    "    featureVectorIGLC = np.zeros((num_of_people,2))\n",
    "    # instagram post freq\n",
    "    featureVectorIG3 = np.zeros((num_of_people,1))\n",
    "    # instagram avg Hue, Saturation, Value, and total faces\n",
    "    featureVectorHSVF = np.zeros((num_of_people,4))\n",
    "    # audio features from openSMILE\n",
    "    featureVectorAUD = np.zeros((num_of_people,1583))\n",
    "    # text frequency\n",
    "    featureVectorTF = np.zeros((num_of_people,14))\n",
    "    # 45 long POS (part of speech) vector\n",
    "    POSFreqVec = np.zeros((num_of_people,45))\n",
    "    # 14 long vector, 10 day sentiment average for past 14 days\n",
    "    SentFreqVec = np.zeros((num_of_people,14))\n",
    "\n",
    "    # labels, 0-8 for corresponding phq questions, last entry is sum of them.\n",
    "    labelVector = np.zeros((num_of_people,10))\n",
    "    \n",
    "    # bag of words with word2vec (try last)\n",
    "    #featureVectorTW = np.zeros((self.num_of_people,300))\n",
    "    # bag of words with word2vec (try last)\n",
    "    #featureVectorTW2V = np.zeros((self.num_of_people,300))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def genCall(self):\n",
    "    \n",
    "        # FILLING IN THE ABOVE VECTORS\n",
    "        \n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "            ## The flow of operation is same under every title\n",
    "            ## if data file is there and has actual data -> feature\n",
    "            ## if data file is there and there is no data in it -> NaN\n",
    "            ## if data file isnt there -> NaN\n",
    "            \n",
    "            ############# Call Log ###############################\n",
    "            try:\n",
    "                a1 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"log\" + \".p\", \"rb\" ))\n",
    "\n",
    "                        \n",
    "                Generator.featureVectorCF[i] = self.f.callFreqVec14(a1, self.list_of_times[i])\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorCF[i:i+1,:] = np.nan\n",
    "                pass     \n",
    "    \n",
    "    CallMtr = featureVectorCF\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    def genText(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            ############# SMS Messages ###########################\n",
    "            try:\n",
    "                a2 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"text\" + \".p\", \"rb\" )) \n",
    "\n",
    "                Generator.POSFreqVec[i] = self.f.POSTagger(a2)\n",
    "                Generator.SentFreqVec[i] = self.f.SentAnalysis(a2, self.list_of_times[i])\n",
    "                Generator.featureVectorTF[i] = self.f.textFreqVec14(a2, self.list_of_times[i])\n",
    "                #featureVectorTW2V[i] = self.f.embeddingToMastersumText(a2)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorTF[i:i+1,:] = np.nan\n",
    "                #featureVectorTW2V[i] = np.nan\n",
    "                pass\n",
    "            \n",
    "    TextMtr = np.hstack((featureVectorTF, \n",
    "                                    SentFreqVec, \n",
    "                                    POSFreqVec))      \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def genTwitter(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            ############# Twitter ###############################\n",
    "            try:\n",
    "                a3 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"tweets\" + \".p\", \"rb\" )) \n",
    "\n",
    "            \n",
    "                #featureVectorTW[i] = self.f.embeddingToMastersum(a3)\n",
    "                Generator.featureVectorFC[i] = self.f.followerCount(a3)\n",
    "                Generator.featureVectorFC2[i] = self.f.followingCount(a3)\n",
    "                Generator.featureVectorTWL[i] = self.f.twitterLikeFreq(a3, self.list_of_times[i])\n",
    "                Generator.featureVectorTWRT[i] = self.f.twitterRetweetFreq(a3, self.list_of_times[i])\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                #featureVectorTW[i] = np.nan\n",
    "                Generator.featureVectorFC[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorFC2[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorTWL[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorTWRT[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "    TwitterMtr = np.hstack((featureVectorFC, \n",
    "                                     featureVectorFC2, \n",
    "                                     featureVectorTWL, \n",
    "                                     featureVectorTWRT))\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def genContacts(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            ############# Contacts ###############################\n",
    "            try:\n",
    "                a4 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"contact\" + \".p\", \"rb\" )) \n",
    "\n",
    "                \n",
    "                Generator.featureVectorNC[i] = self.f.numOfContacts(a4)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorNC[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "    ContactsMtr = featureVectorNC\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def genIG(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            ############# Instagram ###############################\n",
    "            try:\n",
    "                a5 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "\n",
    "            \n",
    "                Generator.featureVectorIG1[i] = self.f.instagramThings(a5)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorIG1[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "            ############# Instagram Media #########################\n",
    "            try:\n",
    "                a6 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"Instagram media\" + \".p\", \"rb\" ))\n",
    "\n",
    "                \n",
    "                Generator.featureVectorIG2[i] = self.f.instagramFilterFreq(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorIGFV[i] = self.f.instagramFilterVector(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorIGLC[i] = self.f.instagramLikeComFreq(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorIG3[i] = self.f.instagramPostFreq(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorHSVF[i] = self.f.averageHSVF(a6, self.list_of_times[i])\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorIG2[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorIGFV[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorIGLC[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorIG3[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorHSVF[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "    IGMtr = np.hstack((featureVectorIG1, \n",
    "                                     featureVectorIG2, \n",
    "                                     featureVectorIGFV,\n",
    "                                     featureVectorIGLC, \n",
    "                                     featureVectorIG3, \n",
    "                                     featureVectorHSVF))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    def genAudio(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            ############# Audio #########################\n",
    "            try:\n",
    "                a7 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"audio\" + \".p\", \"rb\" ))\n",
    "\n",
    "                \n",
    "                Generator.featureVectorAUD [i] = self.f.voiceFeaturizer(a7)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorAUD [i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "    AudioMtr = featureVectorAUD\n",
    "         \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def genLabel(self):\n",
    "        for i in range(0, self.num_of_people):\n",
    "            ############# Label #########################\n",
    "            try:\n",
    "                b1 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"phq\" + \".p\", \"rb\" )) \n",
    "\n",
    "            \n",
    "                Generator.labelVector[i] = self.f.labelGenerator(b1)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.labelVector[i:i+1,:] = np.nan\n",
    "                pass\n",
    "    \n",
    "    LabelMtr = labelVector\n",
    "    \n",
    "    \n",
    "#     BigMatrix = np.hstack((Generator.featureVectorCF, \n",
    "#                                      Generator.featureVectorTF, \n",
    "#                                      Generator.featureVectorFC, \n",
    "#                                      Generator.featureVectorTWL, \n",
    "#                                      Generator.featureVectorTWRT, \n",
    "#                                      Generator.featureVectorNC, \n",
    "#                                      Generator.featureVectorIG1, \n",
    "#                                      Generator.featureVectorIG2, \n",
    "#                                      Generator.featureVectorIGFV,\n",
    "#                                      Generator.featureVectorIGLC, \n",
    "#                                      Generator.featureVectorIG3, \n",
    "#                                      Generator.featureVectorHSVF, \n",
    "#                                      Generator.featureVectorAUD,\n",
    "#                                      Generator.POSFreqVec,\n",
    "#                                      Generator.SentFreqVec,\n",
    "#                                      Generator.labelVector))\n",
    "        \n",
    "        \n",
    "#     audioless = np.hstack((Generator.featureVectorCF, \n",
    "#                                      Generator.featureVectorTF, \n",
    "#                                      Generator.featureVectorFC, \n",
    "#                                      Generator.featureVectorTWL, \n",
    "#                                      Generator.featureVectorTWRT, \n",
    "#                                      Generator.featureVectorNC, \n",
    "#                                      Generator.featureVectorIG1, \n",
    "#                                      Generator.featureVectorIG2, \n",
    "#                                      Generator.featureVectorIGFV,\n",
    "#                                      Generator.featureVectorIGLC, \n",
    "#                                      Generator.featureVectorIG3, \n",
    "#                                      Generator.featureVectorHSVF, \n",
    "#                                      Generator.labelVector))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1240.742434501648\n"
     ]
    }
   ],
   "source": [
    "# Everything I coded in one cell!\n",
    "\n",
    "l = Loader()\n",
    "# l.downloadAndLabel()\n",
    "\n",
    "g = Generator('./datafor23:41', l.lids(), l.lits())\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "# 1-9 phq / 10 sum\n",
    "g.genLabel()\n",
    "\n",
    "# 1-1583 openSMILE\n",
    "g.genAudio() \n",
    "# 1-2 follow-follod / 3 filtfreq / 4-11 special filfreq / 12-13 like-com / 14 postfreq / 15-18 H-S-V-facecount\n",
    "g.genIG()\n",
    "# 1 num of contacts\n",
    "g.genContacts()\n",
    "# 1 followers / 2 following / 3 like freq / 4 RT freq\n",
    "g.genTwitter()\n",
    "# 14 text freq mov avg / 14 sentiment mov avg / 45 POS tags\n",
    "g.genText()\n",
    "# 14 call freq vec mov avg\n",
    "g.genCall()\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ContactsMtr = g.ContactsMtr # 1\n",
    "TwitterMtr = g.TwitterMtr # 4\n",
    "TextMtr = g.TextMtr # 73\n",
    "CallMtr = g.CallMtr # 14\n",
    "IGMtr = g.IGMtr # 18\n",
    "AudioMtr = g.AudioMtr # 1583\n",
    "LabelMtr = g.LabelMtr # 10\n",
    "\n",
    "mtr = np.hstack((ContactsMtr, \n",
    "                TwitterMtr, \n",
    "                TextMtr, \n",
    "                CallMtr, \n",
    "                IGMtr, \n",
    "                AudioMtr, \n",
    "                LabelMtr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mtr)\n",
    "df.to_csv(\"mtr.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mtr = pd.read_csv(\"mtr.csv\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  2.,  1.,  2.,  1.,  2.,  3.,  2.,  1.,  2., 16.],\n",
       "       [ 0.,  1.,  1.,  3.,  2.,  2.,  1.,  2.,  0.,  1., 13.],\n",
       "       [ 0.,  2.,  3.,  2.,  0.,  0.,  1.,  0.,  1.,  2., 11.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  2.,  1.,  2.,  0.,  1.,  0.,  0.,  0.,  7.],\n",
       "       [ 0.,  0.,  0.,  1.,  2.,  1.,  1.,  0.,  0.,  0.,  5.],\n",
       "       [ 0.,  2.,  1.,  0.,  2.,  2.,  3.,  1.,  1.,  2., 14.],\n",
       "       [ 0.,  1.,  2.,  0.,  1.,  0.,  3.,  0.,  0.,  1.,  8.],\n",
       "       [ 0.,  2.,  3.,  2.,  2.,  3.,  2.,  2.,  3.,  2., 21.],\n",
       "       [ 0.,  3.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  5.]])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtr[0:10,1694:1705]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261, 1703)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mtr = np.delete(mtr, 0, axis = 1)\n",
    "mtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  1.,  2.,  1.,  2.,  3.,  2.,  1.,  2., 16.],\n",
       "       [ 1.,  1.,  3.,  2.,  2.,  1.,  2.,  0.,  1., 13.],\n",
       "       [ 2.,  3.,  2.,  0.,  0.,  1.,  0.,  1.,  2., 11.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  1.,  2.,  0.,  1.,  0.,  0.,  0.,  7.],\n",
       "       [ 0.,  0.,  1.,  2.,  1.,  1.,  0.,  0.,  0.,  5.],\n",
       "       [ 2.,  1.,  0.,  2.,  2.,  3.,  1.,  1.,  2., 14.],\n",
       "       [ 1.,  2.,  0.,  1.,  0.,  3.,  0.,  0.,  1.,  8.],\n",
       "       [ 2.,  3.,  2.,  2.,  3.,  2.,  2.,  3.,  2., 21.],\n",
       "       [ 3.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  5.]])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtr[0:10,1693:1703]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "dataframe = pd.DataFrame(mtr)\n",
    "import seaborn as sns\n",
    "corr = dataframe.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "plot = sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "fig = plot.get_figure()\n",
    "fig.savefig('iamge.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PREPROCESSING\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# shuffle row-wise\n",
    "np.random.shuffle(mtr)\n",
    "\n",
    "data = mtr[:,0:1693]\n",
    "labels = mtr[:,1693:1703]\n",
    "\n",
    "# normalize data (features now have gauss dist., 0 mean and unit variance)\n",
    "data = sklearn.preprocessing.scale(data)\n",
    "\n",
    "# replace missing values with mean of their corresponding features\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "data = imp.fit_transform(data)\n",
    "\n",
    "# If NaNs should be dropped instead:\n",
    "# mtr = mtr[~np.isnan(mtr).any(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1., ...,  0.,  0.,  3.],\n",
       "       [ 3.,  2.,  2., ...,  2.,  2., 21.],\n",
       "       [ 2.,  3.,  2., ...,  3.,  2., 21.],\n",
       "       ...,\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  6.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  1.],\n",
       "       [ 2.,  2.,  2., ...,  2.,  2., 18.]])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA SPLIT (#nosnooping)\n",
    "\n",
    "# TEST DATA (%15 percent of data)\n",
    "numofppl_index = mtr.shape[0] - 1\n",
    "cut_index = int(mtr.shape[0] * 0.85)\n",
    "\n",
    "test_label = labels[cut_index:numofppl_index,:]\n",
    "test_data = data[cut_index:numofppl_index,:]\n",
    "\n",
    "# TRAINING AND VALIDATION DATA (%85 percent of data)\n",
    "\n",
    "train_label = labels[0:cut_index,:]\n",
    "train_data = data[0:cut_index,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 3., 2., 3., 2., 3., 3., 2., 0., 1.],\n",
       "       [0., 2., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 2., 1., 1., 0., 2., 0., 0.],\n",
       "       [0., 1., 1., 2., 2., 2., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 3., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 1., 1., 0., 0.],\n",
       "       [0., 2., 1., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [0., 0., 1., 3., 3., 3., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y = train_label[:,].reshape(221,)\n",
    "# X = train_data[:,0:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n gives nth phq answer\n",
    "# 10 gives sum of all phqs\n",
    "def Yer(y, n):\n",
    "    return y[:,n-1:n]\n",
    "\n",
    "ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"]\n",
    "\n",
    "# ftype = \"au\" audio / \"ig\" instagram / \"txt\" text / \"con\" contacts / \"tw\" twitter / \"call\" call\n",
    "# \"all\" = big matrix\n",
    "def Xer(X, ftype):\n",
    "    if(ftype == \"au\"):\n",
    "        return X[:,110:1693]\n",
    "    if(ftype == \"ig\"):\n",
    "        return X[:,92:110]\n",
    "    if(ftype == \"txt\"):\n",
    "        return X[:,5:78]\n",
    "    if(ftype == \"con\"):\n",
    "        return X[:,0:1]\n",
    "    if(ftype == \"tw\"):\n",
    "        return X[:,1:5]\n",
    "    if(ftype == \"call\"):\n",
    "        return X[:,78:92]\n",
    "    if(ftype == \"all\"):\n",
    "        return X[:,0:1693]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 3., 2., 3., 2., 3., 3., 2., 0., 1.],\n",
       "       [0., 2., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 2., 1., 1., 0., 2., 0., 0.],\n",
       "       [0., 1., 1., 2., 2., 2., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 3., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 1., 0., 1., 1., 0., 0.],\n",
       "       [0., 2., 1., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [0., 0., 1., 3., 3., 3., 1., 1., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 2., 1., 2., 2., 2., 3., 1., 1., 2.],\n",
       "       [0., 1., 1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "       [0., 2., 1., 2., 3., 1., 2., 1., 0., 2.],\n",
       "       [0., 2., 2., 3., 2., 1., 2., 1., 2., 1.],\n",
       "       [0., 2., 1., 2., 2., 2., 1., 2., 1., 1.],\n",
       "       [0., 1., 1., 3., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 2., 2., 3., 1., 2., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 2., 1., 1., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 2., 2., 0., 0., 1., 0., 0.],\n",
       "       [0., 2., 1., 2., 1., 2., 2., 2., 3., 2.],\n",
       "       [0., 2., 2., 1., 1., 1., 0., 1., 1., 2.],\n",
       "       [0., 2., 3., 2., 3., 3., 3., 3., 2., 1.],\n",
       "       [0., 2., 2., 2., 2., 3., 2., 2., 2., 2.],\n",
       "       [0., 2., 3., 3., 2., 3., 2., 1., 3., 2.],\n",
       "       [0., 1., 1., 2., 2., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 1., 1., 0., 1.],\n",
       "       [0., 2., 3., 3., 3., 3., 2., 3., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 0., 1., 1., 0., 1.],\n",
       "       [0., 1., 0., 2., 2., 2., 1., 0., 0., 1.],\n",
       "       [0., 1., 2., 2., 3., 3., 1., 2., 0., 0.],\n",
       "       [0., 2., 3., 2., 3., 2., 1., 0., 2., 2.],\n",
       "       [0., 2., 1., 3., 3., 1., 2., 1., 0., 1.],\n",
       "       [0., 0., 1., 0., 1., 0., 1., 0., 0., 0.],\n",
       "       [0., 3., 1., 3., 2., 3., 2., 3., 0., 1.],\n",
       "       [0., 0., 0., 1., 2., 1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 3., 3., 1., 2., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 0., 1., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 2., 3., 2., 1., 0., 1., 0.],\n",
       "       [0., 1., 1., 2., 2., 2., 2., 0., 0., 1.]])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0:40,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-80.85132952342374"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = Xer(train_data, \"all\")\n",
    "y = Yer(train_label, 10).reshape(221,)\n",
    "\n",
    "c_range = list(range(1, 10))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "svr = svm.SVR()\n",
    "\n",
    "grid = GridSearchCV(svr, parameters, cv=2, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, y)\n",
    "\n",
    "grid.grid_scores_[0].mean_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: -80.85133, std: 2.28793, params: {'kernel': 'linear', 'C': 1},\n",
       " mean: -45.75654, std: 2.34488, params: {'kernel': 'rbf', 'C': 1},\n",
       " mean: -45.66917, std: 2.25271, params: {'kernel': 'poly', 'C': 1},\n",
       " mean: -45.97473, std: 2.16432, params: {'kernel': 'sigmoid', 'C': 1},\n",
       " mean: -95.42148, std: 4.78920, params: {'kernel': 'linear', 'C': 2},\n",
       " mean: -46.17972, std: 2.21070, params: {'kernel': 'rbf', 'C': 2},\n",
       " mean: -45.60982, std: 2.13676, params: {'kernel': 'poly', 'C': 2},\n",
       " mean: -46.68628, std: 1.87176, params: {'kernel': 'sigmoid', 'C': 2},\n",
       " mean: -104.28943, std: 5.26622, params: {'kernel': 'linear', 'C': 3},\n",
       " mean: -46.64696, std: 2.14350, params: {'kernel': 'rbf', 'C': 3},\n",
       " mean: -45.60144, std: 2.02948, params: {'kernel': 'poly', 'C': 3},\n",
       " mean: -47.34204, std: 1.74626, params: {'kernel': 'sigmoid', 'C': 3},\n",
       " mean: -112.52939, std: 16.44643, params: {'kernel': 'linear', 'C': 4},\n",
       " mean: -47.10370, std: 2.11196, params: {'kernel': 'rbf', 'C': 4},\n",
       " mean: -45.55242, std: 1.95335, params: {'kernel': 'poly', 'C': 4},\n",
       " mean: -48.01315, std: 1.76247, params: {'kernel': 'sigmoid', 'C': 4},\n",
       " mean: -120.17037, std: 26.67124, params: {'kernel': 'linear', 'C': 5},\n",
       " mean: -47.49661, std: 2.07436, params: {'kernel': 'rbf', 'C': 5},\n",
       " mean: -45.52575, std: 1.90537, params: {'kernel': 'poly', 'C': 5},\n",
       " mean: -48.61020, std: 1.90760, params: {'kernel': 'sigmoid', 'C': 5},\n",
       " mean: -124.82039, std: 33.30400, params: {'kernel': 'linear', 'C': 6},\n",
       " mean: -47.78832, std: 2.05806, params: {'kernel': 'rbf', 'C': 6},\n",
       " mean: -45.53236, std: 1.85497, params: {'kernel': 'poly', 'C': 6},\n",
       " mean: -49.24573, std: 1.99140, params: {'kernel': 'sigmoid', 'C': 6},\n",
       " mean: -125.89658, std: 36.16282, params: {'kernel': 'linear', 'C': 7},\n",
       " mean: -48.04864, std: 2.08296, params: {'kernel': 'rbf', 'C': 7},\n",
       " mean: -45.56056, std: 1.80029, params: {'kernel': 'poly', 'C': 7},\n",
       " mean: -49.82064, std: 2.02650, params: {'kernel': 'sigmoid', 'C': 7},\n",
       " mean: -127.77979, std: 39.79371, params: {'kernel': 'linear', 'C': 8},\n",
       " mean: -48.31941, std: 2.12281, params: {'kernel': 'rbf', 'C': 8},\n",
       " mean: -45.60739, std: 1.76554, params: {'kernel': 'poly', 'C': 8},\n",
       " mean: -50.34477, std: 2.10624, params: {'kernel': 'sigmoid', 'C': 8},\n",
       " mean: -130.05790, std: 43.71114, params: {'kernel': 'linear', 'C': 9},\n",
       " mean: -48.60254, std: 2.16469, params: {'kernel': 'rbf', 'C': 9},\n",
       " mean: -45.67561, std: 1.75287, params: {'kernel': 'poly', 'C': 9},\n",
       " mean: -50.89793, std: 2.18779, params: {'kernel': 'sigmoid', 'C': 9}]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'ds' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-394-baf71eb0da36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mHyperTunerSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-394-baf71eb0da36>\u001b[0m in \u001b[0;36mHyperTunerSVM\u001b[0;34m(trainX, trainy)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorer_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    274\u001b[0m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# Heuristic to ensure user has not passed a metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36mget_scorer\u001b[0;34m(scoring)\u001b[0m\n\u001b[1;32m    234\u001b[0m             raise ValueError('%r is not a valid scoring value. '\n\u001b[1;32m    235\u001b[0m                              \u001b[0;34m'Valid options are %s'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                              % (scoring, sorted(scorers)))\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'ds' is not a valid scoring value. Valid options are ['accuracy', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision', 'completeness_score', 'explained_variance', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'fowlkes_mallows_score', 'homogeneity_score', 'mutual_info_score', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'normalized_mutual_info_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc', 'v_measure_score']"
     ]
    }
   ],
   "source": [
    "# HYPERTUNER WITH SVR\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "results = np.zeros((10,7))\n",
    "\n",
    "def HyperTunerSVM(trainX,trainy):\n",
    "    \n",
    "    ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"] # and \"all\"\n",
    "    phqscores = [1,2,3,4,5,6,7,8,9,10] # just for readability; 10 is sum\n",
    "    for i in range(0,len(ftypes)):\n",
    "        for j in range(0,len(phqscores)):\n",
    "            \n",
    "            X = Xer(trainX, ftypes[i])\n",
    "            y = Yer(trainy, phqscores[j]).reshape(221,)\n",
    "            \n",
    "            c_range = list(range(1, 10))\n",
    "            parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "            svr = svm.SVR()\n",
    "\n",
    "            grid = GridSearchCV(svr, parameters, cv=2, scoring='neg_mean_squared_error')\n",
    "            grid.fit(X, y)\n",
    "\n",
    "            results[j][i] = grid.grid_scores_[0].mean_validation_score\n",
    "\n",
    "            \n",
    "HyperTunerSVM(train_data, train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n",
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=2.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "# ZE HYPATUNA\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "results = np.zeros((10,7))\n",
    "\n",
    "def HyperTunerSVM(trainX,trainy):\n",
    "    \n",
    "    ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"] # and \"all\"\n",
    "    phqscores = [1,2,3,4,5,6,7,8,9,10] # just for readability; 10 is sum\n",
    "    for i in range(0,len(ftypes)):\n",
    "        for j in range(0,len(phqscores)):\n",
    "            \n",
    "            X = Xer(trainX, ftypes[i])\n",
    "            y = Yer(trainy, phqscores[j]).reshape(221,)\n",
    "            \n",
    "            c_range = list(range(1, 10))\n",
    "            parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "            svc = svm.SVC()\n",
    "\n",
    "            grid = GridSearchCV(svc, parameters, cv=2, scoring='accuracy')\n",
    "            grid.fit(X, y)\n",
    "\n",
    "            results[j][i] = grid.grid_scores_[0].mean_validation_score\n",
    "\n",
    "            \n",
    "HyperTunerSVM(train_data, train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>au</th>\n",
       "      <th>ig</th>\n",
       "      <th>txt</th>\n",
       "      <th>con</th>\n",
       "      <th>tw</th>\n",
       "      <th>call</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.375566</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.257919</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.289593</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.280543</td>\n",
       "      <td>0.266968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.285068</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.262443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.312217</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.280543</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.289593</td>\n",
       "      <td>0.203620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.280543</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.357466</td>\n",
       "      <td>0.375566</td>\n",
       "      <td>0.271493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.533937</td>\n",
       "      <td>0.592760</td>\n",
       "      <td>0.592760</td>\n",
       "      <td>0.592760</td>\n",
       "      <td>0.592760</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.488688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.502262</td>\n",
       "      <td>0.561086</td>\n",
       "      <td>0.561086</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.561086</td>\n",
       "      <td>0.556561</td>\n",
       "      <td>0.493213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.040724</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.067873</td>\n",
       "      <td>0.049774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         au        ig       txt       con        tw      call       all\n",
       "0  0.298643  0.343891  0.343891  0.334842  0.343891  0.343891  0.294118\n",
       "1  0.235294  0.357466  0.357466  0.357466  0.357466  0.375566  0.230769\n",
       "2  0.257919  0.294118  0.294118  0.289593  0.294118  0.280543  0.266968\n",
       "3  0.285068  0.334842  0.334842  0.307692  0.334842  0.321267  0.262443\n",
       "4  0.303167  0.330317  0.330317  0.312217  0.330317  0.298643  0.307692\n",
       "5  0.280543  0.298643  0.298643  0.303167  0.298643  0.289593  0.203620\n",
       "6  0.280543  0.357466  0.357466  0.339367  0.357466  0.375566  0.271493\n",
       "7  0.533937  0.592760  0.592760  0.592760  0.592760  0.588235  0.488688\n",
       "8  0.502262  0.561086  0.561086  0.547511  0.561086  0.556561  0.493213\n",
       "9  0.040724  0.063348  0.063348  0.063348  0.063348  0.067873  0.049774"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n",
    "df = pd.DataFrame(results, columns=[\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Maximum number of iteration must be positive; got (max_iter='4000')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-344-a21b79ac396a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mHyperTunerNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-344-a21b79ac396a>\u001b[0m in \u001b[0;36mHyperTunerNB\u001b[0;34m(trainX, trainy)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mresults2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_validation_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m         \"\"\"\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/grid_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    572\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m                 for train, test in cv)\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1673\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m             raise ValueError(\"Maximum number of iteration must be positive;\"\n\u001b[0;32m-> 1205\u001b[0;31m                              \" got (max_iter=%r)\" % self.max_iter)\n\u001b[0m\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m             raise ValueError(\"Tolerance for stopping criteria must be \"\n",
      "\u001b[0;31mValueError\u001b[0m: Maximum number of iteration must be positive; got (max_iter='4000')"
     ]
    }
   ],
   "source": [
    "# LOG REG\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "results2 = np.zeros((10,7))\n",
    "\n",
    "def HyperTunerNB(trainX,trainy):\n",
    "    \n",
    "    ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"] # and \"all\"\n",
    "    phqscores = [1,2,3,4,5,6,7,8,9,10] # just for readability; 10 is sum\n",
    "    for i in range(0,len(ftypes)):\n",
    "        for j in range(0,len(phqscores)):\n",
    "            \n",
    "            X = Xer(trainX, ftypes[i])\n",
    "            y = Yer(trainy, phqscores[j]).reshape(221,)\n",
    "            \n",
    "            c_range = [0.001,0.01,0.1,0.2,0.3,0.5,0.7,1,2,3,4,5,10,50,100,500,1000,10000]\n",
    "            parameters = {'C':c_range}\n",
    "\n",
    "            logistic = LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "\n",
    "\n",
    "            grid = GridSearchCV(logistic, parameters, cv=2, scoring='accuracy')\n",
    "            grid.fit(X, y)\n",
    "\n",
    "            results2[j][i] = grid.grid_scores_[0].mean_validation_score\n",
    "\n",
    "            \n",
    "HyperTunerNB(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28506787, 0.33936652, 0.33936652, 0.33936652, 0.33936652,\n",
       "        0.35294118, 0.29411765],\n",
       "       [0.2760181 , 0.36199095, 0.36199095, 0.36199095, 0.36199095,\n",
       "        0.36651584, 0.26696833],\n",
       "       [0.27149321, 0.29864253, 0.29864253, 0.29864253, 0.29864253,\n",
       "        0.29411765, 0.28959276],\n",
       "       [0.30769231, 0.32126697, 0.32126697, 0.32126697, 0.32126697,\n",
       "        0.32126697, 0.29864253],\n",
       "       [0.25791855, 0.30316742, 0.30316742, 0.30316742, 0.30316742,\n",
       "        0.30769231, 0.24886878],\n",
       "       [0.35294118, 0.32579186, 0.32579186, 0.32579186, 0.32579186,\n",
       "        0.32579186, 0.3438914 ],\n",
       "       [0.30769231, 0.36651584, 0.36651584, 0.36651584, 0.36651584,\n",
       "        0.36651584, 0.29864253],\n",
       "       [0.58823529, 0.58371041, 0.58371041, 0.58371041, 0.58371041,\n",
       "        0.58371041, 0.58823529],\n",
       "       [0.54298643, 0.54751131, 0.54751131, 0.54751131, 0.54751131,\n",
       "        0.54751131, 0.54298643],\n",
       "       [0.09049774, 0.07692308, 0.07692308, 0.07692308, 0.07692308,\n",
       "        0.07692308, 0.09049774]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>au</th>\n",
       "      <th>ig</th>\n",
       "      <th>txt</th>\n",
       "      <th>con</th>\n",
       "      <th>tw</th>\n",
       "      <th>call</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.285068</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.276018</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.266968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.271493</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.289593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.298643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.257919</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.248869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.343891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.366516</td>\n",
       "      <td>0.298643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.583710</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.542986</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.547511</td>\n",
       "      <td>0.542986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.090498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         au        ig       txt       con        tw      call       all\n",
       "0  0.285068  0.339367  0.339367  0.339367  0.339367  0.352941  0.294118\n",
       "1  0.276018  0.361991  0.361991  0.361991  0.361991  0.366516  0.266968\n",
       "2  0.271493  0.298643  0.298643  0.298643  0.298643  0.294118  0.289593\n",
       "3  0.307692  0.321267  0.321267  0.321267  0.321267  0.321267  0.298643\n",
       "4  0.257919  0.303167  0.303167  0.303167  0.303167  0.307692  0.248869\n",
       "5  0.352941  0.325792  0.325792  0.325792  0.325792  0.325792  0.343891\n",
       "6  0.307692  0.366516  0.366516  0.366516  0.366516  0.366516  0.298643\n",
       "7  0.588235  0.583710  0.583710  0.583710  0.583710  0.583710  0.588235\n",
       "8  0.542986  0.547511  0.547511  0.547511  0.547511  0.547511  0.542986\n",
       "9  0.090498  0.076923  0.076923  0.076923  0.076923  0.076923  0.090498"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results2, columns=[\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Xer(trainX, ftypes[i])\n",
    "y = Yer(trainy, phqscores[j]).reshape(221,)\n",
    "\n",
    "c_range = list(range(1, 10))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "grid = GridSearchCV(svc, parameters, cv=2, scoring='accuracy')\n",
    "grid.fit(X, y)\n",
    "\n",
    "results[j][i] = grid.grid_scores_[0].mean_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "c_range = list(range(1, 100))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "parameters['kernel']\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "grid = GridSearchCV(svc, parameters, cv=2, scoring='accuracy')\n",
    "grid.fit(X, y)\n",
    "\n",
    "grid.grid_scores_\n",
    "\n",
    "print(grid.grid_scores_[0].parameters)\n",
    "print(grid.grid_scores_[0].cv_validation_scores)\n",
    "print(grid.grid_scores_[0].mean_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.isfinite(y).all()\n",
    "np.isnan(X).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KNeighborsClassifer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-a52828b1b151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsClassifer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "iris = datasets.load_iris()\n",
    "# parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "# svc = svm.SVC()\n",
    "# clf = GridSearchCV(svc, parameters)\n",
    "# clf.fit(iris.data, iris.target)\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "knn = KNeighborsClassifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save matrix as csv. \n",
    "import pandas as pd\n",
    "# np.savetxt(\"foo.csv\", g.featureMatrix , delimiter=\",\")\n",
    "\n",
    "# another way\n",
    "\n",
    "dff = pd.DataFrame(g.featureMatrix)\n",
    "dff.to_csv(\"foo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 2244)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = pd.read_csv(\"foo.csv\")\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"Q0\":\"3\",\"Q1\":\"1\",\"Q2\":\"0\",\"Q3\":\"2\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}', '{\"Q0\":\"1\",\"Q1\":\"1\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']\n",
      "[]\n",
      "['{\"Q0\":\"0\",\"Q1\":\"0\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']\n",
      "['{\"Q0\":\"2\",\"Q1\":\"1\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']\n",
      "['{\"Q0\":\"3\",\"Q1\":\"1\",\"Q2\":\"3\",\"Q3\":\"2\",\"Q4\":\"1\",\"Q5\":\"3\",\"Q6\":\"3\",\"Q7\":\"1\",\"Q8\":\"0\"}']\n",
      "['{\"Q0\":\"1\",\"Q1\":\"1\",\"Q2\":\"3\",\"Q3\":\"3\",\"Q4\":\"2\",\"Q5\":\"2\",\"Q6\":\"1\",\"Q7\":\"2\",\"Q8\":\"1\"}']\n",
      "['{\"Q0\":\"2\",\"Q1\":\"1\",\"Q2\":\"1\",\"Q3\":\"2\",\"Q4\":\"0\",\"Q5\":\"2\",\"Q6\":\"2\",\"Q7\":\"1\",\"Q8\":\"2\"}']\n"
     ]
    }
   ],
   "source": [
    "for i in l.lids()[23:30]:\n",
    "    b1 = pickle.load( open( \"datafor15:20\" + \"/DP\" + i +  \"phq\" + \".p\", \"rb\" )) \n",
    "    print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7276', '6830', '7664']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.lids()[23:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539833 is your Amazon security code.\n"
     ]
    }
   ],
   "source": [
    "b1 = pickle.load( open( \"datafor16:13\" + \"/DP\" + \"6578\" +  \"text\" + \".p\", \"rb\" )) \n",
    "\n",
    "print(json.loads(b1[0])[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('7664', 'phq', '{\"Q0\":\"0\",\"Q1\":\"0\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{\"Q0\":\"0\",\"Q1\":\"0\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apickle = []\n",
    "exampleLookup = (\"7664\", \"phq\")\n",
    "for row in c.execute('SELECT DISTINCT * FROM data WHERE id=? AND type=? ', exampleLookup):\n",
    "            \n",
    "    apickle.append(row[2])\n",
    "    \n",
    "    print(row)\n",
    "    \n",
    "apickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-04 to 2018-01-04 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-03 to 2018-01-03 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-02 to 2018-01-02 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-01 to 2018-01-01 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-31 to 2017-12-31 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-30 to 2017-12-30 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-29 to 2017-12-29 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-28 to 2017-12-28 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-27 to 2017-12-27 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-26 to 2017-12-26 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-25 to 2017-12-25 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-24 to 2017-12-24 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-23 to 2017-12-23 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-22 to 2017-12-22 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n']"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load( open( \"datafor16:13\" + \"/DP\" + \"0660\" +  \"gps\" + \".p\", \"rb\" ))\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WASTELAND OF PLAY CELLS BELOW\n",
    "import pickle\n",
    "\n",
    "filedir = './datafor' + '288'\n",
    "\n",
    "## access example\n",
    "\n",
    "# instagram = pickle.load( open( filedir + \"/DP\" + str(int(l.lids()[4])) +  \"tweets\" + \".p\", \"rb\" )) \n",
    "\n",
    "a3 = pickle.load( open( filedir + \"/DP\" + '1995377' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "# a3 = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "#instagram = pickle.load( open( filedir + \"/DP\" + '11852603' +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "\n",
    "\n",
    "\n",
    "# json.loads(a3.json()[0])\n",
    "\n",
    "a3.json()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"_id\":\"4\",\"thread_id\":\"1\",\"address\":\" 15085301734\",\"person\":\"null\",\"date\":\"1460240309687\",\"date_sent\":\"0\",\"protocol\":\"null\",\"read\":\"1\",\"status\":\"-1\",\"type\":\"2\",\"reply_path_present\":\"null\",\"subject\":\"null\",\"body\":\"Your face\",\"service_center\":\"null\",\"locked\":\"0\",\"error_code\":\"0\",\"seen\":\"1\",\"deletable\":\"0\",\"sim_slot\":\"0\",\"sim_imsi\":\"null\",\"hidden\":\"0\",\"group_id\":\"null\",\"group_type\":\"null\",\"delivery_date\":\"null\",\"app_id\":\"0\",\"msg_id\":\"0\",\"callback_number\":\"null\",\"reserved\":\"0\",\"pri\":\"0\",\"teleservice_id\":\"0\",\"link_url\":\"null\",\"svc_cmd\":\"0\",\"svc_cmd_content\":\"null\",\"roam_pending\":\"0\",\"spam_report\":\"0\",\"safe_message\":\"0\",\"sub_id\":\"-1\",\"creator\":\"com.android.mms\",\"secret_mode\":\"0\",\"favorite\":\"0\",\"d_rpt_cnt\":\"0\",\"using_mode\":\"0\",\"from_address\":\"null\",\"announcements_subtype\":\"0\",\"announcements_scenario_id\":\"null\",\"device_name\":\"null\"}'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# temp = requests.get('http://depressionmqp.wpi.edu:8080/getdata?id=' + str(98945548) + '&type=' + \"text\")\n",
    "# fintemp = json.loads(temp.text)[\"data\"]\n",
    "\n",
    "# while(json.loads(temp.text)[\"nextURL\"] != ''):\n",
    "#     temp = requests.get('http://depressionmqp.wpi.edu:8080' + json.loads(temp.text)[\"nextURL\"])\n",
    "#     fintemp += json.loads(temp.text)[\"data\"]\n",
    "\n",
    "    \n",
    "fintemp[8817]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kelsey = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "\n",
    "# for fil in kelsey:\n",
    "#     print(kelsey[fil])\n",
    "\n",
    "'Valencia' in kelsey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagramMedia = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "filters = {'Lark':0,'Slumber':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "filtervec = np.ones((8,))\n",
    "\n",
    "for i in range(0,3):\n",
    "    filt = json.loads(a[i])['filter']\n",
    "    if(filt in filters):\n",
    "        filters[filt] += 1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i in range(0,8):\n",
    "    filtervec[i] = filters[list(filters)[i]]\n",
    "    \n",
    "filtervec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'JpegImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-613-3a98c835da3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'JpegImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "## TWITTER 70522438\n",
    "## IG   19671950\n",
    "\n",
    "\n",
    "# instagramMedia = pickle.load( open( filedir + \"/DP\" + '82562975' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "instagramMedia = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "\n",
    "url = json.loads(instagramMedia.json()[0])['images']['thumbnail']['url']\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtervec = np.ones((8,))\n",
    "filtervec/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = embeddingToMastersum(a3)\n",
    "w.reshape((-1, 1)).T.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1511765588'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timenow  = str(int(time.time())) # for temporal congruency\n",
    "timenow1 = timenow\n",
    "timenow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1513024716885,\n",
       " 1513026200334,\n",
       " 1513026939825,\n",
       " 1513027798716,\n",
       " 1513028059396,\n",
       " 1513028060408,\n",
       " 1513029109178,\n",
       " 1513034337824,\n",
       " 1513034463419,\n",
       " 1513035395273]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.lits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "image = cv2.imread('./faces.jpg')\n",
    "grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#faces = face_cascade.detectMultiScale(grayImage)\n",
    "faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "print(len(faces))\n",
    "\n",
    "#type(face_cascade) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datafor19:07',\n",
       " 'MLIntP3.ipynb',\n",
       " 'ILLSTOPBLINKINGSOON',\n",
       " '.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'OpenCV Shenaniganry.ipynb',\n",
       " 'README.md',\n",
       " 'haarcascade_frontalface_default.xml',\n",
       " 'faces.jpg',\n",
       " 'glove.twitter.27B.zip',\n",
       " 'MLInt.ipynb',\n",
       " 'datafor690',\n",
       " 'glove.twitter.27B.50d.txt',\n",
       " 'GoogleNews-vectors-negative300.bin']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_file_list = os.listdir(\".\")\n",
    "dir_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "\n",
    "# connect to file\n",
    "conn = sql.connect('phonedata.db')\n",
    "# create cursor for making calls to database\n",
    "c = conn.cursor()\n",
    "\n",
    "list_of_ids = []\n",
    "list_of_times = []\n",
    "\n",
    "\n",
    "for row in c.execute('SELECT * FROM ids'):\n",
    "    # one result\n",
    "    list_of_ids.append(row[0])\n",
    "    list_of_times.append(row[1])\n",
    "    # prints th id\n",
    "#     print row[0]\n",
    "#     # prints the timestamp\n",
    "#     print row[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-374-44dcd50a8971>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-374-44dcd50a8971>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    l = \"5904\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Loader' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-375-e75269d816bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Loader' has no len()"
     ]
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "a1 = pickle.load( open( \"datafor15:06\" + \"/DP\" + \"0660\" +  \"tweets\" + \".p\", \"rb\" )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# l = Loader()\n",
    "# list_of_ids = l.lids()\n",
    "# for idd in range(0,len(list_of_ids)):\n",
    "#     # print(phq)\n",
    "#     phq = pickle.load(open( \"datafor23:41\" + \"/DP\" + str(list_of_ids[idd]) +  \"phq\" + \".p\", \"rb\" ))\n",
    "#     labelVector = np.zeros((10,))\n",
    "#     sumOfScores = 0\n",
    "\n",
    "#     # print(phq)\n",
    "\n",
    "#     for i in range(0,9):\n",
    "#         print(i)\n",
    "#         temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "\n",
    "#         labelVector[i] = temp\n",
    "#         labelVector[9] += temp\n",
    "\n",
    "#     #print(labelVector[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " try:\n",
    "    exit(0)\n",
    "catch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
