{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is its own cell because it takes a while to load this thing\n",
    "from gensim import models\n",
    "\n",
    "# takes a lilbit\n",
    "# model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  \n",
    "model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and permanent stuff\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## class Loader\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, list_of_ids = [], list_of_times = []):\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    filedir = \"\"\n",
    "    #list_of_ids = []\n",
    "    #list_of_times = []\n",
    "    list_of_types = ['text','log','contact','calender','file','tweets', 'Instagram', 'Instagram media', 'phq']\n",
    "    \n",
    "    def deletdatboi(self, nombre):\n",
    "        shutil.rmtree('./datafor' + list_of_scrape_times[nombre])\n",
    "        \n",
    "    def lids(self):\n",
    "        return self.list_of_ids\n",
    "    \n",
    "    def lits(self):\n",
    "        return self.list_of_times\n",
    "    \n",
    "    def downloadAndLabel(self):\n",
    "        \n",
    "        r = requests.get('http://depressionmqp.wpi.edu:8080/getids')\n",
    "        list_of_idtime = r.json()\n",
    "        \n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_ids.append( list_of_idtime[i]['id'].encode('ascii','ignore') )\n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_times.append( list_of_idtime[i]['date'] )\n",
    "\n",
    "        number_cols = len(self.list_of_types)\n",
    "        number_rows = len(self.list_of_ids)\n",
    "        \n",
    "        # an artifact of the past\n",
    "        # list_of_jsons = [[0] * number_cols for i in range(number_rows)]\n",
    "\n",
    "        \n",
    "        ### create directory for this particular scrape/pull\n",
    "        timenow  = str(int(time.time())) # for temporal congruency\n",
    "        list_of_scrape_times.append(timenow)\n",
    "        os.mkdir('./datafor' + timenow[-3:])\n",
    "        Loader.filedir = './datafor' + timenow[-3:]\n",
    "\n",
    "\n",
    "        for i in range(0,number_rows):\n",
    "            for j in range(0,number_cols):\n",
    "                temp = requests.get('http://depressionmqp.wpi.edu:8080/getdata?id=' + str(int(self.list_of_ids[i])) + '&type=' + self.list_of_types[j])\n",
    "\n",
    "                pickle.dump(temp, open( Loader.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  self.list_of_types[j] + \".p\", \"wb\" ))\n",
    "\n",
    "                # loads it into memory. i will not use this for now\n",
    "                # for sake of architectural sanity\n",
    "                # list_of_jsons[i][j] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Featurizer featurizes methods that convert json objects of the appropriate type into features\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.name = name\n",
    "\n",
    "    # takes in element of list_of_jsons and scrape date, spits out text frequency\n",
    "    def dailyTextFreq(self, texts, scrapedate):\n",
    "        \n",
    "        a = texts.json()\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "        \n",
    "        # calculate unix time for two weeks prior\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        text_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        # find the index of the oldest message that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            text_date = int(json.loads(a[len(a)-(i+1)])['date'].encode('ascii','ignore'))\n",
    "            if (two_weeks_prior < text_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        # divide by 14 to get daily freq\n",
    "        return float(saved_index+1)/14\n",
    "    \n",
    "\n",
    "    # takes in element of list_of_jsons(resp object) and scrape date, spits out call frequency \n",
    "    def dailyCallFreq(self, calls, scrapedate):\n",
    "\n",
    "        a = calls.json()\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        call_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            call_date = int(json.loads(a[len(a)-(i+1)])['date'].encode('ascii','ignore'))\n",
    "            if (two_weeks_prior < call_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        return float(saved_index+1)/14\n",
    "    \n",
    "    \n",
    "    #input is tweets, return master vector\n",
    "    def embeddingToMastersum(self, tweets):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,10):\n",
    "            try:\n",
    "                masterSum += self.tweetToEmbedding(tweets.json()[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is tweet, returns vector embedding of entire tweet.\n",
    "    # eg: responseobject.json()[0]\n",
    "    def tweetToEmbedding(self, tweet):\n",
    "\n",
    "        q = json.loads(tweet)['text'].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    #input is tweets, return follocount\n",
    "    def followerCount(self, tweets):\n",
    "\n",
    "        followerCount = 0\n",
    "\n",
    "        # get\n",
    "        try:\n",
    "            followerCount = json.loads(tweets.json()[0])['user']['followers_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followerCount\n",
    "\n",
    "    #input is tweets, return friends (who user follows)\n",
    "    def followingCount(self, tweets):\n",
    "\n",
    "        followingCount = 0\n",
    "\n",
    "        try:\n",
    "            followingCount = json.loads(tweets.json()[0])['user']['friends_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followingCount\n",
    "\n",
    "    #input is tweets, return avg likes per post for the last 2 weeks\n",
    "    def twitterLikeFreq(self, tweets, scrapedate):\n",
    "\n",
    "        a = tweets.json()\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        tweet_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        for i in range(0,len(a)):\n",
    "\n",
    "            utc = json.loads(a[len(a)-(i+1)])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\")))\n",
    "\n",
    "            if (two_weeks_prior < tweet_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        sum_of_likes = 0\n",
    "\n",
    "        for i in range(0, saved_index):\n",
    "            sum_of_likes += json.loads(tweets.json()[i])['favorite_count']\n",
    "\n",
    "\n",
    "        return sum_of_likes/14\n",
    "\n",
    "    #input is tweets, return avg retweets per post for the last 2 weeks\n",
    "    def twitterRetweetFreq(self, tweets, scrapedate):\n",
    "\n",
    "        a = tweets.json()\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        tweet_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        for i in range(0,len(a)):\n",
    "\n",
    "            utc = json.loads(a[len(a)-(i+1)])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\")))\n",
    "\n",
    "            if (two_weeks_prior < tweet_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        sum_of_retweets = 0\n",
    "\n",
    "        for i in range(0, saved_index):\n",
    "            sum_of_likes += json.loads(tweets.json()[i])['retweet_count']\n",
    "\n",
    "\n",
    "        return sum_of_retweets/14\n",
    "    \n",
    "    # input is contacts, returns number of contacts\n",
    "    def numOfContacts(self, contacts):\n",
    "        return len(contacts.json())\n",
    "    \n",
    "    #input is instagram, return two features: follows count, followed by count  \n",
    "    def instagramThings(self, instagram):\n",
    "        \n",
    "        followsFollowed = np.zeros((2,))\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if(type(json.loads(instagram.json()[0])) == str):\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            else: # its a dictionary like it's supposed to be\n",
    "                followsFollowed[0] = json.loads(instagram.json()[0])['data']['counts']['follows']\n",
    "                followsFollowed[1] = json.loads(instagram.json()[0])['data']['counts']['followed_by']\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        return followsFollowed\n",
    "    \n",
    "    # takes in scrape date, spits out filter usage frequency \n",
    "    def instagramFilterFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia.json()\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = instagramMedia.json()[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "                    \n",
    "        negative_count = 0\n",
    "\n",
    "        for i in range(0,saved_index):\n",
    "            if(json.loads(a[i])['filter'] == Normal):\n",
    "                negative_count += 1\n",
    "\n",
    "        # filter usage freq = posts with filters count / total post count \n",
    "        return ((saved_index+1)-negative_count)/(saved_index+1)\n",
    "    \n",
    "    \n",
    "    def instagramFilterVector(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        filters = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "        filtervec = np.zeros((8,))\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia.json()\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = instagramMedia.json()[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "                    \n",
    "        for i in range(0,saved_index):\n",
    "            filt = json.loads(a[i])['filter']\n",
    "            if(filt in filters):\n",
    "                filters[filt] += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for i in range(0,8):\n",
    "            filtervec[i] = filters[list(filters)[i]]\n",
    "\n",
    "        # percentage of how much the filters are used as a normalized vector\n",
    "        return filtervec/(saved_index+1)\n",
    "    \n",
    "    def instagramLikeComFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        counts = np.zeros((2,))\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia.json()\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = instagramMedia.json()[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "                \n",
    "        for i in range(0,saved_index):\n",
    "            counts[0] += json.loads(instagramMedia.json()[i])['likes']['count']\n",
    "            counts[1] += json.loads(instagramMedia.json()[i])['comments']['count']\n",
    "        \n",
    "            \n",
    "        # likes per post for past 2 weeks\n",
    "        if (not(counts[0] != 0 and counts[1] != 0)):\n",
    "            return np.zeros((2,))\n",
    "        else:\n",
    "            return counts/saved_index\n",
    "\n",
    "    # IG post frequency for the past 2 weeks\n",
    "    def instagramPostFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia.json()\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = instagramMedia.json()[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "                \n",
    "        return (saved_index+1)/14\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(self, filedir, list_of_ids, list_of_times):\n",
    "        self.filedir = filedir\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.f = Featurizer()\n",
    "        self.num_of_people = len(list_of_ids)\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    featureMatrix = None\n",
    "    \n",
    "    def generateMatrix(self):\n",
    "\n",
    "        featureVectorCF = np.zeros((self.num_of_people,1))\n",
    "        featureVectorTF = np.zeros((self.num_of_people,1))\n",
    "        featureVectorTW = np.zeros((self.num_of_people,300))\n",
    "        featureVectorFC = np.zeros((self.num_of_people,1))\n",
    "        featureVectorFC2 = np.zeros((self.num_of_people,1))\n",
    "        featureVectorTWL = np.zeros((self.num_of_people,1))\n",
    "        featureVectorTWRT = np.zeros((self.num_of_people,1))\n",
    "        featureVectorNC = np.zeros((self.num_of_people,1))\n",
    "        featureVectorIG1 = np.zeros((self.num_of_people,2))\n",
    "        featureVectorIG2 = np.zeros((self.num_of_people,1))\n",
    "        featureVectorIGFV = np.zeros((self.num_of_people,8))\n",
    "        featureVectorIGLC = np.zeros((self.num_of_people,2))\n",
    "        featureVectorIG3 = np.zeros((self.num_of_people,1))\n",
    "        \n",
    "        for i in range(0, self.num_of_people):\n",
    "\n",
    "            #list_of_jsons[i][1]\n",
    "\n",
    "            a1 = pickle.load( open( self.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  \"log\" + \".p\", \"rb\" )) \n",
    "            a2 = pickle.load( open( self.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  \"text\" + \".p\", \"rb\" )) \n",
    "            a3 = pickle.load( open( self.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  \"tweets\" + \".p\", \"rb\" )) \n",
    "            a4 = pickle.load( open( self.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  \"contact\" + \".p\", \"rb\" )) \n",
    "            a5 = pickle.load( open( self.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "            a6 = pickle.load( open( self.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "            \n",
    "            # print(self.list_of_ids[i])\n",
    "            # print(i)\n",
    "            \n",
    "            featureVectorCF[i] = self.f.dailyCallFreq(a1, self.list_of_times[i])\n",
    "            featureVectorTF[i] = self.f.dailyTextFreq(a2, self.list_of_times[i])\n",
    "            featureVectorTW[i] = self.f.embeddingToMastersum(a3)\n",
    "            featureVectorFC[i] = self.f.followerCount(a3)\n",
    "            featureVectorFC2[i] = self.f.followingCount(a3)\n",
    "            featureVectorTWL[i] = self.f.twitterLikeFreq(a3, self.list_of_times[i])\n",
    "            featureVectorTWRT[i] = self.f.twitterRetweetFreq(a3, self.list_of_times[i])\n",
    "            featureVectorNC[i] = self.f.numOfContacts(a4)\n",
    "            featureVectorIG1[i] = self.f.instagramThings(a5)\n",
    "            featureVectorIG2[i] = self.f.instagramFilterFreq(a6, self.list_of_times[i])\n",
    "            featureVectorIGFV[i] = self.f.instagramFilterVector(a6, self.list_of_times[i])\n",
    "            featureVectorIGLC[i] = self.f.instagramLikeComFreq(a6, self.list_of_times[i])\n",
    "            instagramPostFreq\n",
    "            \n",
    "\n",
    "            Generator.featureMatrix = np.hstack((featureVectorCF,featureVectorTF,\n",
    "featureVectorTW, featureVectorFC, featureVectorTWL, featureVectorTWRT,\n",
    "featureVectorNC,featureVectorIG1, featureVectorIG2, featureVectorIGFV,\n",
    "featureVectorIGLC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/ipykernel_launcher.py:319: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAMN YOU DAMON!!(ig)\n",
      "DAMN YOU DAMON!!(ig)\n",
      "DAMN YOU DAMON!!(ig)\n",
      "DAMN YOU DAMON!!(ig)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(140, 319)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything I coded in one cell!\n",
    "\n",
    "# l = Loader()\n",
    "# l.downloadAndLabel()\n",
    "\n",
    "g = Generator(l.filedir, l.lids(), l.lits())\n",
    "g.generateMatrix()\n",
    "g.featureMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WASTELAND OF PLAY CELLS BELOW\n",
    "\n",
    "\n",
    "filedir = './datafor' + '288'\n",
    "\n",
    "## access example\n",
    "\n",
    "# instagram = pickle.load( open( filedir + \"/DP\" + str(int(l.lids()[4])) +  \"tweets\" + \".p\", \"rb\" )) \n",
    "\n",
    "a3 = pickle.load( open( filedir + \"/DP\" + '1995377' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "# a3 = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "#instagram = pickle.load( open( filedir + \"/DP\" + '11852603' +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "\n",
    "\n",
    "\n",
    "# json.loads(a3.json()[0])\n",
    "\n",
    "a3.json()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kelsey = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "\n",
    "# for fil in kelsey:\n",
    "#     print(kelsey[fil])\n",
    "\n",
    "'Valencia' in kelsey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagramMedia = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "filters = {'Lark':0,'Slumber':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "filtervec = np.ones((8,))\n",
    "\n",
    "for i in range(0,3):\n",
    "    filt = json.loads(a[i])['filter']\n",
    "    if(filt in filters):\n",
    "        filters[filt] += 1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i in range(0,8):\n",
    "    filtervec[i] = filters[list(filters)[i]]\n",
    "    \n",
    "filtervec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TWITTER 70522438\n",
    "## IG   19671950\n",
    "\n",
    "\n",
    "# instagramMedia = pickle.load( open( filedir + \"/DP\" + '82562975' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "instagramMedia = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "\n",
    "len(instagramMedia.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtervec = np.ones((8,))\n",
    "filtervec/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = embeddingToMastersum(a3)\n",
    "w.reshape((-1, 1)).T.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1511765588'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timenow  = str(int(time.time())) # for temporal congruency\n",
    "timenow1 = timenow\n",
    "timenow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'588'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timenow[-3:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
