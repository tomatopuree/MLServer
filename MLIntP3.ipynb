{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is its own cell because it takes a while to load this thing\n",
    "from gensim import models\n",
    "\n",
    "# takes a little bit. increase limit at own risk.\n",
    "# model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  \n",
    "model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and list of scrape times.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "import cv2\n",
    "import urllib.request\n",
    "import sqlite3 as sql\n",
    "\n",
    "list_of_scrape_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## class Loader\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, list_of_ids = [], list_of_times = []):\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    filedir = \"\"\n",
    "\n",
    "    ## 'file' omitted because it's not used in generating features\n",
    "    list_of_types = ['text','log','contact','calender','gps','tweets','Instagram', 'Instagram media', 'audio','phq']\n",
    "    \n",
    "    def deletdatboi(self, nombre):\n",
    "        shutil.rmtree('./datafor' + list_of_scrape_times[nombre])\n",
    "        \n",
    "    def lids(self):\n",
    "        return self.list_of_ids\n",
    "    \n",
    "    def lits(self):\n",
    "        return self.list_of_times\n",
    "    \n",
    "    def downloadAndLabel(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            self.list_of_ids.append(row[0])\n",
    "            self.list_of_times.append(row[1])\n",
    "\n",
    "        number_cols = len(self.list_of_types)\n",
    "        number_rows = len(self.list_of_ids)\n",
    "\n",
    "        ### create directory for this particular scrape/pull\n",
    "        timenow  = str(int(time.time())) # for temporal congruency\n",
    "        timereadable = datetime.datetime.fromtimestamp(int(timenow)).strftime('%H:%M')\n",
    "        list_of_scrape_times.append(timereadable)\n",
    "        os.mkdir('./datafor' + timereadable)\n",
    "        Loader.filedir = './datafor' + timereadable\n",
    "\n",
    "        for i in range(0,number_rows):\n",
    "            for j in range(0,number_cols):\n",
    "\n",
    "                # print(self.list_of_ids[i] + \" \"  + self.list_of_types[j])\n",
    "                \n",
    "                apickle = []\n",
    "\n",
    "                exampleLookup = (self.list_of_ids[i], self.list_of_types[j])\n",
    "                for row in c.execute('SELECT DISTINCT* FROM data WHERE id=? AND type=?', exampleLookup):\n",
    "                    # the row with the ID, type, and content\n",
    "                    apickle.append(row[2])\n",
    "\n",
    "                # print(apickle)    \n",
    "                \n",
    "                pickle.dump(apickle, open( Loader.filedir  + \"/DP\" + self.list_of_ids[i] +  self.list_of_types[j] + \".p\", \"wb\" ))\n",
    "        \n",
    "        ## OLD CODE FOR INTERFACING THROUGH THE WEBSERVER\n",
    "        ## CODE ABOVE ACCESSES A .DB FILE LOCALLY\n",
    "        '''\n",
    "        \n",
    "        r = requests.get('http://depressionmqp.wpi.edu:8080/getids')\n",
    "        list_of_idtime = r.json()\n",
    "        \n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_ids.append( list_of_idtime[i]['id'].encode('ascii','ignore') )\n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_times.append( list_of_idtime[i]['date'] )\n",
    "\n",
    "        number_cols = len(self.list_of_types)\n",
    "        number_rows = len(self.list_of_ids)\n",
    "        \n",
    "        ### create directory for this particular scrape/pull\n",
    "        timenow  = str(int(time.time())) # for temporal congruency\n",
    "        timereadable = datetime.datetime.fromtimestamp(int(timenow)).strftime('%H:%M')\n",
    "        list_of_scrape_times.append(timereadable)\n",
    "        os.mkdir('./datafor' + timereadable)\n",
    "        Loader.filedir = './datafor' + timereadable\n",
    "\n",
    "\n",
    "        for i in range(0,number_rows):\n",
    "            for j in range(0,number_cols):\n",
    "                \n",
    "                temp = requests.get('http://depressionmqp.wpi.edu:8080/getdata?id=' + str(int(self.list_of_ids[i])) + '&type=' + self.list_of_types[j])\n",
    "                fintemp = json.loads(temp.text)[\"data\"]\n",
    "                \n",
    "                while(json.loads(temp.text)[\"nextURL\"] != ''):\n",
    "                    temp = requests.get('http://depressionmqp.wpi.edu:8080' + json.loads(temp.text)[\"nextURL\"])\n",
    "                    fintemp += json.loads(temp.text)[\"data\"]\n",
    "                \n",
    "                pickle.dump(fintemp, open( Loader.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  self.list_of_types[j] + \".p\", \"wb\" ))\n",
    "\n",
    "                # loads it into memory. i will not use this for now\n",
    "                # for sake of architectural sanity\n",
    "                # list_of_jsons[i][j] = temp\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Featurizer featurizes methods that convert json objects of the appropriate type into features\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.name = name\n",
    "\n",
    "    # takes in text pickle and scrapedate, spits out text frequency\n",
    "    def dailyTextFreq(self, texts, scrapedate):\n",
    "        \n",
    "        a = texts\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "        \n",
    "        # calculate unix time for two weeks prior\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        text_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        # find the index of the oldest message that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            text_date = int(json.loads(a[len(a)-(i+1)])['date'].encode('ascii','ignore'))\n",
    "            if (two_weeks_prior < text_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        # divide by 14 to get daily freq\n",
    "        return float(saved_index+1)/14\n",
    "    \n",
    "\n",
    "    # takes in call pickle and scrapedate, spits out call frequency\n",
    "    def dailyCallFreq(self, calls, scrapedate):\n",
    "\n",
    "        a = calls\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        call_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            call_date = int(json.loads(a[len(a)-(i+1)])['date'].encode('ascii','ignore'))\n",
    "            if (two_weeks_prior < call_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        return float(saved_index+1)/14\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, return master vector\n",
    "    def embeddingToMastersum(self, tweets):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(tweets)):\n",
    "            try:\n",
    "                masterSum += self.tweetToEmbedding(tweets[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is one single tweet, returns vector embedding of entire tweet.\n",
    "    # eg: responseobject.json()[0]\n",
    "    def tweetToEmbedding(self, tweet):\n",
    "\n",
    "        q = tweet['text'].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, returns follow count\n",
    "    def followerCount(self, tweets):\n",
    "\n",
    "        followerCount = 0\n",
    "\n",
    "        # get\n",
    "        try:\n",
    "            followerCount = json.loads(tweets[0])['user']['followers_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followerCount\n",
    "\n",
    "    # input is tweets pickle, returns friend count\n",
    "    def followingCount(self, tweets):\n",
    "\n",
    "        followingCount = 0\n",
    "\n",
    "        try:\n",
    "            followingCount = json.loads(tweets[0])['user']['friends_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followingCount\n",
    "\n",
    "    # input is tweets pickle, return avg likes per post for the last 2 weeks\n",
    "    def twitterLikeFreq(self, tweets, scrapedate):\n",
    "\n",
    "        a = tweets\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        tweet_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        for i in range(0,len(a)):\n",
    "\n",
    "            utc = json.loads(a[len(a)-(i+1)])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\")))\n",
    "\n",
    "            if (two_weeks_prior < tweet_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        sum_of_likes = 0\n",
    "\n",
    "        for i in range(0, saved_index):\n",
    "            sum_of_likes += json.loads(tweets[i])['favorite_count']\n",
    "\n",
    "\n",
    "        return sum_of_likes/14\n",
    "\n",
    "    # input is tweets pickle, return avg retweets per post for the last 2 weeks\n",
    "    def twitterRetweetFreq(self, tweets, scrapedate):\n",
    "\n",
    "        a = tweets\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        tweet_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        for i in range(0,len(a)):\n",
    "\n",
    "            utc = json.loads(a[len(a)-(i+1)])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\")))\n",
    "\n",
    "            if (two_weeks_prior < tweet_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        sum_of_retweets = 0\n",
    "\n",
    "        for i in range(0, saved_index):\n",
    "            sum_of_likes += json.loads(tweets[i])['retweet_count']\n",
    "\n",
    "\n",
    "        return sum_of_retweets/14\n",
    "    \n",
    "    # input is contacts pickle, returns number of contacts\n",
    "    def numOfContacts(self, contacts):\n",
    "        return len(contacts)\n",
    "    \n",
    "    # input is instagram pickle, return two features: follows count, followed by count  \n",
    "    def instagramThings(self, instagram):\n",
    "        \n",
    "        followsFollowed = np.zeros((2,))\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if(type(json.loads(instagram[0])) == str):\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            else: # its a dictionary like it's supposed to be\n",
    "                followsFollowed[0] = json.loads(instagram[0])['data']['counts']['follows']\n",
    "                followsFollowed[1] = json.loads(instagram[0])['data']['counts']['followed_by']\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        return followsFollowed\n",
    "    \n",
    "    # takes in instagramMedia pickle scrape date, spits out filter usage frequency for the past 2 weeks\n",
    "    def instagramFilterFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "                    \n",
    "        negative_count = 0\n",
    "\n",
    "        for i in range(0,saved_index):\n",
    "            if(json.loads(a[i])['filter'] == Normal):\n",
    "                negative_count += 1\n",
    "\n",
    "        # filter usage freq = posts with filters count / total post count \n",
    "        return ((saved_index+1)-negative_count)/(saved_index+1)\n",
    "    \n",
    "    # takes in instagramMedia pickle and scrape date, returns a vector that\n",
    "    # contains a normalized percentage (0-1) for the usage of the filters\n",
    "    # listed below for the past 2 weeks: \n",
    "    # Valencia, X-Pro II, Hefe, Amaro, Rise, Willow, Crema, Inkwell\n",
    "    \n",
    "    # output example: [0.5,0,0,0,0,0.5,0,0] \n",
    "    # interpretation: user used valencia half the time, Willow the other half\n",
    "    # of time, for the past 2 weeks of Instagram posts.\n",
    "    def instagramFilterVector(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        filters = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "        filtervec = np.zeros((8,))\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "                    \n",
    "        for i in range(0,saved_index):\n",
    "            filt = json.loads(a[i])['filter']\n",
    "            if(filt in filters):\n",
    "                filters[filt] += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for i in range(0,8):\n",
    "            filtervec[i] = filters[list(filters)[i]]\n",
    "\n",
    "        # percentage of how much the filters are used as a normalized vector\n",
    "        return filtervec/(saved_index+1)\n",
    "    \n",
    "    # takes in InstagramMedia, returns comment and like frequency for the \n",
    "    # past 2 weeks.\n",
    "    def instagramLikeComFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        counts = np.zeros((2,))\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "                \n",
    "        for i in range(0,saved_index):\n",
    "            counts[0] += json.loads(instagramMedia[i])['likes']['count']\n",
    "            counts[1] += json.loads(instagramMedia[i])['comments']['count']\n",
    "        \n",
    "            \n",
    "        # because dividing by 0 is a big No-No\n",
    "        if (not(counts[0] != 0 and counts[1] != 0)):\n",
    "            return np.zeros((2,))\n",
    "        else:\n",
    "            # likes per post for past 2 weeks\n",
    "            return counts/saved_index\n",
    "\n",
    "\n",
    "    # takes in instagramMedia pickle and scrapedate, returns IG post \n",
    "    # frequency for the past 2 weeks\n",
    "    def instagramPostFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "                \n",
    "        return (saved_index+1)/14\n",
    "        \n",
    "        \n",
    "    # takes instagramMedia, returns pixelwise average values for [H,S,V]\n",
    "    # (Hue, Saturation, Value) for all posts in the past 2 weeks, the \n",
    "    # count of faces as a frequency of faces per picture, for the past\n",
    "    # 2 weeks as well.\n",
    "    def averageHSVF(self, instagramMedia, scrapedate):\n",
    "\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        ## ms into secs\n",
    "        scrapedate = int(str(scrapedate)[:-3]) \n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if(os.path.exists(\"./ILLSTOPBLINKINGSOON\")):\n",
    "            shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        os.mkdir('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        avgs = np.zeros((4,))\n",
    "\n",
    "        ## to avoid division by 0\n",
    "        if(saved_index == 0):\n",
    "            return avgs\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(0,saved_index):\n",
    "            url = json.loads(instagramMedia[i])['images']['thumbnail']['url']\n",
    "            urllib.request.urlretrieve(url, './ILLSTOPBLINKINGSOON/' + str(i) + '.jpg')\n",
    "\n",
    "\n",
    "        # face_cascade here is a pre trained classifier for frontal faces \n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "            \n",
    "        avgHue = 0\n",
    "        avgSatur = 0\n",
    "        avgVal = 0\n",
    "        avgFaces = 0\n",
    "\n",
    "        ## GODS OF PROGRAMMING, FORGIVE ME FOR THIS TRIPLE NEST\n",
    "\n",
    "        for k in range(0, saved_index):\n",
    "            ## BGR and not RGB because imread reads in BGR\n",
    "            img = cv2.imread('./ILLSTOPBLINKINGSOON/' + str(k) + '.jpg')\n",
    "            hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "            \n",
    "            avgFaces += len(faces)\n",
    "            \n",
    "            for i in range(0,hsv.shape[0]):\n",
    "                for j in range(0,hsv.shape[1]):\n",
    "                    avgHue += hsv[i,j,0]\n",
    "                    avgSatur += hsv[i,j,1]\n",
    "                    avgVal += hsv[i,j,2]\n",
    "                    \n",
    "        \n",
    "\n",
    "        sums = [avgHue,avgSatur,avgVal]\n",
    "\n",
    "        ## 22500 = 150x150 = instagram photo thumbnail shape\n",
    "        avgs = list(map(lambda x: x/(22500*saved_index), sums)).append(avgFaces/saved_index)\n",
    "\n",
    "        shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        return avgs      \n",
    "\n",
    "    \n",
    "    #input is texts pickle, return master vector \n",
    "    def embeddingToMastersumText(self, texts):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(texts)):\n",
    "            try:\n",
    "                masterSum += self.textToEmbedding(texts[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is single text, returns vector embedding of entire text.\n",
    "    def textToEmbedding(self, text):\n",
    "        \n",
    "        q = json.loads(text)[\"body\"].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # returns [q1,q2,q3,q4,q5,q6,q7,q8,q9] and sum of all these scores\n",
    "    def labelGenerator(self, phq):\n",
    "        \n",
    "        labelVector = np.zeros((10,))\n",
    "        sumOfScores = 0\n",
    "        \n",
    "        print(phq)\n",
    "        \n",
    "        for i in range(0,9):\n",
    "            temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "            labelVector[i] = temp\n",
    "            labelVector[9] += temp\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(self, filedir, list_of_ids, list_of_times):\n",
    "        self.filedir = filedir\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.f = Featurizer()\n",
    "        self.num_of_people = len(list_of_ids)\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    featureMatrix = None\n",
    "    \n",
    "    def generateMatrix(self):\n",
    "\n",
    "        # these are the feature vectors, one for every feature.\n",
    "        # each feature vector contains the value for one feature\n",
    "        # for every user. \n",
    "        featureVectorCF = np.zeros((self.num_of_people,1))\n",
    "        featureVectorTF = np.zeros((self.num_of_people,1))\n",
    "        featureVectorTW = np.zeros((self.num_of_people,300))\n",
    "        featureVectorFC = np.zeros((self.num_of_people,1))\n",
    "        featureVectorFC2 = np.zeros((self.num_of_people,1))\n",
    "        featureVectorTWL = np.zeros((self.num_of_people,1))\n",
    "        featureVectorTWRT = np.zeros((self.num_of_people,1))\n",
    "        featureVectorNC = np.zeros((self.num_of_people,1))\n",
    "        featureVectorIG1 = np.zeros((self.num_of_people,2))\n",
    "        featureVectorIG2 = np.zeros((self.num_of_people,1))\n",
    "        featureVectorIGFV = np.zeros((self.num_of_people,8))\n",
    "        featureVectorIGLC = np.zeros((self.num_of_people,2))\n",
    "        featureVectorIG3 = np.zeros((self.num_of_people,1))\n",
    "        featureVectorHSVF = np.zeros((self.num_of_people,4))\n",
    "        featureVectorTW2V = np.zeros((self.num_of_people,300))\n",
    "        \n",
    "        labelVector = np.zeros((self.num_of_people,10))\n",
    "        \n",
    "        for i in range(0, self.num_of_people):\n",
    "\n",
    "            #list_of_jsons[i][1]\n",
    "\n",
    "                \n",
    "            a1 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"log\" + \".p\", \"rb\" ))    \n",
    "            a2 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"text\" + \".p\", \"rb\" )) \n",
    "            a3 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"tweets\" + \".p\", \"rb\" )) \n",
    "            a4 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"contact\" + \".p\", \"rb\" )) \n",
    "            a5 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "            a6 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"Instagram media\" + \".p\", \"rb\" ))\n",
    "            \n",
    "            b1 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"phq\" + \".p\", \"rb\" )) \n",
    "            \n",
    "            \n",
    "            if((len(a1) + len(a2) + len(a3) + len(a4) + len(a5) + len(a6) + len(b1)) == 0):\n",
    "                # false entry in ID list\n",
    "                continue\n",
    "            \n",
    "            # print(self.list_of_ids[i])\n",
    "            # print(i)\n",
    "            \n",
    "            featureVectorCF[i] = self.f.dailyCallFreq(a1, self.list_of_times[i])\n",
    "            featureVectorTF[i] = self.f.dailyTextFreq(a2, self.list_of_times[i])\n",
    "            featureVectorTW[i] = self.f.embeddingToMastersum(a3)\n",
    "            featureVectorFC[i] = self.f.followerCount(a3)\n",
    "            featureVectorFC2[i] = self.f.followingCount(a3)\n",
    "            featureVectorTWL[i] = self.f.twitterLikeFreq(a3, self.list_of_times[i])\n",
    "            featureVectorTWRT[i] = self.f.twitterRetweetFreq(a3, self.list_of_times[i])\n",
    "            featureVectorNC[i] = self.f.numOfContacts(a4)\n",
    "            featureVectorIG1[i] = self.f.instagramThings(a5)\n",
    "            featureVectorIG2[i] = self.f.instagramFilterFreq(a6, self.list_of_times[i])\n",
    "            featureVectorIGFV[i] = self.f.instagramFilterVector(a6, self.list_of_times[i])\n",
    "            featureVectorIGLC[i] = self.f.instagramLikeComFreq(a6, self.list_of_times[i])\n",
    "            featureVectorIG3[i] = self.f.instagramPostFreq(a6, self.list_of_times[i])\n",
    "            featureVectorHSVF[i] = self.f.averageHSVF(a6, self.list_of_times[i])\n",
    "            featureVectorTW2V[i] = self.f.embeddingToMastersumText(a2)\n",
    "            \n",
    "            labelVector[i] = self.f.labelGenerator(b1)\n",
    "\n",
    "            Generator.featureMatrix = np.hstack((featureVectorCF,featureVectorTF,\n",
    "featureVectorTW, featureVectorFC, featureVectorTWL, featureVectorTWRT,\n",
    "featureVectorNC,featureVectorIG1, featureVectorIG2, featureVectorIGFV,\n",
    "featureVectorIGLC, featureVectorIG3, featureVectorHSVF, featureVectorTW2V,\n",
    "labelVector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Everything I coded in one cell!\n",
    "\n",
    "l = Loader()\n",
    "l.downloadAndLabel()\n",
    "\n",
    "# g = Generator(l.filedir, l.lids(), l.lits())\n",
    "# g.generateMatrix()\n",
    "# g.featureMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"Q0\":\"2\",\"Q1\":\"0\",\"Q2\":\"1\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"1\",\"Q6\":\"0\",\"Q7\":\"1\",\"Q8\":\"1\"}', '{\"Q0\":\"2\",\"Q1\":\"0\",\"Q2\":\"1\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"1\",\"Q6\":\"0\",\"Q7\":\"1\",\"Q8\":\"1\"}']\n"
     ]
    }
   ],
   "source": [
    "for i in l.lids()[17:18]:\n",
    "    b1 = pickle.load( open( \"datafor15:06\" + \"/DP\" + i +  \"phq\" + \".p\", \"rb\" )) \n",
    "    print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7835'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.lids()[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"Q0\":\"2\",\"Q1\":\"0\",\"Q2\":\"1\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"1\",\"Q6\":\"0\",\"Q7\":\"1\",\"Q8\":\"1\"}', '{\"Q0\":\"2\",\"Q1\":\"0\",\"Q2\":\"1\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"1\",\"Q6\":\"0\",\"Q7\":\"1\",\"Q8\":\"1\"}']\n"
     ]
    }
   ],
   "source": [
    "b1 = pickle.load( open( \"datafor15:06\" + \"/DP\" + \"7835\" +  \"phq\" + \".p\", \"rb\" )) \n",
    "\n",
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "apickle = []\n",
    "exampleLookup = (\"7835\", \"phq\")\n",
    "for row in c.execute('SELECT DISTINCT * FROM data WHERE id=? AND type=? ', exampleLookup):\n",
    "    # the row with the ID, type, and content\n",
    "    apickle.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"Q0\":\"2\",\"Q1\":\"0\",\"Q2\":\"1\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"1\",\"Q6\":\"0\",\"Q7\":\"1\",\"Q8\":\"1\"}']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WASTELAND OF PLAY CELLS BELOW\n",
    "import pickle\n",
    "\n",
    "filedir = './datafor' + '288'\n",
    "\n",
    "## access example\n",
    "\n",
    "# instagram = pickle.load( open( filedir + \"/DP\" + str(int(l.lids()[4])) +  \"tweets\" + \".p\", \"rb\" )) \n",
    "\n",
    "a3 = pickle.load( open( filedir + \"/DP\" + '1995377' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "# a3 = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "#instagram = pickle.load( open( filedir + \"/DP\" + '11852603' +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "\n",
    "\n",
    "\n",
    "# json.loads(a3.json()[0])\n",
    "\n",
    "a3.json()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"_id\":\"4\",\"thread_id\":\"1\",\"address\":\" 15085301734\",\"person\":\"null\",\"date\":\"1460240309687\",\"date_sent\":\"0\",\"protocol\":\"null\",\"read\":\"1\",\"status\":\"-1\",\"type\":\"2\",\"reply_path_present\":\"null\",\"subject\":\"null\",\"body\":\"Your face\",\"service_center\":\"null\",\"locked\":\"0\",\"error_code\":\"0\",\"seen\":\"1\",\"deletable\":\"0\",\"sim_slot\":\"0\",\"sim_imsi\":\"null\",\"hidden\":\"0\",\"group_id\":\"null\",\"group_type\":\"null\",\"delivery_date\":\"null\",\"app_id\":\"0\",\"msg_id\":\"0\",\"callback_number\":\"null\",\"reserved\":\"0\",\"pri\":\"0\",\"teleservice_id\":\"0\",\"link_url\":\"null\",\"svc_cmd\":\"0\",\"svc_cmd_content\":\"null\",\"roam_pending\":\"0\",\"spam_report\":\"0\",\"safe_message\":\"0\",\"sub_id\":\"-1\",\"creator\":\"com.android.mms\",\"secret_mode\":\"0\",\"favorite\":\"0\",\"d_rpt_cnt\":\"0\",\"using_mode\":\"0\",\"from_address\":\"null\",\"announcements_subtype\":\"0\",\"announcements_scenario_id\":\"null\",\"device_name\":\"null\"}'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# temp = requests.get('http://depressionmqp.wpi.edu:8080/getdata?id=' + str(98945548) + '&type=' + \"text\")\n",
    "# fintemp = json.loads(temp.text)[\"data\"]\n",
    "\n",
    "# while(json.loads(temp.text)[\"nextURL\"] != ''):\n",
    "#     temp = requests.get('http://depressionmqp.wpi.edu:8080' + json.loads(temp.text)[\"nextURL\"])\n",
    "#     fintemp += json.loads(temp.text)[\"data\"]\n",
    "\n",
    "    \n",
    "fintemp[8817]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kelsey = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "\n",
    "# for fil in kelsey:\n",
    "#     print(kelsey[fil])\n",
    "\n",
    "'Valencia' in kelsey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagramMedia = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "filters = {'Lark':0,'Slumber':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "filtervec = np.ones((8,))\n",
    "\n",
    "for i in range(0,3):\n",
    "    filt = json.loads(a[i])['filter']\n",
    "    if(filt in filters):\n",
    "        filters[filt] += 1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i in range(0,8):\n",
    "    filtervec[i] = filters[list(filters)[i]]\n",
    "    \n",
    "filtervec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'JpegImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-613-3a98c835da3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'JpegImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "## TWITTER 70522438\n",
    "## IG   19671950\n",
    "\n",
    "\n",
    "# instagramMedia = pickle.load( open( filedir + \"/DP\" + '82562975' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "instagramMedia = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "\n",
    "url = json.loads(instagramMedia.json()[0])['images']['thumbnail']['url']\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtervec = np.ones((8,))\n",
    "filtervec/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = embeddingToMastersum(a3)\n",
    "w.reshape((-1, 1)).T.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1511765588'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timenow  = str(int(time.time())) # for temporal congruency\n",
    "timenow1 = timenow\n",
    "timenow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1513024716885,\n",
       " 1513026200334,\n",
       " 1513026939825,\n",
       " 1513027798716,\n",
       " 1513028059396,\n",
       " 1513028060408,\n",
       " 1513029109178,\n",
       " 1513034337824,\n",
       " 1513034463419,\n",
       " 1513035395273]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.lits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "image = cv2.imread('./faces.jpg')\n",
    "grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#faces = face_cascade.detectMultiScale(grayImage)\n",
    "faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "print(len(faces))\n",
    "\n",
    "#type(face_cascade) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datafor19:07',\n",
       " 'MLIntP3.ipynb',\n",
       " 'ILLSTOPBLINKINGSOON',\n",
       " '.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'OpenCV Shenaniganry.ipynb',\n",
       " 'README.md',\n",
       " 'haarcascade_frontalface_default.xml',\n",
       " 'faces.jpg',\n",
       " 'glove.twitter.27B.zip',\n",
       " 'MLInt.ipynb',\n",
       " 'datafor690',\n",
       " 'glove.twitter.27B.50d.txt',\n",
       " 'GoogleNews-vectors-negative300.bin']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_file_list = os.listdir(\".\")\n",
    "dir_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "\n",
    "# connect to file\n",
    "conn = sql.connect('phonedata.db')\n",
    "# create cursor for making calls to database\n",
    "c = conn.cursor()\n",
    "\n",
    "list_of_ids = []\n",
    "list_of_times = []\n",
    "\n",
    "\n",
    "for row in c.execute('SELECT * FROM ids'):\n",
    "    # one result\n",
    "    list_of_ids.append(row[0])\n",
    "    list_of_times.append(row[1])\n",
    "    # prints th id\n",
    "#     print row[0]\n",
    "#     # prints the timestamp\n",
    "#     print row[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "a1 = pickle.load( open( \"datafor15:06\" + \"/DP\" + \"0660\" +  \"tweets\" + \".p\", \"rb\" )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "a1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
