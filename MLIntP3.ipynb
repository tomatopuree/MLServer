{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is its own cell because it takes a while to load this thing\n",
    "from gensim import models\n",
    "\n",
    "# takes a little bit. increase limit at own risk.\n",
    "# model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  \n",
    "model = models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=500000)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and list of scrape times.\n",
    "\n",
    "import sklearn\n",
    "import requests\n",
    "import json\n",
    "import _pickle as pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "import cv2\n",
    "import urllib.request\n",
    "import sqlite3 as sql\n",
    "import base64\n",
    "import ffmpy3\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "list_of_scrape_times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## class Loader\n",
    "\n",
    "class Loader:\n",
    "    def __init__(self, list_of_ids = [], list_of_times = []):\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    filedir = \"\"\n",
    "\n",
    "    ## 'file' omitted because it's not used in generating features\n",
    "    list_of_types = ['text','log','contact','calender','gps','tweets','Instagram', 'Instagram media', 'audio','phq']\n",
    "    \n",
    "    def deletdatboi(self, nombre):\n",
    "        shutil.rmtree('./datafor' + list_of_scrape_times[nombre])\n",
    "        \n",
    "    def lids(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        list_of_approved_ids = []\n",
    "        text_file = open(\"codes.txt\", \"r\")\n",
    "        list_of_approved_ids = text_file.read().split('\\n')\n",
    "        del list_of_approved_ids[-1]\n",
    "        \n",
    "        self.list_of_ids = []\n",
    "        \n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            if (row[0] in list_of_approved_ids):\n",
    "                self.list_of_ids.append(row[0])\n",
    "        \n",
    "        return self.list_of_ids\n",
    "    \n",
    "    def lits(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        self.list_of_times = []\n",
    "        \n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            self.list_of_times.append(row[1])\n",
    "        return self.list_of_times\n",
    "    \n",
    "    def downloadAndLabel(self):\n",
    "        # connect to file\n",
    "        conn = sql.connect('phonedata.db')\n",
    "        # create cursor for making calls to database\n",
    "        c = conn.cursor()\n",
    "\n",
    "        self.list_of_ids = []\n",
    "        \n",
    "        for row in c.execute('SELECT * FROM ids'):\n",
    "            # one result\n",
    "            self.list_of_ids.append(row[0])\n",
    "            self.list_of_times.append(row[1])\n",
    "\n",
    "        number_cols = len(self.list_of_types)\n",
    "        number_rows = len(self.list_of_ids)\n",
    "\n",
    "        ### create directory for this particular scrape/pull\n",
    "        timenow  = str(int(time.time())) # for temporal congruency\n",
    "        timereadable = datetime.datetime.fromtimestamp(int(timenow)).strftime('%H:%M')\n",
    "        list_of_scrape_times.append(timereadable)\n",
    "        os.mkdir('./datafor' + timereadable)\n",
    "        Loader.filedir = './datafor' + timereadable\n",
    "\n",
    "        # alex gave me a list of approved mturk ids\n",
    "        list_of_approved_ids = []\n",
    "        text_file = open(\"codes.txt\", \"r\")\n",
    "        list_of_approved_ids = text_file.read().split('\\n')\n",
    "        del list_of_approved_ids[-1]\n",
    "        \n",
    "        for i in range(0,number_rows):\n",
    "            for j in range(0,number_cols):\n",
    "                \n",
    "                if (self.list_of_ids[i] not in list_of_approved_ids):\n",
    "                    break\n",
    "                \n",
    "                apickle = []\n",
    "\n",
    "                exampleLookup = (self.list_of_ids[i], self.list_of_types[j])\n",
    "                for row in c.execute('SELECT DISTINCT* FROM data WHERE id=? AND type=?', exampleLookup):\n",
    "                    # the row with the ID, type, and content\n",
    "                    apickle.append(row[2])\n",
    "                    \n",
    "                    # WEIRD: duplicate ids in data table with different data\n",
    "                    if exampleLookup[1] == \"phq\" or exampleLookup[1] == \"audio\":\n",
    "                        break\n",
    "\n",
    "                # print(apickle)    \n",
    "                \n",
    "                pickle.dump(apickle, open( Loader.filedir  + \"/DP\" + self.list_of_ids[i] +  self.list_of_types[j] + \".p\", \"wb\" ))\n",
    "        \n",
    "        ## OLD CODE FOR INTERFACING THROUGH THE WEBSERVER\n",
    "        ## CODE ABOVE ACCESSES A .DB FILE LOCALLY\n",
    "        '''\n",
    "        \n",
    "        r = requests.get('http://depressionmqp.wpi.edu:8080/getids')\n",
    "        list_of_idtime = r.json()\n",
    "        \n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_ids.append( list_of_idtime[i]['id'].encode('ascii','ignore') )\n",
    "\n",
    "        for i in range(0,len(list_of_idtime)):\n",
    "            self.list_of_times.append( list_of_idtime[i]['date'] )\n",
    "\n",
    "        number_cols = len(self.list_of_types)\n",
    "        number_rows = len(self.list_of_ids)\n",
    "        \n",
    "        ### create directory for this particular scrape/pull\n",
    "        timenow  = str(int(time.time())) # for temporal congruency\n",
    "        timereadable = datetime.datetime.fromtimestamp(int(timenow)).strftime('%H:%M')\n",
    "        list_of_scrape_times.append(timereadable)\n",
    "        os.mkdir('./datafor' + timereadable)\n",
    "        Loader.filedir = './datafor' + timereadable\n",
    "\n",
    "\n",
    "        for i in range(0,number_rows):\n",
    "            for j in range(0,number_cols):\n",
    "                \n",
    "                temp = requests.get('http://depressionmqp.wpi.edu:8080/getdata?id=' + str(int(self.list_of_ids[i])) + '&type=' + self.list_of_types[j])\n",
    "                fintemp = json.loads(temp.text)[\"data\"]\n",
    "                \n",
    "                while(json.loads(temp.text)[\"nextURL\"] != ''):\n",
    "                    temp = requests.get('http://depressionmqp.wpi.edu:8080' + json.loads(temp.text)[\"nextURL\"])\n",
    "                    fintemp += json.loads(temp.text)[\"data\"]\n",
    "                \n",
    "                pickle.dump(fintemp, open( Loader.filedir + \"/DP\" + str(int(self.list_of_ids[i])) +  self.list_of_types[j] + \".p\", \"wb\" ))\n",
    "\n",
    "                # loads it into memory. i will not use this for now\n",
    "                # for sake of architectural sanity\n",
    "                # list_of_jsons[i][j] = temp\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Featurizer featurizes methods that convert json objects of the appropriate type into features\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.name = name\n",
    "\n",
    "    # takes in text pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of texts sent in the 24 hours before the scrape\n",
    "    # the last element is count of texts sent on the 24 hour window 14 days before the scrapedate\n",
    "    def textFreqVec14(self, text, scrapedate):\n",
    "        \n",
    "        textFreqVec = np.zeros((14,))\n",
    "        \n",
    "#         if(len(text)==0):\n",
    "#             textFreqVec[:] = np.nan\n",
    "#             return textFreqVec\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # assuming unordered texts (WHICH TURNS OUT IS THE CASE)\n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+1)*mseconds_in_day)\n",
    "            for t in range(0,len(text)):\n",
    "                text_date = int(json.loads(text[t])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > text_date) and (text_date > time_during_week_lb)):\n",
    "                    textFreqVec[day] += 1.0\n",
    "                    \n",
    "        return textFreqVec\n",
    "    \n",
    "\n",
    "    # takes in call pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of calls sent in the 24 hours before the scrape\n",
    "    # the last element is count of calls sent on the 24 hour window 14 days before the scrapedate\n",
    "    def callFreqVec14(self, call, scrapedate):\n",
    "        \n",
    "        callFreqVec = np.zeros((14,))\n",
    "        \n",
    "#         if(len(call)==0):\n",
    "#             callFreqVec[:] = np.nan\n",
    "#             return callFreqVec\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+1)*mseconds_in_day)\n",
    "            for c in range(0,len(call)):\n",
    "                call_date = int(json.loads(call[c])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > call_date) and (call_date > time_during_week_lb)):\n",
    "                    callFreqVec[day] += 1.0\n",
    "                    \n",
    "        return callFreqVec\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, return master vector\n",
    "    def embeddingToMastersum(self, tweets):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(tweets)):\n",
    "            try:\n",
    "                masterSum += self.tweetToEmbedding(tweets[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is one single tweet, returns vector embedding of entire tweet.\n",
    "    # eg: responseobject.json()[0]\n",
    "    def tweetToEmbedding(self, tweet):\n",
    "\n",
    "        q = tweet['text'].split()\n",
    "        \n",
    "        sumVector = np.zeros((300,))\n",
    "        \n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "        \n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, returns follow count\n",
    "    def followerCount(self, tweets):\n",
    "\n",
    "        followerCount = 0\n",
    "\n",
    "#         if(len(tweets)==0):\n",
    "#             followerCount = np.nan\n",
    "#             return followerCount\n",
    "        \n",
    "        # get\n",
    "        try:\n",
    "            followerCount = json.loads(tweets[0])['user']['followers_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followerCount\n",
    "\n",
    "    # input is tweets pickle, returns friend count\n",
    "    def followingCount(self, tweets):\n",
    "\n",
    "        followingCount = 0\n",
    "\n",
    "#         if(len(tweets)==0):\n",
    "#             followingCount = np.nan\n",
    "#             return followingCount\n",
    "        \n",
    "        try:\n",
    "            followingCount = json.loads(tweets[0])['user']['friends_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followingCount\n",
    "\n",
    "    # input is tweets pickle, return avg likes per post for the last 2 weeks\n",
    "    def twitterLikeFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitLikeVec = np.zeros((1,))\n",
    "        \n",
    "#         if(len(tweets)==0):\n",
    "#             twitLikeVec[:] = np.nan\n",
    "#             return twitLikeVec\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitLikeVec[0] += 1.0\n",
    "                    \n",
    "        if(twitLikeVec[0] == 0):\n",
    "            return np.zeros((1,))\n",
    "                    \n",
    "        return twitLikeVec/14\n",
    "\n",
    "\n",
    "    # input is tweets pickle, return avg retweets per post for the last 2 weeks\n",
    "    def twitterRetweetFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitRTVec = np.zeros((1,))\n",
    "        \n",
    "#         if(len(tweets)==0):\n",
    "#             twitRTVec[:] = np.nan\n",
    "#             return twitRTVec\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitRTVec[0] += json.loads(tweets[t])['favorite_count']\n",
    "                    \n",
    "        return twitRTVec/14\n",
    "    \n",
    "    # input is contacts pickle, returns number of contacts\n",
    "    def numOfContacts(self, contacts):\n",
    "        \n",
    "        return len(contacts)\n",
    "    \n",
    "    # input is instagram pickle, return two features: follows count, followed by count  \n",
    "    def instagramThings(self, instagram):\n",
    "        \n",
    "        followsFollowed = np.zeros((2,))\n",
    "        \n",
    "        try:\n",
    "            if(type(json.loads(instagram[0])) == str):\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            else: # its a dictionary like it's supposed to be\n",
    "                followsFollowed[0] = json.loads(instagram[0])['data']['counts']['follows']\n",
    "                followsFollowed[1] = json.loads(instagram[0])['data']['counts']['followed_by']\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        return followsFollowed\n",
    "    \n",
    "    # takes in instagramMedia pickle scrape date, spits out filter usage frequency for the past 2 weeks\n",
    "    def instagramFilterFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        instaFiltVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                instaFiltVec[0] += 1.0\n",
    "                    \n",
    "       \n",
    "        nofilter_count = 0\n",
    "\n",
    "        for i in range(0,int(instaFiltVec[0])):\n",
    "            if(json.loads(instagramMedia[i])['filter'] == \"Normal\"):\n",
    "                nofilter_count += 1\n",
    "        \n",
    "        instaFiltVec[0] -= nofilter_count\n",
    "        \n",
    "        return instaFiltVec\n",
    "        \n",
    "       \n",
    "    \n",
    "    # takes in instagramMedia pickle and scrape date, returns a vector that\n",
    "    # contains a normalized percentage (0-1) for the usage of the filters\n",
    "    # listed below for the past 2 weeks: \n",
    "    # 'Amaro': 0,\n",
    "    #  'Crema': 0,\n",
    "    #  'Hefe': 5,\n",
    "    #  'Inkwell': 0,\n",
    "    #  'Rise': 0,\n",
    "    #  'Valencia': 0,\n",
    "    #  'Willow': 0,\n",
    "    #  'X-Pro II': 0}\n",
    "    \n",
    "    # output example: [0.5,0,0,0,0,0.5,0,0] \n",
    "    # interpretation: user used valencia half the time, Willow the other half\n",
    "    # of time, for the past 2 weeks of Instagram posts.1`sas\n",
    "    def instagramFilterVector(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        filters = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "        filtervec = np.zeros((8,))\n",
    "        \n",
    "        numposts = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                numposts += 1\n",
    "                \n",
    "        if(numposts == 0):\n",
    "            return np.zeros((8,))\n",
    "                \n",
    "        for i in range(0,numposts):\n",
    "            filt = json.loads(instagramMedia[i])['filter']\n",
    "            if(filt in filters):\n",
    "                filters[filt] += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for i in range(0,8):\n",
    "            filtervec[i] = filters[list(filters)[i]]\n",
    "\n",
    "        # percentage of how much the filters are used as a normalized vector\n",
    "        return filtervec/(numposts)\n",
    "    \n",
    "    # takes in InstagramMedia, returns comment and like frequency for the \n",
    "    # past 2 weeks.\n",
    "    def instagramLikeComFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        counts = np.zeros((2,))\n",
    "        \n",
    "        postcount = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount += 1\n",
    "        \n",
    "        if(postcount == 0):\n",
    "            return np.zeros((2,))        \n",
    "        \n",
    "        for i in range(0,postcount):\n",
    "            counts[0] += json.loads(instagramMedia[i])['likes']['count']\n",
    "            counts[1] += json.loads(instagramMedia[i])['comments']['count']\n",
    "        \n",
    "        if (counts[0] + counts[1] == 0):\n",
    "            return np.zeros((2,))\n",
    "        else:\n",
    "            # likes per post for past 2 weeks\n",
    "            return counts/postcount\n",
    "\n",
    "\n",
    "    # takes in instagramMedia pickle and scrapedate, returns IG post \n",
    "    # frequency for the past 2 weeks\n",
    "    def instagramPostFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        postcount = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount[0] += 1.0\n",
    "                \n",
    "        return postcount/14\n",
    "        \n",
    "        \n",
    "    # takes instagramMedia, returns pixelwise average values for [H,S,V]\n",
    "    # (Hue, Saturation, Value) for all posts in the past 2 weeks, the \n",
    "    # count of faces as a frequency of faces per picture, for the past\n",
    "    # 2 weeks as well.\n",
    "    \n",
    "    # testvariable:\n",
    "    # empty string: \"\" if not testing\n",
    "    # \n",
    "    def averageHSVF(self, instagramMedia, scrapedate):\n",
    "\n",
    "\n",
    "        counts = np.zeros((2,))\n",
    "        \n",
    "        postcount = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_time']\n",
    "            \n",
    "            if(instagramMedia != '[object Object]'):\n",
    "                igpost_date = int(json.loads(instagramMedia[t])['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount += 1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if(os.path.exists(\"./ILLSTOPBLINKINGSOON\")):\n",
    "            shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        os.mkdir('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        avgs = np.zeros((4,))\n",
    "\n",
    "        if(postcount == 0):\n",
    "            return avgs\n",
    "\n",
    "        for i in range(0,postcount):\n",
    "            url = json.loads(instagramMedia[i])['images']['thumbnail']['url']\n",
    "            urllib.request.urlretrieve(url, './ILLSTOPBLINKINGSOON/' + str(i) + '.jpg')\n",
    "\n",
    "        # face_cascade here is a pre trained classifier for frontal faces \n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "            \n",
    "        avgHue = 0\n",
    "        avgSatur = 0\n",
    "        avgVal = 0\n",
    "        avgFaces = 0\n",
    "\n",
    "        ## GODS OF PROGRAMMING, FORGIVE ME FOR THIS TRIPLE NEST\n",
    "\n",
    "        for k in range(0, postcount):\n",
    "            ## BGR and not RGB because imread reads in BGR\n",
    "            img = cv2.imread('./ILLSTOPBLINKINGSOON/' + str(k) + '.jpg')\n",
    "            \n",
    "            hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "            \n",
    "            avgFaces += len(faces)\n",
    "            \n",
    "            for i in range(0,hsv.shape[0]):\n",
    "                for j in range(0,hsv.shape[1]):\n",
    "                    avgHue += hsv[i,j,0]\n",
    "                    avgSatur += hsv[i,j,1]\n",
    "                    avgVal += hsv[i,j,2]\n",
    "                    \n",
    "        \n",
    "\n",
    "        sums = [avgHue,avgSatur,avgVal]\n",
    "\n",
    "        ## 22500 = 150x150 = instagram photo thumbnail shape\n",
    "        avgs = list(map(lambda x: x/(22500*postcount), sums)).append(avgFaces/postcount)\n",
    "\n",
    "        shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        return avgs      \n",
    "\n",
    "    \n",
    "    #input is texts pickle, return master vector \n",
    "    def embeddingToMastersumText(self, texts):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(texts)):\n",
    "            try:\n",
    "                masterSum += self.textToEmbedding(texts[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is single text, returns vector embedding of entire text.\n",
    "    def textToEmbedding(self, text):\n",
    "        \n",
    "        q = json.loads(text)[\"body\"].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # returns [q1,q2,q3,q4,q5,q6,q7,q8,q9] and sum of all these scores\n",
    "    def labelGenerator(self, phq):\n",
    "        \n",
    "        labelVector = np.zeros((10,))\n",
    "        sumOfScores = 0\n",
    "        \n",
    "        # print(phq)\n",
    "        \n",
    "        for i in range(0,9):\n",
    "            temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "            labelVector[i] = temp\n",
    "            labelVector[9] += temp\n",
    "            \n",
    "        return labelVector\n",
    "\n",
    "        \n",
    "    def voiceFeaturizer(self, voice):\n",
    "\n",
    "        audiofeaturevec = np.zeros((1583,))\n",
    "\n",
    "        if(len(voice) == 0):\n",
    "            return audiofeaturevec\n",
    "\n",
    "        # base64 string -> bitstring -> bitstream -> write into 3gp file\n",
    "        bytestream = base64.b64decode(voice[0])\n",
    "        fh = open(\"audio.3gp\",\"wb\")\n",
    "        fh.write(bytestream)\n",
    "        fh.close()\n",
    "\n",
    "        # wav -> 3gp\n",
    "        ff = ffmpy3.FFmpeg( inputs={'audio.3gp': None}, outputs={'audio.wav': None})\n",
    "        ff.run()\n",
    "\n",
    "        os.remove(\"audio.3gp\")\n",
    "\n",
    "        # call to openSMILE\n",
    "        os.system( os.getcwd() + '/openSMILE-2.1.0/bin/linux_x64_standalone_static/SMILExtract -C ' + os.getcwd() + '/openSMILE-2.1.0/config/emobase2010.conf -I audio.wav -O \"out.csv\"')\n",
    "\n",
    "        os.remove(\"audio.wav\")\n",
    "\n",
    "        # csv file has a giant header. last line contains the features we want\n",
    "        # so we read the last line, cut out more useless string with 'l[9:]\n",
    "        with FileReadBackwards(\"out.csv\", encoding=\"utf-8\") as frb:\n",
    "            for l in frb:\n",
    "                b = l[9:].split(',')\n",
    "                break\n",
    "\n",
    "        os.remove(\"out.csv\")\n",
    "\n",
    "        # make list of string into list of floats\n",
    "        a = list(map(float, b))\n",
    "\n",
    "        # make sklearn happy\n",
    "        for i in range(0,len(a)):\n",
    "            audiofeaturevec[i] = a[i]\n",
    "\n",
    "        return audiofeaturevec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(self, filedir, list_of_ids, list_of_times):\n",
    "        self.filedir = filedir\n",
    "        self.list_of_ids = list_of_ids\n",
    "        self.f = Featurizer()\n",
    "        self.num_of_people = len(list_of_ids)\n",
    "        self.list_of_times = list_of_times\n",
    "    \n",
    "    BigMatrix = None\n",
    "    audioless = None\n",
    "    \n",
    "    l = Loader()\n",
    "    num_of_people = len(l.lids())\n",
    "    \n",
    "    # call frequency\n",
    "    featureVectorCF = np.zeros((num_of_people,14))\n",
    "    # text frequency\n",
    "    featureVectorTF = np.zeros((num_of_people,14))\n",
    "    # follower count\n",
    "    featureVectorFC = np.zeros((num_of_people,1))\n",
    "    # following count\n",
    "    featureVectorFC2 = np.zeros((num_of_people,1))\n",
    "    # twitter like frequency\n",
    "    featureVectorTWL = np.zeros((num_of_people,1))\n",
    "    # twitter retweet frequency\n",
    "    featureVectorTWRT = np.zeros((num_of_people,1))\n",
    "    # num of contacts\n",
    "    featureVectorNC = np.zeros((num_of_people,1))\n",
    "    # instagram follows, followed by\n",
    "    featureVectorIG1 = np.zeros((num_of_people,2))\n",
    "    # instagram filter usage freq\n",
    "    featureVectorIG2 = np.zeros((num_of_people,1))\n",
    "    # instagram filter vec: Valencia, X-Pro II, Hefe, Amaro, Rise, Willow, Crema, Inkwell\n",
    "    featureVectorIGFV = np.zeros((num_of_people,8))\n",
    "    # instagram like freq, comment freq\n",
    "    featureVectorIGLC = np.zeros((num_of_people,2))\n",
    "    # instagram post freq\n",
    "    featureVectorIG3 = np.zeros((num_of_people,1))\n",
    "    # instagram avg Hue, Saturation, Value, and total faces\n",
    "    featureVectorHSVF = np.zeros((num_of_people,4))\n",
    "    # audio features from openSMILE\n",
    "    featureVectorAUD = np.zeros((num_of_people,1583))\n",
    "\n",
    "    # labels, 0-8 for corresponding phq questions, last entry is sum of them.\n",
    "    labelVector = np.zeros((num_of_people,10))\n",
    "    \n",
    "    # bag of words with word2vec (try last)\n",
    "    #featureVectorTW = np.zeros((self.num_of_people,300))\n",
    "    # bag of words with word2vec (try last)\n",
    "    #featureVectorTW2V = np.zeros((self.num_of_people,300))\n",
    "    \n",
    "\n",
    "    def generateMatrix(self):\n",
    "\n",
    "        # these are the feature vectors, one for every feature.\n",
    "        # each feature vector contains the value for one feature\n",
    "        # for every user. \n",
    "\n",
    "        \n",
    "        # FILLING IN THE ABOVE VECTORS\n",
    "        \n",
    "        for i in range(0, self.num_of_people):\n",
    "            \n",
    "#             if(i%25 == 0):\n",
    "#                 print(self.list_of_ids[i])\n",
    "            \n",
    "            ## The flow of operation is same under every title\n",
    "            ## if data file is there and has actual data -> feature\n",
    "            ## if data file is there and there is no data in it -> NaN\n",
    "            ## if data file isnt there -> NaN\n",
    "            \n",
    "            ############# Call Log ###############################\n",
    "            try:\n",
    "                a1 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"log\" + \".p\", \"rb\" ))\n",
    "#                 if(len(a1)==0): \n",
    "#                     Generator.featureVectorCF[i:i+1,:] = np.nan\n",
    "#                     continue\n",
    "                        \n",
    "                Generator.featureVectorCF[i] = self.f.callFreqVec14(a1, self.list_of_times[i])\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorCF[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            ############# SMS Messages ###########################\n",
    "            try:\n",
    "                a2 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"text\" + \".p\", \"rb\" )) \n",
    "#                 if(len(a2)==0): \n",
    "# #                     Generator.featureVectorTF[i:i+1,:] = np.nan\n",
    "# #                     #featureVectorTW2V[i] = np.nan\n",
    "# #                     continue\n",
    "            \n",
    "                Generator.featureVectorTF[i] = self.f.textFreqVec14(a2, self.list_of_times[i])\n",
    "                #featureVectorTW2V[i] = self.f.embeddingToMastersumText(a2)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorTF[i:i+1,:] = np.nan\n",
    "                #featureVectorTW2V[i] = np.nan\n",
    "                pass\n",
    "            \n",
    "            ############# Twitter ###############################\n",
    "            try:\n",
    "                a3 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"tweets\" + \".p\", \"rb\" )) \n",
    "#                 if(len(a3)==0):\n",
    "#                     #featureVectorTW[i] = np.nan\n",
    "#                     Generator.featureVectorFC[i] = np.nan\n",
    "#                     Generator.featureVectorFC2[i] = np.nan\n",
    "#                     Generator.featureVectorTWL[i] = np.nan\n",
    "#                     Generator.featureVectorTWRT[i] = np.nan\n",
    "#                     continue\n",
    "            \n",
    "                #featureVectorTW[i] = self.f.embeddingToMastersum(a3)\n",
    "                Generator.featureVectorFC[i] = self.f.followerCount(a3)\n",
    "                Generator.featureVectorFC2[i] = self.f.followingCount(a3)\n",
    "                Generator.featureVectorTWL[i] = self.f.twitterLikeFreq(a3, self.list_of_times[i])\n",
    "                Generator.featureVectorTWRT[i] = self.f.twitterRetweetFreq(a3, self.list_of_times[i])\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                #featureVectorTW[i] = np.nan\n",
    "                Generator.featureVectorFC[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorFC2[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorTWL[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorTWRT[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "            ############# Contacts ###############################\n",
    "            try:\n",
    "                a4 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"contact\" + \".p\", \"rb\" )) \n",
    "#                 if(len(a4)==0):\n",
    "#                     Generator.featureVectorNC[i] = np.nan\n",
    "#                     continue\n",
    "                \n",
    "                Generator.featureVectorNC[i] = self.f.numOfContacts(a4)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorNC[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "            ############# Instagram ###############################\n",
    "            try:\n",
    "                a5 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "#                 if(len(a5)==0): \n",
    "#                     Generator.featureVectorIG1[i] = np.nan\n",
    "#                     continue\n",
    "            \n",
    "                Generator.featureVectorIG1[i] = self.f.instagramThings(a5)\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorIG1[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "            ############# Instagram Media #########################\n",
    "            try:\n",
    "                a6 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"Instagram media\" + \".p\", \"rb\" ))\n",
    "#                 if(len(a6)==0):\n",
    "#                     Generator.featureVectorIG2[i] = np.nan\n",
    "#                     Generator.featureVectorIGFV[i] = np.nan\n",
    "#                     Generator.featureVectorIGLC[i] = np.nan\n",
    "#                     Generator.featureVectorIG3[i] = np.nan\n",
    "#                     Generator.featureVectorHSVF[i] = np.nan\n",
    "#                     continue\n",
    "                \n",
    "                Generator.featureVectorIG2[i] = self.f.instagramFilterFreq(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorIGFV[i] = self.f.instagramFilterVector(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorIGLC[i] = self.f.instagramLikeComFreq(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorIG3[i] = self.f.instagramPostFreq(a6, self.list_of_times[i])\n",
    "                Generator.featureVectorHSVF[i] = self.f.averageHSVF(a6, self.list_of_times[i])\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.featureVectorIG2[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorIGFV[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorIGLC[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorIG3[i:i+1,:] = np.nan\n",
    "                Generator.featureVectorHSVF[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "            ############# Audio #########################\n",
    "#             try:\n",
    "#                 a7 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"audio\" + \".p\", \"rb\" ))\n",
    "#                 if(len(a7)==0):\n",
    "#                     Generator.featureVectorAUD [i] = np.nan\n",
    "#                     continue\n",
    "                \n",
    "#                 Generator.featureVectorAUD [i] = self.f.voiceFeaturizer(a7)\n",
    "            \n",
    "#             except FileNotFoundError:\n",
    "#                 Generator.featureVectorAUD [i:i+1,:] = np.nan\n",
    "#                 pass\n",
    "            \n",
    "            ############# Label #########################\n",
    "            try:\n",
    "                b1 = pickle.load( open( self.filedir + \"/DP\" + self.list_of_ids[i] +  \"phq\" + \".p\", \"rb\" )) \n",
    "#                 if(len(b1)==0):\n",
    "#                     print(\"len = 0 \" + self.list_of_ids[i])\n",
    "#                     Generator.labelVector[i] = np.nan\n",
    "#                     continue\n",
    "            \n",
    "                Generator.labelVector[i] = self.f.labelGenerator(b1)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                Generator.labelVector[i:i+1,:] = np.nan\n",
    "                pass\n",
    "            \n",
    "                       \n",
    "        Generator.BigMatrix = np.hstack((Generator.featureVectorCF, \n",
    "                                         Generator.featureVectorTF, \n",
    "                                         Generator.featureVectorFC, \n",
    "                                         Generator.featureVectorTWL, \n",
    "                                         Generator.featureVectorTWRT, \n",
    "                                         Generator.featureVectorNC, \n",
    "                                         Generator.featureVectorIG1, \n",
    "                                         Generator.featureVectorIG2, \n",
    "                                         Generator.featureVectorIGFV,\n",
    "                                         Generator.featureVectorIGLC, \n",
    "                                         Generator.featureVectorIG3, \n",
    "                                         Generator.featureVectorHSVF, \n",
    "                                         Generator.featureVectorAUD, \n",
    "                                         Generator.labelVector))\n",
    "        \n",
    "        Generator.audioless = np.hstack((Generator.featureVectorCF, \n",
    "                                         Generator.featureVectorTF, \n",
    "                                         Generator.featureVectorFC, \n",
    "                                         Generator.featureVectorTWL, \n",
    "                                         Generator.featureVectorTWRT, \n",
    "                                         Generator.featureVectorNC, \n",
    "                                         Generator.featureVectorIG1, \n",
    "                                         Generator.featureVectorIG2, \n",
    "                                         Generator.featureVectorIGFV,\n",
    "                                         Generator.featureVectorIGLC, \n",
    "                                         Generator.featureVectorIG3, \n",
    "                                         Generator.featureVectorHSVF, \n",
    "                                         Generator.labelVector))\n",
    "\n",
    "#     CallF = None\n",
    "\n",
    "#     CallF = featureVectorCF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261, 1643)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything I coded in one cell!\n",
    "\n",
    "l = Loader()\n",
    "# l.downloadAndLabel()\n",
    "\n",
    "g = Generator('./datafor23:41', l.lids(), l.lits())\n",
    "g.generateMatrix()\n",
    "mtr = g.BigMatrix\n",
    "g.BigMatrix.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261, 60)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtr = g.audioless\n",
    "mtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# replace missing values with mean of their corresponding features\n",
    "# imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "# mtr = imp.fit_transform(mtr)\n",
    "# shuffle row-wise\n",
    "np.random.shuffle(mtr)\n",
    "\n",
    "data = mtr[:,0:14]\n",
    "labels = mtr[:,50:61]\n",
    "\n",
    "# normalize data (features now have gauss dist., 0 mean and unit variance)\n",
    "data = sklearn.preprocessing.scale(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45174364, -0.43120947, -0.46227116, -0.34143674, -0.37824797,\n",
       "        -0.3667097 , -0.41995871, -0.27824165, -0.43403791, -0.34692746,\n",
       "        -0.3699291 , -0.38043859, -0.36949047, -0.36860458],\n",
       "       [-0.45174364, -0.43120947, -0.46227116, -0.34143674, -0.37824797,\n",
       "        -0.3667097 , -0.41995871, -0.27824165, -0.43403791, -0.34692746,\n",
       "        -0.3699291 , -0.38043859, -0.36949047, -0.36860458],\n",
       "       [-0.45174364, -0.43120947, -0.46227116, -0.34143674, -0.37824797,\n",
       "        -0.3667097 , -0.41995871, -0.27824165, -0.43403791, -0.34692746,\n",
       "        -0.3699291 , -0.38043859, -0.36949047, -0.36860458],\n",
       "       [-0.45174364, -0.43120947, -0.46227116, -0.34143674, -0.37824797,\n",
       "        -0.3667097 , -0.41995871, -0.27824165, -0.43403791, -0.34692746,\n",
       "        -0.3699291 , -0.38043859, -0.36949047, -0.36860458],\n",
       "       [ 0.47768838, -0.30461141,  0.37910103, -0.34143674,  0.10021432,\n",
       "        -0.3667097 ,  0.43413914, -0.20690465, -0.29260983, -0.09644871,\n",
       "         0.20995976, -0.38043859,  0.08220514, -0.26880604],\n",
       "       [-0.31896764,  0.32837887,  1.05219878,  0.2312817 ,  0.41918919,\n",
       "         1.19138012,  1.43058663,  1.14849844,  1.54595525,  0.90546629,\n",
       "         0.78984863,  0.71934196,  0.19512904,  0.13038814],\n",
       "       [-0.45174364, -0.43120947, -0.46227116, -0.34143674, -0.37824797,\n",
       "        -0.3667097 , -0.41995871, -0.27824165, -0.43403791, -0.34692746,\n",
       "        -0.3699291 , -0.38043859, -0.36949047, -0.36860458],\n",
       "       [ 1.00879238, -0.30461141, -0.46227116,  0.2312817 , -0.37824797,\n",
       "         0.52362734, -0.13525943, -0.13556764, -0.29260983, -0.34692746,\n",
       "        -0.07998467,  1.19067649,  0.08220514,  0.23018668],\n",
       "       [-0.18619164, -0.0514153 ,  1.38874766,  0.91854382,  2.81150066,\n",
       "         0.30104308, -0.27760907,  0.00710637,  0.13167442,  0.52974816,\n",
       "         0.93482084,  0.40511895,  0.75974856, -0.36860458],\n",
       "       [ 0.21213637,  1.97415359, -0.46227116,  0.57491276,  2.17355093,\n",
       "         4.08497549,  4.84697804,  0.4351284 ,  2.53595183,  2.40833879,\n",
       "         0.06498755,  0.87645347,  2.90530273,  1.12837357]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  1.,  3.,  2.,  3.,  1.,  1.,  0.,  0., 13.],\n",
       "       [ 2.,  2.,  1.,  2.,  1.,  2.,  2.,  0.,  0., 12.],\n",
       "       [ 1.,  1.,  3.,  1.,  0.,  1.,  0.,  0.,  0.,  7.],\n",
       "       [ 2.,  3.,  2.,  3.,  3.,  3.,  3.,  2.,  1., 22.],\n",
       "       [ 1.,  1.,  2.,  1.,  2.,  1.,  0.,  0.,  1.,  9.],\n",
       "       [ 1.,  1.,  2.,  1.,  3.,  2.,  1.,  2.,  2., 15.],\n",
       "       [ 3.,  3.,  3.,  3.,  3.,  3.,  1.,  2.,  3., 24.],\n",
       "       [ 0.,  0.,  1.,  2.,  1.,  1.,  0.,  0.,  0.,  5.],\n",
       "       [ 0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  3.],\n",
       "       [ 2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2., 18.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:10,:] # alex dis works and the top numbers are fukd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA SPLIT (#nosnooping)\n",
    "\n",
    "# TEST DATA (%15 percent of data)\n",
    "numofppl_index = g.BigMatrix.shape[0] - 1\n",
    "cut_index = int(g.BigMatrix.shape[0] * 0.85)\n",
    "\n",
    "test_label = g.BigMatrix[cut_index:numofppl_index,:]\n",
    "test_data = g.BigMatrix[cut_index:numofppl_index,:]\n",
    "\n",
    "# TRAINING AND TEST DATA (%85 percent of data)\n",
    "\n",
    "train_data = g.BigMatrix[0:cut_index,:]\n",
    "train_label = g.BigMatrix[0:cut_index,:]\n",
    "\n",
    "\n",
    "X = train_data[:,0:13]\n",
    "y = train_label[:,1642:1643]#.reshape((376,))\n",
    "\n",
    "mtrx = np.hstack((X,y))\n",
    "\n",
    "\n",
    "mtrx = pd.DataFrame(mtrx)\n",
    "\n",
    "\n",
    "\n",
    "mtrx = mtrx.dropna(axis=0, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [16.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [15.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [12.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [13.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [14.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [19.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [14.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [11.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [18.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [nan],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train_label[:,1642:1643]#.reshape((376,))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [16.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [15.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [12.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [13.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [14.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [19.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [14.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [11.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [18.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrx[:,13:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "mtrx = imp.fit_transform(mtrx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = mtrx[:,0:13]\n",
    "y = mtrx[:,13:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218,)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7     False\n",
       "8     False\n",
       "9     False\n",
       "10    False\n",
       "11    False\n",
       "12    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isfinite(y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218,)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y = y.reshape(218,)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218,)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y/shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], 'kernel': ('linear', 'rbf', 'poly', 'sigmoid')},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "c_range = list(range(1, 30))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "parameters['kernel']\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "grid = GridSearchCV(svc, parameters, cv=3, scoring='accuracy')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'linear', 'C': 1}\n",
      "[0.88607595 0.94285714 0.91304348]\n",
      "0.9128440366972477\n"
     ]
    }
   ],
   "source": [
    "grid.grid_scores_\n",
    "\n",
    "print(grid.grid_scores_[0].parameters)\n",
    "print(grid.grid_scores_[0].cv_validation_scores)\n",
    "print(grid.grid_scores_[0].mean_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KNeighborsClassifer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-a52828b1b151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'KNeighborsClassifer' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "iris = datasets.load_iris()\n",
    "# parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "# svc = svm.SVC()\n",
    "# clf = GridSearchCV(svc, parameters)\n",
    "# clf.fit(iris.data, iris.target)\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "knn = KNeighborsClassifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save matrix as csv. \n",
    "import pandas as pd\n",
    "# np.savetxt(\"foo.csv\", g.featureMatrix , delimiter=\",\")\n",
    "\n",
    "# another way\n",
    "\n",
    "dff = pd.DataFrame(g.featureMatrix)\n",
    "dff.to_csv(\"foo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(484, 2244)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = pd.read_csv(\"foo.csv\")\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"Q0\":\"3\",\"Q1\":\"1\",\"Q2\":\"0\",\"Q3\":\"2\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}', '{\"Q0\":\"1\",\"Q1\":\"1\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']\n",
      "[]\n",
      "['{\"Q0\":\"0\",\"Q1\":\"0\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']\n",
      "['{\"Q0\":\"2\",\"Q1\":\"1\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']\n",
      "['{\"Q0\":\"3\",\"Q1\":\"1\",\"Q2\":\"3\",\"Q3\":\"2\",\"Q4\":\"1\",\"Q5\":\"3\",\"Q6\":\"3\",\"Q7\":\"1\",\"Q8\":\"0\"}']\n",
      "['{\"Q0\":\"1\",\"Q1\":\"1\",\"Q2\":\"3\",\"Q3\":\"3\",\"Q4\":\"2\",\"Q5\":\"2\",\"Q6\":\"1\",\"Q7\":\"2\",\"Q8\":\"1\"}']\n",
      "['{\"Q0\":\"2\",\"Q1\":\"1\",\"Q2\":\"1\",\"Q3\":\"2\",\"Q4\":\"0\",\"Q5\":\"2\",\"Q6\":\"2\",\"Q7\":\"1\",\"Q8\":\"2\"}']\n"
     ]
    }
   ],
   "source": [
    "for i in l.lids()[23:30]:\n",
    "    b1 = pickle.load( open( \"datafor15:20\" + \"/DP\" + i +  \"phq\" + \".p\", \"rb\" )) \n",
    "    print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7276', '6830', '7664']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.lids()[23:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539833 is your Amazon security code.\n"
     ]
    }
   ],
   "source": [
    "b1 = pickle.load( open( \"datafor16:13\" + \"/DP\" + \"6578\" +  \"text\" + \".p\", \"rb\" )) \n",
    "\n",
    "print(json.loads(b1[0])[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('7664', 'phq', '{\"Q0\":\"0\",\"Q1\":\"0\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['{\"Q0\":\"0\",\"Q1\":\"0\",\"Q2\":\"0\",\"Q3\":\"0\",\"Q4\":\"0\",\"Q5\":\"0\",\"Q6\":\"0\",\"Q7\":\"0\",\"Q8\":\"0\"}']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apickle = []\n",
    "exampleLookup = (\"7664\", \"phq\")\n",
    "for row in c.execute('SELECT DISTINCT * FROM data WHERE id=? AND type=? ', exampleLookup):\n",
    "            \n",
    "    apickle.append(row[2])\n",
    "    \n",
    "    print(row)\n",
    "    \n",
    "apickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-04 to 2018-01-04 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-03 to 2018-01-03 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-02 to 2018-01-02 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2018-01-01 to 2018-01-01 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-31 to 2017-12-31 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-30 to 2017-12-30 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-29 to 2017-12-29 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-28 to 2017-12-28 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-27 to 2017-12-27 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-26 to 2017-12-26 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-25 to 2017-12-25 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-24 to 2017-12-24 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-23 to 2017-12-23 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n',\n",
       " '<?xml version=\"1.0\" encoding=\"UTF-8\"?><kml xmlns=\"http://www.opengis.net/kml/2.2\" xmlns:gx=\"http://www.google.com/kml/ext/2.2\"><Document><name> Location history from 2017-12-22 to 2017-12-22 </name><open>1</open><description></description><StyleMap id=\"multiTrack\"><Pair><key>normal</key><styleUrl>#multiTrack_n</styleUrl></Pair><Pair><key>highlight</key><styleUrl>#multiTrack_h</styleUrl></Pair></StyleMap><Style id=\"multiTrack_n\"><IconStyle><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>6</width></LineStyle></Style><Style id=\"multiTrack_h\"><IconStyle><scale>1.2</scale><Icon><href>https://earth.google.com/images/kml-icons/track-directional/track-0.png</href></Icon></IconStyle><LineStyle><color>99ffac59</color><width>8</width></LineStyle></Style></Document></kml>\\n']"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load( open( \"datafor16:13\" + \"/DP\" + \"0660\" +  \"gps\" + \".p\", \"rb\" ))\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## WASTELAND OF PLAY CELLS BELOW\n",
    "import pickle\n",
    "\n",
    "filedir = './datafor' + '288'\n",
    "\n",
    "## access example\n",
    "\n",
    "# instagram = pickle.load( open( filedir + \"/DP\" + str(int(l.lids()[4])) +  \"tweets\" + \".p\", \"rb\" )) \n",
    "\n",
    "a3 = pickle.load( open( filedir + \"/DP\" + '1995377' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "# a3 = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "#instagram = pickle.load( open( filedir + \"/DP\" + '11852603' +  \"Instagram\" + \".p\", \"rb\" )) \n",
    "\n",
    "\n",
    "\n",
    "# json.loads(a3.json()[0])\n",
    "\n",
    "a3.json()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"_id\":\"4\",\"thread_id\":\"1\",\"address\":\" 15085301734\",\"person\":\"null\",\"date\":\"1460240309687\",\"date_sent\":\"0\",\"protocol\":\"null\",\"read\":\"1\",\"status\":\"-1\",\"type\":\"2\",\"reply_path_present\":\"null\",\"subject\":\"null\",\"body\":\"Your face\",\"service_center\":\"null\",\"locked\":\"0\",\"error_code\":\"0\",\"seen\":\"1\",\"deletable\":\"0\",\"sim_slot\":\"0\",\"sim_imsi\":\"null\",\"hidden\":\"0\",\"group_id\":\"null\",\"group_type\":\"null\",\"delivery_date\":\"null\",\"app_id\":\"0\",\"msg_id\":\"0\",\"callback_number\":\"null\",\"reserved\":\"0\",\"pri\":\"0\",\"teleservice_id\":\"0\",\"link_url\":\"null\",\"svc_cmd\":\"0\",\"svc_cmd_content\":\"null\",\"roam_pending\":\"0\",\"spam_report\":\"0\",\"safe_message\":\"0\",\"sub_id\":\"-1\",\"creator\":\"com.android.mms\",\"secret_mode\":\"0\",\"favorite\":\"0\",\"d_rpt_cnt\":\"0\",\"using_mode\":\"0\",\"from_address\":\"null\",\"announcements_subtype\":\"0\",\"announcements_scenario_id\":\"null\",\"device_name\":\"null\"}'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# temp = requests.get('http://depressionmqp.wpi.edu:8080/getdata?id=' + str(98945548) + '&type=' + \"text\")\n",
    "# fintemp = json.loads(temp.text)[\"data\"]\n",
    "\n",
    "# while(json.loads(temp.text)[\"nextURL\"] != ''):\n",
    "#     temp = requests.get('http://depressionmqp.wpi.edu:8080' + json.loads(temp.text)[\"nextURL\"])\n",
    "#     fintemp += json.loads(temp.text)[\"data\"]\n",
    "\n",
    "    \n",
    "fintemp[8817]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kelsey = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "\n",
    "# for fil in kelsey:\n",
    "#     print(kelsey[fil])\n",
    "\n",
    "'Valencia' in kelsey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instagramMedia = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "filters = {'Lark':0,'Slumber':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "filtervec = np.ones((8,))\n",
    "\n",
    "for i in range(0,3):\n",
    "    filt = json.loads(a[i])['filter']\n",
    "    if(filt in filters):\n",
    "        filters[filt] += 1\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "for i in range(0,8):\n",
    "    filtervec[i] = filters[list(filters)[i]]\n",
    "    \n",
    "filtervec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'JpegImageFile' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-613-3a98c835da3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'JpegImageFile' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "## TWITTER 70522438\n",
    "## IG   19671950\n",
    "\n",
    "\n",
    "# instagramMedia = pickle.load( open( filedir + \"/DP\" + '82562975' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "instagramMedia = pickle.load( open( filedir + \"/DP\" + '19671950' +  \"Instagram media\" + \".p\", \"rb\" )) \n",
    "\n",
    "url = json.loads(instagramMedia.json()[0])['images']['thumbnail']['url']\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25,  0.25])"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtervec = np.ones((8,))\n",
    "filtervec/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = embeddingToMastersum(a3)\n",
    "w.reshape((-1, 1)).T.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1511765588'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timenow  = str(int(time.time())) # for temporal congruency\n",
    "timenow1 = timenow\n",
    "timenow1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1513024716885,\n",
       " 1513026200334,\n",
       " 1513026939825,\n",
       " 1513027798716,\n",
       " 1513028059396,\n",
       " 1513028060408,\n",
       " 1513029109178,\n",
       " 1513034337824,\n",
       " 1513034463419,\n",
       " 1513035395273]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.lits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "image = cv2.imread('./faces.jpg')\n",
    "grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#faces = face_cascade.detectMultiScale(grayImage)\n",
    "faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "print(len(faces))\n",
    "\n",
    "#type(face_cascade) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datafor19:07',\n",
       " 'MLIntP3.ipynb',\n",
       " 'ILLSTOPBLINKINGSOON',\n",
       " '.git',\n",
       " '.ipynb_checkpoints',\n",
       " 'OpenCV Shenaniganry.ipynb',\n",
       " 'README.md',\n",
       " 'haarcascade_frontalface_default.xml',\n",
       " 'faces.jpg',\n",
       " 'glove.twitter.27B.zip',\n",
       " 'MLInt.ipynb',\n",
       " 'datafor690',\n",
       " 'glove.twitter.27B.50d.txt',\n",
       " 'GoogleNews-vectors-negative300.bin']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_file_list = os.listdir(\".\")\n",
    "dir_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "\n",
    "# connect to file\n",
    "conn = sql.connect('phonedata.db')\n",
    "# create cursor for making calls to database\n",
    "c = conn.cursor()\n",
    "\n",
    "list_of_ids = []\n",
    "list_of_times = []\n",
    "\n",
    "\n",
    "for row in c.execute('SELECT * FROM ids'):\n",
    "    # one result\n",
    "    list_of_ids.append(row[0])\n",
    "    list_of_times.append(row[1])\n",
    "    # prints th id\n",
    "#     print row[0]\n",
    "#     # prints the timestamp\n",
    "#     print row[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-374-44dcd50a8971>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-374-44dcd50a8971>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    l = \"5904\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Loader' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-375-e75269d816bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Loader' has no len()"
     ]
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "a1 = pickle.load( open( \"datafor15:06\" + \"/DP\" + \"0660\" +  \"tweets\" + \".p\", \"rb\" )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# l = Loader()\n",
    "# list_of_ids = l.lids()\n",
    "# for idd in range(0,len(list_of_ids)):\n",
    "#     # print(phq)\n",
    "#     phq = pickle.load(open( \"datafor23:41\" + \"/DP\" + str(list_of_ids[idd]) +  \"phq\" + \".p\", \"rb\" ))\n",
    "#     labelVector = np.zeros((10,))\n",
    "#     sumOfScores = 0\n",
    "\n",
    "#     # print(phq)\n",
    "\n",
    "#     for i in range(0,9):\n",
    "#         print(i)\n",
    "#         temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "\n",
    "#         labelVector[i] = temp\n",
    "#         labelVector[9] += temp\n",
    "\n",
    "#     #print(labelVector[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    " try:\n",
    "    exit(0)\n",
    "catch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
