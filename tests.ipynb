{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Featurizer featurizes methods that convert json objects of the appropriate type into features\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.name = name\n",
    "\n",
    "    # takes in text pickle and scrapedate, spits out text frequency\n",
    "    def dailyTextFreq(self, texts, scrapedate):\n",
    "        \n",
    "        a = texts\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "        \n",
    "        # calculate unix time for two weeks prior\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        text_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        # find the index of the oldest message that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            text_date = int(json.loads(a[len(a)-(i+1)])['date'].encode('ascii','ignore'))\n",
    "            if (two_weeks_prior < text_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        # divide by 14 to get daily freq\n",
    "        return float(saved_index+1)/14\n",
    "    \n",
    "\n",
    "    # takes in call pickle and scrapedate, spits out call frequency\n",
    "    def dailyCallFreq(self, calls, scrapedate):\n",
    "\n",
    "        a = calls\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        call_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            call_date = int(json.loads(a[len(a)-(i+1)])['date'].encode('ascii','ignore'))\n",
    "            if (two_weeks_prior < call_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        return float(saved_index+1)/14\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, return master vector\n",
    "    def embeddingToMastersum(self, tweets):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(tweets)):\n",
    "            try:\n",
    "                masterSum += self.tweetToEmbedding(tweets[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is one single tweet, returns vector embedding of entire tweet.\n",
    "    # eg: responseobject.json()[0]\n",
    "    def tweetToEmbedding(self, tweet):\n",
    "\n",
    "        q = tweet['text'].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, returns follow count\n",
    "    def followerCount(self, tweets):\n",
    "\n",
    "        followerCount = 0\n",
    "\n",
    "        # get\n",
    "        try:\n",
    "            followerCount = json.loads(tweets[0])['user']['followers_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followerCount\n",
    "\n",
    "    # input is tweets pickle, returns friend count\n",
    "    def followingCount(self, tweets):\n",
    "\n",
    "        followingCount = 0\n",
    "\n",
    "        try:\n",
    "            followingCount = json.loads(tweets[0])['user']['friends_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followingCount\n",
    "\n",
    "    # input is tweets pickle, return avg likes per post for the last 2 weeks\n",
    "    def twitterLikeFreq(self, tweets, scrapedate):\n",
    "\n",
    "        a = tweets\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        tweet_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        for i in range(0,len(a)):\n",
    "\n",
    "            utc = json.loads(a[len(a)-(i+1)])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\")))\n",
    "\n",
    "            if (two_weeks_prior < tweet_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        sum_of_likes = 0\n",
    "\n",
    "        for i in range(0, saved_index):\n",
    "            sum_of_likes += json.loads(tweets[i])['favorite_count']\n",
    "\n",
    "\n",
    "        return sum_of_likes/14\n",
    "\n",
    "    # input is tweets pickle, return avg retweets per post for the last 2 weeks\n",
    "    def twitterRetweetFreq(self, tweets, scrapedate):\n",
    "\n",
    "        a = tweets\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        tweet_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        for i in range(0,len(a)):\n",
    "\n",
    "            utc = json.loads(a[len(a)-(i+1)])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\")))\n",
    "\n",
    "            if (two_weeks_prior < tweet_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "        sum_of_retweets = 0\n",
    "\n",
    "        for i in range(0, saved_index):\n",
    "            sum_of_likes += json.loads(tweets[i])['retweet_count']\n",
    "\n",
    "\n",
    "        return sum_of_retweets/14\n",
    "    \n",
    "    # input is contacts pickle, returns number of contacts\n",
    "    def numOfContacts(self, contacts):\n",
    "        return len(contacts)\n",
    "    \n",
    "    # input is instagram pickle, return two features: follows count, followed by count  \n",
    "    def instagramThings(self, instagram):\n",
    "        \n",
    "        followsFollowed = np.zeros((2,))\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if(type(json.loads(instagram[0])) == str):\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            else: # its a dictionary like it's supposed to be\n",
    "                followsFollowed[0] = json.loads(instagram[0])['data']['counts']['follows']\n",
    "                followsFollowed[1] = json.loads(instagram[0])['data']['counts']['followed_by']\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        return followsFollowed\n",
    "    \n",
    "    # takes in instagramMedia pickle scrape date, spits out filter usage frequency for the past 2 weeks\n",
    "    def instagramFilterFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "                    \n",
    "        negative_count = 0\n",
    "\n",
    "        for i in range(0,saved_index):\n",
    "            if(json.loads(a[i])['filter'] == Normal):\n",
    "                negative_count += 1\n",
    "\n",
    "        # filter usage freq = posts with filters count / total post count \n",
    "        return ((saved_index+1)-negative_count)/(saved_index+1)\n",
    "    \n",
    "    # takes in instagramMedia pickle and scrape date, returns a vector that\n",
    "    # contains a normalized percentage (0-1) for the usage of the filters\n",
    "    # listed below for the past 2 weeks: \n",
    "    # Valencia, X-Pro II, Hefe, Amaro, Rise, Willow, Crema, Inkwell\n",
    "    \n",
    "    # output example: [0.5,0,0,0,0,0.5,0,0] \n",
    "    # interpretation: user used valencia half the time, Willow the other half\n",
    "    # of time, for the past 2 weeks of Instagram posts.\n",
    "    def instagramFilterVector(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        filters = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "        filtervec = np.zeros((8,))\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            \n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "                    \n",
    "        for i in range(0,saved_index):\n",
    "            filt = json.loads(a[i])['filter']\n",
    "            if(filt in filters):\n",
    "                filters[filt] += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for i in range(0,8):\n",
    "            filtervec[i] = filters[list(filters)[i]]\n",
    "\n",
    "        # percentage of how much the filters are used as a normalized vector\n",
    "        return filtervec/(saved_index+1)\n",
    "    \n",
    "    # takes in InstagramMedia, returns comment and like frequency for the \n",
    "    # past 2 weeks.\n",
    "    def instagramLikeComFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        counts = np.zeros((2,))\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "                \n",
    "        for i in range(0,saved_index):\n",
    "            counts[0] += json.loads(instagramMedia[i])['likes']['count']\n",
    "            counts[1] += json.loads(instagramMedia[i])['comments']['count']\n",
    "        \n",
    "            \n",
    "        # because dividing by 0 is a big No-No\n",
    "        if (not(counts[0] != 0 and counts[1] != 0)):\n",
    "            return np.zeros((2,))\n",
    "        else:\n",
    "            # likes per post for past 2 weeks\n",
    "            return counts/saved_index\n",
    "\n",
    "\n",
    "    # takes in instagramMedia pickle and scrapedate, returns IG post \n",
    "    # frequency for the past 2 weeks\n",
    "    def instagramPostFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "        \n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "                \n",
    "        return (saved_index+1)/14\n",
    "        \n",
    "        \n",
    "    # takes instagramMedia, returns pixelwise average values for [H,S,V]\n",
    "    # (Hue, Saturation, Value) for all posts in the past 2 weeks, the \n",
    "    # count of faces as a frequency of faces per picture, for the past\n",
    "    # 2 weeks as well.\n",
    "    def averageHSVF(self, instagramMedia, scrapedate):\n",
    "\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        ## ms into secs\n",
    "        scrapedate = int(str(scrapedate)[:-3]) \n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if(os.path.exists(\"./ILLSTOPBLINKINGSOON\")):\n",
    "            shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        os.mkdir('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        avgs = np.zeros((4,))\n",
    "\n",
    "        ## to avoid division by 0\n",
    "        if(saved_index == 0):\n",
    "            return avgs\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(0,saved_index):\n",
    "            url = json.loads(instagramMedia[i])['images']['thumbnail']['url']\n",
    "            urllib.request.urlretrieve(url, './ILLSTOPBLINKINGSOON/' + str(i) + '.jpg')\n",
    "\n",
    "\n",
    "        # face_cascade here is a pre trained classifier for frontal faces \n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "            \n",
    "        avgHue = 0\n",
    "        avgSatur = 0\n",
    "        avgVal = 0\n",
    "        avgFaces = 0\n",
    "\n",
    "        ## GODS OF PROGRAMMING, FORGIVE ME FOR THIS TRIPLE NEST\n",
    "\n",
    "        for k in range(0, saved_index):\n",
    "            ## BGR and not RGB because imread reads in BGR\n",
    "            img = cv2.imread('./ILLSTOPBLINKINGSOON/' + str(k) + '.jpg')\n",
    "            hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "            \n",
    "            avgFaces += len(faces)\n",
    "            \n",
    "            for i in range(0,hsv.shape[0]):\n",
    "                for j in range(0,hsv.shape[1]):\n",
    "                    avgHue += hsv[i,j,0]\n",
    "                    avgSatur += hsv[i,j,1]\n",
    "                    avgVal += hsv[i,j,2]\n",
    "                    \n",
    "        \n",
    "\n",
    "        sums = [avgHue,avgSatur,avgVal]\n",
    "\n",
    "        ## 22500 = 150x150 = instagram photo thumbnail shape\n",
    "        avgs = list(map(lambda x: x/(22500*saved_index), sums)).append(avgFaces/saved_index)\n",
    "\n",
    "        shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        return avgs      \n",
    "\n",
    "    \n",
    "    #input is texts pickle, return master vector \n",
    "    def embeddingToMastersumText(self, texts):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(texts)):\n",
    "            try:\n",
    "                masterSum += self.textToEmbedding(texts[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is single text, returns vector embedding of entire text.\n",
    "    def textToEmbedding(self, text):\n",
    "        \n",
    "        q = json.loads(text)[\"body\"].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # returns [q1,q2,q3,q4,q5,q6,q7,q8,q9] and sum of all these scores\n",
    "    def labelGenerator(self, phq):\n",
    "        \n",
    "        labelVector = np.zeros((10,))\n",
    "        sumOfScores = 0\n",
    "        \n",
    "        print(phq)\n",
    "        \n",
    "        for i in range(0,9):\n",
    "            temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "            labelVector[i] = temp\n",
    "            labelVector[9] += temp\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "\n",
    "f = Featurizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.009s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestFeaturizer(unittest.TestCase):\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    \n",
    "    def test_dailyTextFreq(self):\n",
    "        self.assertEqual(3,3)\n",
    "        \n",
    "    def test_dailyTextFre(self):\n",
    "        self.assertEqual(4,4)\n",
    "        \n",
    "    def add(self,x,y):\n",
    "        return x + y\n",
    "        \n",
    "    def test_errorexample(self):\n",
    "        self.assertRaises(TypeError,self.add,\"3\",4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
