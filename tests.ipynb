{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapedate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-008be87e8ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmseconds_in_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m86400000\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtime_during_week_ub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrapedate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmseconds_in_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtime_during_week_lb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrapedate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmseconds_in_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scrapedate' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "import cv2\n",
    "import urllib.request\n",
    "import sqlite3 as sql\n",
    "import base64\n",
    "import ffmpy3\n",
    "from file_read_backwards import FileReadBackwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Featurizer featurizes methods that convert json objects of the appropriate type into features\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.name = name\n",
    "\n",
    "    # takes in text pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of texts sent in the 24 hours before the scrape\n",
    "    # the last element is count of texts sent on the 24 hour window 14 days before the scrapedate\n",
    "    def textFreqVec14(self, text, scrapedate):\n",
    "        \n",
    "        textFreqVec = np.zeros((14,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # assuming unordered texts (WHICH TURNS OUT IS THE CASE)\n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+1)*mseconds_in_day)\n",
    "            for t in range(0,len(text)):\n",
    "                text_date = int(json.loads(text[t])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > text_date) and (text_date > time_during_week_lb)):\n",
    "                    textFreqVec[day] += 1.0\n",
    "                    \n",
    "        return textFreqVec\n",
    "    \n",
    "\n",
    "    # takes in call pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of calls sent in the 24 hours before the scrape\n",
    "    # the last element is count of calls sent on the 24 hour window 14 days before the scrapedate\n",
    "    def callFreqVec14(self, call, scrapedate):\n",
    "        \n",
    "        callFreqVec = np.zeros((14,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+1)*mseconds_in_day)\n",
    "            for c in range(0,len(call)):\n",
    "                call_date = int(json.loads(call[c])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > call_date) and (call_date > time_during_week_lb)):\n",
    "                    callFreqVec[day] += 1.0\n",
    "                    \n",
    "        return callFreqVec\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, return master vector\n",
    "    def embeddingToMastersum(self, tweets):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(tweets)):\n",
    "            try:\n",
    "                masterSum += self.tweetToEmbedding(tweets[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is one single tweet, returns vector embedding of entire tweet.\n",
    "    # eg: responseobject.json()[0]\n",
    "    def tweetToEmbedding(self, tweet):\n",
    "\n",
    "        q = tweet['text'].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, returns follow count\n",
    "    def followerCount(self, tweets):\n",
    "\n",
    "        followerCount = 0\n",
    "\n",
    "        # get\n",
    "        try:\n",
    "            followerCount = json.loads(tweets[0])['user']['followers_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followerCount\n",
    "\n",
    "    # input is tweets pickle, returns friend count\n",
    "    def followingCount(self, tweets):\n",
    "\n",
    "        followingCount = 0\n",
    "\n",
    "        try:\n",
    "            followingCount = json.loads(tweets[0])['user']['friends_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followingCount\n",
    "\n",
    "    # input is tweets pickle, return avg likes per post for the last 2 weeks\n",
    "    def twitterLikeFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitLikeVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitLikeVec[0] += 1.0\n",
    "                    \n",
    "        return twitLikeVec/14\n",
    "\n",
    "\n",
    "    # input is tweets pickle, return avg retweets per post for the last 2 weeks\n",
    "    def twitterRetweetFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitRTVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitRTVec[0] += json.loads(tweets[i])['favorite_count']\n",
    "                    \n",
    "        return twitRTVec/14\n",
    "    \n",
    "    # input is contacts pickle, returns number of contacts\n",
    "    def numOfContacts(self, contacts):\n",
    "        return len(contacts)\n",
    "    \n",
    "    # input is instagram pickle, return two features: follows count, followed by count  \n",
    "    def instagramThings(self, instagram):\n",
    "        \n",
    "        followsFollowed = np.zeros((2,))\n",
    "        \n",
    "        try:\n",
    "            if(type(json.loads(instagram[0])) == str):\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            else: # its a dictionary like it's supposed to be\n",
    "                followsFollowed[0] = json.loads(instagram[0])['data']['counts']['follows']\n",
    "                followsFollowed[1] = json.loads(instagram[0])['data']['counts']['followed_by']\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        return followsFollowed\n",
    "    \n",
    "    # takes in instagramMedia pickle scrape date, spits out filter usage frequency for the past 2 weeks\n",
    "    def instagramFilterFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        instaFiltVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_at']\n",
    "            \n",
    "            if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                instaFiltVec[0] += 1.0\n",
    "                    \n",
    "       \n",
    "        nofilter_count = 0\n",
    "\n",
    "        for i in range(0,instaFiltVec[0]):\n",
    "            if(json.loads(tweets[i])['filter'] == Normal):\n",
    "                nofilter_count += 1\n",
    "        \n",
    "        instaFiltVec[0] -= nofilter_count\n",
    "        \n",
    "        return instaFiltVec\n",
    "        \n",
    "       \n",
    "    \n",
    "    # takes in instagramMedia pickle and scrape date, returns a vector that\n",
    "    # contains a normalized percentage (0-1) for the usage of the filters\n",
    "    # listed below for the past 2 weeks: \n",
    "    # Valencia, X-Pro II, Hefe, Amaro, Rise, Willow, Crema, Inkwell\n",
    "    \n",
    "    # output example: [0.5,0,0,0,0,0.5,0,0] \n",
    "    # interpretation: user used valencia half the time, Willow the other half\n",
    "    # of time, for the past 2 weeks of Instagram posts.1`sas\n",
    "    def instagramFilterVector(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        filters = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "        filtervec = np.zeros((8,))\n",
    "        \n",
    "        numposts = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "            \n",
    "            if(this != '[object Object]'):\n",
    "                igpost_date = int(json.loads(this)['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                numposts += 1.0\n",
    "                \n",
    "                \n",
    "        for i in range(0,numposts):\n",
    "            filt = json.loads(instagramMedia[i])['filter']\n",
    "            if(filt in filters):\n",
    "                filters[filt] += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for i in range(0,8):\n",
    "            filtervec[i] = filters[list(filters)[i]]\n",
    "\n",
    "        # percentage of how much the filters are used as a normalized vector\n",
    "        return filtervec/(numposts)\n",
    "    \n",
    "    # takes in InstagramMedia, returns comment and like frequency for the \n",
    "    # past 2 weeks.\n",
    "    def instagramLikeComFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        counts = np.zeros((2,))\n",
    "        \n",
    "        postcount = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_at']\n",
    "            \n",
    "            if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount += 1.0\n",
    "        \n",
    "        for i in range(0,postcount):\n",
    "            counts[0] += json.loads(instagramMedia[i])['likes']['count']\n",
    "            counts[1] += json.loads(instagramMedia[i])['comments']['count']\n",
    "        \n",
    "        if (counts[0] + counts[1] == 0):\n",
    "            return np.zeros((2,))\n",
    "        else:\n",
    "            # likes per post for past 2 weeks\n",
    "            return counts/postcount\n",
    "\n",
    "\n",
    "    # takes in instagramMedia pickle and scrapedate, returns IG post \n",
    "    # frequency for the past 2 weeks\n",
    "    def instagramPostFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        postcount = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_at']\n",
    "            \n",
    "            if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount[0] += 1.0\n",
    "                \n",
    "        return postcount/14\n",
    "        \n",
    "        \n",
    "    # takes instagramMedia, returns pixelwise average values for [H,S,V]\n",
    "    # (Hue, Saturation, Value) for all posts in the past 2 weeks, the \n",
    "    # count of faces as a frequency of faces per picture, for the past\n",
    "    # 2 weeks as well.\n",
    "    \n",
    "    # testvariable:\n",
    "    # empty string: \"\" if not testing\n",
    "    # \n",
    "    def averageHSVF(self, instagramMedia, scrapedate):\n",
    "\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        ## ms into secs\n",
    "        scrapedate = int(str(scrapedate)[:-3]) \n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if(os.path.exists(\"./ILLSTOPBLINKINGSOON\")):\n",
    "            shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        os.mkdir('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        avgs = np.zeros((4,))\n",
    "\n",
    "        ## to avoid division by 0\n",
    "        if(saved_index == 0):\n",
    "            return avgs\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(0,saved_index):\n",
    "            url = json.loads(instagramMedia[i])['images']['thumbnail']['url']\n",
    "            urllib.request.urlretrieve(url, './ILLSTOPBLINKINGSOON/' + str(i) + '.jpg')\n",
    "\n",
    "\n",
    "        # face_cascade here is a pre trained classifier for frontal faces \n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "            \n",
    "        avgHue = 0\n",
    "        avgSatur = 0\n",
    "        avgVal = 0\n",
    "        avgFaces = 0\n",
    "\n",
    "        ## GODS OF PROGRAMMING, FORGIVE ME FOR THIS TRIPLE NEST\n",
    "\n",
    "        for k in range(0, saved_index):\n",
    "            ## BGR and not RGB because imread reads in BGR\n",
    "            img = cv2.imread('./ILLSTOPBLINKINGSOON/' + str(k) + '.jpg')\n",
    "            \n",
    "            hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "            \n",
    "            avgFaces += len(faces)\n",
    "            \n",
    "            for i in range(0,hsv.shape[0]):\n",
    "                for j in range(0,hsv.shape[1]):\n",
    "                    avgHue += hsv[i,j,0]\n",
    "                    avgSatur += hsv[i,j,1]\n",
    "                    avgVal += hsv[i,j,2]\n",
    "                    \n",
    "        \n",
    "\n",
    "        sums = [avgHue,avgSatur,avgVal]\n",
    "\n",
    "        ## 22500 = 150x150 = instagram photo thumbnail shape\n",
    "        avgs = list(map(lambda x: x/(22500*saved_index), sums)).append(avgFaces/saved_index)\n",
    "\n",
    "        shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        return avgs      \n",
    "\n",
    "    \n",
    "    #input is texts pickle, return master vector \n",
    "    def embeddingToMastersumText(self, texts):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(texts)):\n",
    "            try:\n",
    "                masterSum += self.textToEmbedding(texts[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is single text, returns vector embedding of entire text.\n",
    "    def textToEmbedding(self, text):\n",
    "        \n",
    "        q = json.loads(text)[\"body\"].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # returns [q1,q2,q3,q4,q5,q6,q7,q8,q9] and sum of all these scores\n",
    "    def labelGenerator(self, phq):\n",
    "        \n",
    "        labelVector = np.zeros((10,))\n",
    "        sumOfScores = 0\n",
    "        \n",
    "        # print(phq)\n",
    "        \n",
    "        for i in range(0,9):\n",
    "            temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "            labelVector[i] = temp\n",
    "            labelVector[9] += temp\n",
    "            \n",
    "        return labelVector\n",
    "\n",
    "    \n",
    "        import base64\n",
    "        import ffmpy3\n",
    "\n",
    "        bytestream = base64.b64decode(voice[0])\n",
    "\n",
    "        fh = open(\"test.3gp\",\"wb\")\n",
    "        fh.write(bytestream)\n",
    "        fh.close()\n",
    "\n",
    "        ff = ffmpy3.FFmpeg( inputs={'test.3gp': None}, outputs={'test1.wav': None})\n",
    "        ff.run()\n",
    "\n",
    "        from file_read_backwards import FileReadBackwards\n",
    "\n",
    "        with FileReadBackwards(\"out.csv\", encoding=\"utf-8\") as frb:\n",
    "            for l in frb:\n",
    "                #print(l)\n",
    "                break\n",
    "\n",
    "        b = l[9:].split(',')\n",
    "        a = list(map(float, b))\n",
    "\n",
    "        \n",
    "    def voiceFeaturizer(self, voice):\n",
    "\n",
    "        audiofeaturevec = np.zeros((1583,))\n",
    "\n",
    "        if(len(voice) == 0):\n",
    "            return audiofeaturevec\n",
    "\n",
    "        # base64 string -> bitstring -> bitstream -> write into 3gp file\n",
    "        bytestream = base64.b64decode(voice[0])\n",
    "        fh = open(\"audio.3gp\",\"wb\")\n",
    "        fh.write(bytestream)\n",
    "        fh.close()\n",
    "\n",
    "        # wav -> 3gp\n",
    "        ff = ffmpy3.FFmpeg( inputs={'audio.3gp': None}, outputs={'audio.wav': None})\n",
    "        ff.run()\n",
    "\n",
    "        os.remove(\"audio.3gp\")\n",
    "\n",
    "        # call to openSMILE\n",
    "        os.system( os.getcwd() + '/openSMILE-2.1.0/bin/linux_x64_standalone_static/SMILExtract -C ' + os.getcwd() + '/openSMILE-2.1.0/config/emobase2010.conf -I audio.wav -O \"out.csv\"')\n",
    "\n",
    "        os.remove(\"audio.wav\")\n",
    "\n",
    "        # csv file has a giant header. last line contains the features we want\n",
    "        # so we read the last line, cut out more useless string with 'l[9:]\n",
    "        with FileReadBackwards(\"out.csv\", encoding=\"utf-8\") as frb:\n",
    "            for l in frb:\n",
    "                b = l[9:].split(',')\n",
    "                break\n",
    "\n",
    "        os.remove(\"out.csv\")\n",
    "\n",
    "        # make list of string into list of floats\n",
    "        a = list(map(float, b))\n",
    "\n",
    "        # make sklearn happy\n",
    "        for i in range(0,len(a)):\n",
    "            audiofeaturevec[i] = a[i]\n",
    "\n",
    "        return audiofeaturevec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1514323274000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# f = Featurizer()\n",
    "\n",
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"Instagram media\" + \".p\", \"rb\" ))        \n",
    "# for i in range(0,len(b1)):\n",
    "#     print(int(json.loads(b1[i])['date'].encode('ascii','ignore')))\n",
    "\n",
    "igpost_date = int(json.loads(b1[0])['created_time']) * 1000\n",
    "igpost_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"audio\" + \".p\", \"rb\" ))        \n",
    "\n",
    "len(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-a76e26a90efb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# base64 string -> bitstring -> bitstream -> write into 3gp file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbytestream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'voice' is not defined"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import ffmpy3\n",
    "import os\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "# base64 string -> bitstring -> bitstream -> write into 3gp file\n",
    "bytestream = base64.b64decode(voice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/ipykernel_launcher.py:15: ResourceWarning: unclosed file <_io.BufferedReader name='datafor16:13/DP5904log.p'>\n",
      "  from ipykernel import kernelapp as app\n",
      "E../home/vape/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: ResourceWarning: unclosed file <_io.BufferedReader name='datafor16:13/DP7276phq.p'>\n",
      "  import sys\n",
      "./home/vape/.local/lib/python3.5/site-packages/ipykernel_launcher.py:11: ResourceWarning: unclosed file <_io.BufferedReader name='datafor16:13/DP0660text.p'>\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "E\n",
      "======================================================================\n",
      "ERROR: test_callFreqVec14 (__main__.TestFeaturizer)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-5a4136992325>\", line 16, in test_callFreqVec14\n",
      "    self.assertSequenceEqual(f.callFreqVec14(b1,1515042722290),[23, 34, 16, 6, 8, 26, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  File \"/usr/lib/python3.5/unittest/case.py\", line 944, in assertSequenceEqual\n",
      "    if seq1 == seq2:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_textFreqVec14 (__main__.TestFeaturizer)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-5a4136992325>\", line 12, in test_textFreqVec14\n",
      "    self.assertSequenceEqual(f.textFreqVec14(b1,1515042722290),[2, 3, 0, 0, 2, 5, 4, 2, 2, 2, 1, 1, 2, 0])\n",
      "  File \"/usr/lib/python3.5/unittest/case.py\", line 944, in assertSequenceEqual\n",
      "    if seq1 == seq2:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.093s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    }
   ],
   "source": [
    "class TestFeaturizer(unittest.TestCase):\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    \n",
    "    def test_labelGenerator(self):\n",
    "        b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"7276\" +  \"phq\" + \".p\", \"rb\" ))\n",
    "        self.assertSequenceEqual(f.labelGenerator(b1).tolist(),[3,1,0,2,0,0,0,0,0,6])\n",
    "        \n",
    "    def test_textFreqVec14(self):\n",
    "        b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"0660\" +  \"text\" + \".p\", \"rb\" ))\n",
    "        self.assertSequenceEqual(f.textFreqVec14(b1,1515042722290),[2, 3, 0, 0, 2, 5, 4, 2, 2, 2, 1, 1, 2, 0])\n",
    "        \n",
    "    def test_callFreqVec14(self):\n",
    "        b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"5904\" +  \"log\" + \".p\", \"rb\" ))\n",
    "        self.assertSequenceEqual(f.callFreqVec14(b1,1515042722290),[23, 34, 16, 6, 8, 26, 0, 0, 0, 0, 0, 0, 0, 0])    \n",
    "        \n",
    "    def test_dailyTextFre(self):\n",
    "        self.assertEqual(4,4)\n",
    "        \n",
    "    def add(self,x,y):\n",
    "        return x + y\n",
    "        \n",
    "    def test_errorexample(self):\n",
    "        self.assertRaises(TypeError,self.add,\"3\",4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1514323274"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"Instagram media\" + \".p\", \"rb\" ))\n",
    "len(b1)\n",
    "\n",
    "igpost_date = int(json.loads(b1[0])['created_time'])\n",
    "igpost_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linear', 'rbf', 'poly', 'sigmoid', 'precomputed')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# replace missing values with mean of their corresponding features\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "mtr = imp.fit_transform(mtr)\n",
    "\n",
    "# normalize (features now have gauss dist., 0 mean and unit variance)\n",
    "mtr = sklearn.preprocessing.scale(mtr)\n",
    "\n",
    "# all of our data\n",
    "bigBadMatrix = g.featureMatrix\n",
    "\n",
    "# shuffle row-wise\n",
    "np.random.shuffle(bigBadMatrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DATA SPLIT (#nosnooping)\n",
    "\n",
    "# TEST DATA (%15 percent of data)\n",
    "numofppl_index = ftrMtrx.shape[0] - 1\n",
    "cut_index = int(ftrMtrx.shape[0] * 0.85)\n",
    "\n",
    "test_data = bigBadMatrix[cut_index:numofppl,:]\n",
    "\n",
    "# TRAINING AND TEST DATA (%85 percent of data)\n",
    "\n",
    "train_data = bigBadMatrix[0:cut_index,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## hyper parameter optimization\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c_range = list(range(1, 30))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid', 'precomputed'), 'C':c_range}\n",
    "parameters['kernel']\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "np.random.shuffle\n",
    "\n",
    "\n",
    "\n",
    "svc = svm.SVC()\n",
    "grid = GridSearchCV(svc, parameters, cv=2, scoring='accuracy')\n",
    "\n",
    "clf.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Normal'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"Instagram media\" + \".p\", \"rb\" ))\n",
    "\n",
    "filt = json.loads(b1[0])['filter']\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 4,  5,  6],\n",
       "       [ 7,  8,  9],\n",
       "       [10, 11, 12]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], np.int32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [11]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,1:2] # subcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2,:] #subrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  8,  9],\n",
       "       [ 1,  2,  3],\n",
       "       [10, 11, 12],\n",
       "       [ 4,  5,  6]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 9 is out of bounds for axis 0 with size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-4da2c0b65d7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabelVector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlabelVector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mlabelVector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 9 is out of bounds for axis 0 with size 9"
     ]
    }
   ],
   "source": [
    "labelVector = np.zeros((9,))\n",
    "sumOfScores = 0\n",
    "\n",
    "# print(phq)\n",
    "a = {\"Q0\":\"3\",\"Q1\":\"3\",\"Q2\":\"3\",\"Q3\":\"3\",\"Q4\":\"3\",\"Q5\":\"2\",\"Q6\":\"3\",\"Q7\":\"3\",\"Q8\":\"1\"}\n",
    "for i in range(0,8):\n",
    "    temp = int(a['Q' + str(i)])\n",
    "    labelVector[i] = temp\n",
    "    labelVector[9] += temp\n",
    "\n",
    "labelVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((9,))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
