{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapedate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-008be87e8ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmseconds_in_day\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m86400000\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtime_during_week_ub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrapedate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmseconds_in_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtime_during_week_lb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrapedate\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmseconds_in_day\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scrapedate' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "import cv2\n",
    "import urllib.request\n",
    "import sqlite3 as sql\n",
    "import base64\n",
    "import ffmpy3\n",
    "from file_read_backwards import FileReadBackwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Featurizer featurizes methods that convert json objects of the appropriate type into features\n",
    "\n",
    "class Featurizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        #self.name = name\n",
    "\n",
    "    # takes in text pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of texts sent in the 24 hours before the scrape\n",
    "    # the last element is count of texts sent on the 24 hour window 14 days before the scrapedate\n",
    "    def textFreqVec14(self, text, scrapedate):\n",
    "        \n",
    "        textFreqVec = np.zeros((14,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # moving average index\n",
    "        n = 5\n",
    "        \n",
    "        # assuming unordered texts (WHICH TURNS OUT IS THE CASE)\n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+n)*mseconds_in_day)\n",
    "            for t in range(0,len(text)):\n",
    "                text_date = int(json.loads(text[t])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > text_date) and (text_date > time_during_week_lb)):\n",
    "                    textFreqVec[day] += 1.0\n",
    "                    \n",
    "        return textFreqVec\n",
    "    \n",
    "\n",
    "    # takes in call pickle and scrapedate, returns vector of 14 elements\n",
    "    # the first element is count of calls sent in the 24 hours before the scrape\n",
    "    # the last element is count of calls sent on the 24 hour window 14 days before the scrapedate\n",
    "    def callFreqVec14(self, call, scrapedate):\n",
    "        \n",
    "        callFreqVec = np.zeros((14,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # moving average index\n",
    "        n = 5\n",
    "        \n",
    "        for day in range(0,14):\n",
    "            time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "            time_during_week_lb = scrapedate - ((day+n)*mseconds_in_day)\n",
    "            for c in range(0,len(call)):\n",
    "                call_date = int(json.loads(call[c])['date'].encode('ascii','ignore'))\n",
    "                if ((time_during_week_ub > call_date) and (call_date > time_during_week_lb)):\n",
    "                    callFreqVec[day] += 1.0\n",
    "                    \n",
    "        return callFreqVec\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, return master vector\n",
    "    def embeddingToMastersum(self, tweets):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(tweets)):\n",
    "            try:\n",
    "                masterSum += self.tweetToEmbedding(tweets[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is one single tweet, returns vector embedding of entire tweet.\n",
    "    # eg: responseobject.json()[0]\n",
    "    def tweetToEmbedding(self, tweet):\n",
    "\n",
    "        q = tweet['text'].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # input is tweets pickle, returns follow count\n",
    "    def followerCount(self, tweets):\n",
    "\n",
    "        followerCount = 0\n",
    "\n",
    "        # get\n",
    "        try:\n",
    "            followerCount = json.loads(tweets[0])['user']['followers_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followerCount\n",
    "\n",
    "    # input is tweets pickle, returns friend count\n",
    "    def followingCount(self, tweets):\n",
    "\n",
    "        followingCount = 0\n",
    "\n",
    "        try:\n",
    "            followingCount = json.loads(tweets[0])['user']['friends_count']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        return followingCount\n",
    "\n",
    "    # input is tweets pickle, return avg likes per post for the last 2 weeks\n",
    "    def twitterLikeFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitLikeVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitLikeVec[0] += 1.0\n",
    "                    \n",
    "        return twitLikeVec/14\n",
    "\n",
    "\n",
    "    # input is tweets pickle, return avg retweets per post for the last 2 weeks\n",
    "    def twitterRetweetFreq(self, tweets, scrapedate):\n",
    "\n",
    "        twitRTVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "\n",
    "            tweet_date = int(time.mktime(time.strptime(utc,\"%a %b %d %H:%M:%S +0000 %Y\"))) * 1000\n",
    "\n",
    "            if ((time_during_week_ub > tweet_date) and (tweet_date > time_during_week_lb)):\n",
    "                twitRTVec[0] += json.loads(tweets[i])['favorite_count']\n",
    "                    \n",
    "        return twitRTVec/14\n",
    "    \n",
    "    # input is contacts pickle, returns number of contacts\n",
    "    def numOfContacts(self, contacts):\n",
    "        return len(contacts)\n",
    "    \n",
    "    # input is instagram pickle, return two features: follows count, followed by count  \n",
    "    def instagramThings(self, instagram):\n",
    "        \n",
    "        followsFollowed = np.zeros((2,))\n",
    "        \n",
    "        try:\n",
    "            if(type(json.loads(instagram[0])) == str):\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            else: # its a dictionary like it's supposed to be\n",
    "                followsFollowed[0] = json.loads(instagram[0])['data']['counts']['follows']\n",
    "                followsFollowed[1] = json.loads(instagram[0])['data']['counts']['followed_by']\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        return followsFollowed\n",
    "    \n",
    "    # takes in instagramMedia pickle scrape date, spits out filter usage frequency for the past 2 weeks\n",
    "    def instagramFilterFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        instaFiltVec = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_at']\n",
    "            \n",
    "            if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                instaFiltVec[0] += 1.0\n",
    "                    \n",
    "       \n",
    "        nofilter_count = 0\n",
    "\n",
    "        for i in range(0,instaFiltVec[0]):\n",
    "            if(json.loads(tweets[i])['filter'] == Normal):\n",
    "                nofilter_count += 1\n",
    "        \n",
    "        instaFiltVec[0] -= nofilter_count\n",
    "        \n",
    "        return instaFiltVec\n",
    "        \n",
    "       \n",
    "    \n",
    "    # takes in instagramMedia pickle and scrape date, returns a vector that\n",
    "    # contains a normalized percentage (0-1) for the usage of the filters\n",
    "    # listed below for the past 2 weeks: \n",
    "    # Valencia, X-Pro II, Hefe, Amaro, Rise, Willow, Crema, Inkwell\n",
    "    \n",
    "    # output example: [0.5,0,0,0,0,0.5,0,0] \n",
    "    # interpretation: user used valencia half the time, Willow the other half\n",
    "    # of time, for the past 2 weeks of Instagram posts.1`sas\n",
    "    def instagramFilterVector(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        filters = {'Valencia':0,'X-Pro II':0, 'Hefe':0, 'Amaro':0, 'Rise':0, 'Willow':0, 'Crema':0, 'Inkwell':0}\n",
    "        filtervec = np.zeros((8,))\n",
    "        \n",
    "        numposts = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(tweets)):\n",
    "\n",
    "            utc = json.loads(tweets[t])['created_at']\n",
    "            \n",
    "            if(this != '[object Object]'):\n",
    "                igpost_date = int(json.loads(this)['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                numposts += 1.0\n",
    "                \n",
    "                \n",
    "        for i in range(0,numposts):\n",
    "            filt = json.loads(instagramMedia[i])['filter']\n",
    "            if(filt in filters):\n",
    "                filters[filt] += 1\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        for i in range(0,8):\n",
    "            filtervec[i] = filters[list(filters)[i]]\n",
    "\n",
    "        # percentage of how much the filters are used as a normalized vector\n",
    "        return filtervec/(numposts)\n",
    "    \n",
    "    # takes in InstagramMedia, returns comment and like frequency for the \n",
    "    # past 2 weeks.\n",
    "    def instagramLikeComFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        counts = np.zeros((2,))\n",
    "        \n",
    "        postcount = 0\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_at']\n",
    "            \n",
    "            if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount += 1.0\n",
    "        \n",
    "        for i in range(0,postcount):\n",
    "            counts[0] += json.loads(instagramMedia[i])['likes']['count']\n",
    "            counts[1] += json.loads(instagramMedia[i])['comments']['count']\n",
    "        \n",
    "        if (counts[0] + counts[1] == 0):\n",
    "            return np.zeros((2,))\n",
    "        else:\n",
    "            # likes per post for past 2 weeks\n",
    "            return counts/postcount\n",
    "\n",
    "\n",
    "    # takes in instagramMedia pickle and scrapedate, returns IG post \n",
    "    # frequency for the past 2 weeks\n",
    "    def instagramPostFreq(self, instagramMedia, scrapedate):\n",
    "        \n",
    "        postcount = np.zeros((1,))\n",
    "        \n",
    "        mseconds_in_twoweeks = 1209600000;\n",
    "        mseconds_in_day = 86400000;\n",
    "        \n",
    "        # upper and lower bounds\n",
    "        time_during_week_ub = scrapedate - ((0)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((14)*mseconds_in_day)\n",
    "        for t in range(0,len(instagramMedia)):\n",
    "\n",
    "            utc = json.loads(instagramMedia[t])['created_at']\n",
    "            \n",
    "            if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time']) * 1000\n",
    "            else:\n",
    "                print(\"DAMN YOU DAMON!!(ig)\")\n",
    "\n",
    "            if ((time_during_week_ub > igpost_date) and (igpost_date > time_during_week_lb)):\n",
    "                postcount[0] += 1.0\n",
    "                \n",
    "        return postcount/14\n",
    "        \n",
    "        \n",
    "    # takes instagramMedia, returns pixelwise average values for [H,S,V]\n",
    "    # (Hue, Saturation, Value) for all posts in the past 2 weeks, the \n",
    "    # count of faces as a frequency of faces per picture, for the past\n",
    "    # 2 weeks as well.\n",
    "    \n",
    "    # testvariable:\n",
    "    # empty string: \"\" if not testing\n",
    "    # \n",
    "    def averageHSVF(self, instagramMedia, scrapedate):\n",
    "\n",
    "\n",
    "        seconds_intwo_weeks = 1209600;\n",
    "\n",
    "        ## ms into secs\n",
    "        scrapedate = int(str(scrapedate)[:-3]) \n",
    "\n",
    "        two_weeks_prior = scrapedate - seconds_intwo_weeks\n",
    "\n",
    "        igpost_date = 0\n",
    "        saved_index = 0\n",
    "\n",
    "        a = instagramMedia\n",
    "\n",
    "        # find the index of the oldest call that is not older than the date \"two weeks prior\"\n",
    "        for i in range(0,len(a)):\n",
    "            try:\n",
    "                this = a[len(a)-(i+1)]\n",
    "                if(this != '[object Object]'):\n",
    "                    igpost_date = int(json.loads(this)['created_time'])\n",
    "                else:\n",
    "                    print(\"DAMN YOU DAMON!!(ig)\")\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "            if (two_weeks_prior < igpost_date):\n",
    "                saved_index = len(a) - (i+1)\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if(os.path.exists(\"./ILLSTOPBLINKINGSOON\")):\n",
    "            shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        os.mkdir('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        avgs = np.zeros((4,))\n",
    "\n",
    "        ## to avoid division by 0\n",
    "        if(saved_index == 0):\n",
    "            return avgs\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(0,saved_index):\n",
    "            url = json.loads(instagramMedia[i])['images']['thumbnail']['url']\n",
    "            urllib.request.urlretrieve(url, './ILLSTOPBLINKINGSOON/' + str(i) + '.jpg')\n",
    "\n",
    "\n",
    "        # face_cascade here is a pre trained classifier for frontal faces \n",
    "        face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "            \n",
    "        avgHue = 0\n",
    "        avgSatur = 0\n",
    "        avgVal = 0\n",
    "        avgFaces = 0\n",
    "\n",
    "        ## GODS OF PROGRAMMING, FORGIVE ME FOR THIS TRIPLE NEST\n",
    "\n",
    "        for k in range(0, saved_index):\n",
    "            ## BGR and not RGB because imread reads in BGR\n",
    "            img = cv2.imread('./ILLSTOPBLINKINGSOON/' + str(k) + '.jpg')\n",
    "            \n",
    "            hsv = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "            \n",
    "            grayImage = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(grayImage,  scaleFactor=1.1, minNeighbors=5, flags = cv2.CASCADE_SCALE_IMAGE)\n",
    "            \n",
    "            avgFaces += len(faces)\n",
    "            \n",
    "            for i in range(0,hsv.shape[0]):\n",
    "                for j in range(0,hsv.shape[1]):\n",
    "                    avgHue += hsv[i,j,0]\n",
    "                    avgSatur += hsv[i,j,1]\n",
    "                    avgVal += hsv[i,j,2]\n",
    "                    \n",
    "        \n",
    "\n",
    "        sums = [avgHue,avgSatur,avgVal]\n",
    "\n",
    "        ## 22500 = 150x150 = instagram photo thumbnail shape\n",
    "        avgs = list(map(lambda x: x/(22500*saved_index), sums)).append(avgFaces/saved_index)\n",
    "\n",
    "        shutil.rmtree('./ILLSTOPBLINKINGSOON')\n",
    "\n",
    "        return avgs      \n",
    "\n",
    "    \n",
    "    #input is texts pickle, return master vector \n",
    "    def embeddingToMastersumText(self, texts):\n",
    "        \n",
    "        masterSum = np.zeros((300,))\n",
    "\n",
    "        # add every word vector into master sum\n",
    "        for i in range(0,len(texts)):\n",
    "            try:\n",
    "                masterSum += self.textToEmbedding(texts[i])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return masterSum\n",
    "\n",
    "    \n",
    "    # input is single text, returns vector embedding of entire text.\n",
    "    def textToEmbedding(self, text):\n",
    "        \n",
    "        q = json.loads(text)[\"body\"].split()\n",
    "\n",
    "        sumVector = np.zeros((300,))\n",
    "\n",
    "        # turn every word into embedding for 1 tweet, add all vectors\n",
    "        for i in range(0,len(q)):\n",
    "            try:\n",
    "                sumVector += model[q[i]]\n",
    "            except KeyError:\n",
    "                pass\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "        return sumVector\n",
    "    \n",
    "    \n",
    "    # returns [q1,q2,q3,q4,q5,q6,q7,q8,q9] and sum of all these scores\n",
    "    def labelGenerator(self, phq):\n",
    "        \n",
    "        labelVector = np.zeros((10,))\n",
    "        sumOfScores = 0\n",
    "        \n",
    "        # print(phq)\n",
    "        \n",
    "        for i in range(0,9):\n",
    "            temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "            labelVector[i] = temp\n",
    "            labelVector[9] += temp\n",
    "            \n",
    "        return labelVector\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    def voiceFeaturizer(self, voice):\n",
    "\n",
    "        audiofeaturevec = np.zeros((1583,))\n",
    "\n",
    "        if(len(voice) == 0):\n",
    "            return audiofeaturevec\n",
    "\n",
    "        # base64 string -> bitstring -> bitstream -> write into 3gp file\n",
    "        bytestream = base64.b64decode(voice[0])\n",
    "        fh = open(\"audio.3gp\",\"wb\")\n",
    "        fh.write(bytestream)\n",
    "        fh.close()\n",
    "\n",
    "        # wav -> 3gp\n",
    "        ff = ffmpy3.FFmpeg( inputs={'audio.3gp': None}, outputs={'audio.wav': None})\n",
    "        ff.run()\n",
    "\n",
    "        os.remove(\"audio.3gp\")\n",
    "\n",
    "        # call to openSMILE\n",
    "        os.system( os.getcwd() + '/openSMILE-2.1.0/bin/linux_x64_standalone_static/SMILExtract -C ' + os.getcwd() + '/openSMILE-2.1.0/config/emobase2010.conf -I audio.wav -O \"out.csv\"')\n",
    "\n",
    "        os.remove(\"audio.wav\")\n",
    "\n",
    "        # csv file has a giant header. last line contains the features we want\n",
    "        # so we read the last line, cut out more useless string with 'l[9:]\n",
    "        with FileReadBackwards(\"out.csv\", encoding=\"utf-8\") as frb:\n",
    "            for l in frb:\n",
    "                b = l[9:].split(',')\n",
    "                break\n",
    "\n",
    "        os.remove(\"out.csv\")\n",
    "\n",
    "        # make list of string into list of floats\n",
    "        a = list(map(float, b))\n",
    "\n",
    "        # make sklearn happy\n",
    "        for i in range(0,len(a)):\n",
    "            audiofeaturevec[i] = a[i]\n",
    "\n",
    "        return audiofeaturevec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1514323274000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# f = Featurizer()\n",
    "\n",
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"Instagram media\" + \".p\", \"rb\" ))        \n",
    "# for i in range(0,len(b1)):\n",
    "#     print(int(json.loads(b1[i])['date'].encode('ascii','ignore')))\n",
    "\n",
    "igpost_date = int(json.loads(b1[0])['created_time']) * 1000\n",
    "igpost_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"audio\" + \".p\", \"rb\" ))        \n",
    "\n",
    "len(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-a76e26a90efb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# base64 string -> bitstring -> bitstream -> write into 3gp file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbytestream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'voice' is not defined"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import ffmpy3\n",
    "import os\n",
    "from file_read_backwards import FileReadBackwards\n",
    "\n",
    "# base64 string -> bitstring -> bitstream -> write into 3gp file\n",
    "bytestream = base64.b64decode(voice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vape/.local/lib/python3.5/site-packages/ipykernel_launcher.py:15: ResourceWarning: unclosed file <_io.BufferedReader name='datafor16:13/DP5904log.p'>\n",
      "  from ipykernel import kernelapp as app\n",
      "E../home/vape/.local/lib/python3.5/site-packages/ipykernel_launcher.py:7: ResourceWarning: unclosed file <_io.BufferedReader name='datafor16:13/DP7276phq.p'>\n",
      "  import sys\n",
      "./home/vape/.local/lib/python3.5/site-packages/ipykernel_launcher.py:11: ResourceWarning: unclosed file <_io.BufferedReader name='datafor16:13/DP0660text.p'>\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "E\n",
      "======================================================================\n",
      "ERROR: test_callFreqVec14 (__main__.TestFeaturizer)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-5a4136992325>\", line 16, in test_callFreqVec14\n",
      "    self.assertSequenceEqual(f.callFreqVec14(b1,1515042722290),[23, 34, 16, 6, 8, 26, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "  File \"/usr/lib/python3.5/unittest/case.py\", line 944, in assertSequenceEqual\n",
      "    if seq1 == seq2:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_textFreqVec14 (__main__.TestFeaturizer)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-72-5a4136992325>\", line 12, in test_textFreqVec14\n",
      "    self.assertSequenceEqual(f.textFreqVec14(b1,1515042722290),[2, 3, 0, 0, 2, 5, 4, 2, 2, 2, 1, 1, 2, 0])\n",
      "  File \"/usr/lib/python3.5/unittest/case.py\", line 944, in assertSequenceEqual\n",
      "    if seq1 == seq2:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.093s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    }
   ],
   "source": [
    "class TestFeaturizer(unittest.TestCase):\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
    "    \n",
    "    def test_labelGenerator(self):\n",
    "        b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"7276\" +  \"phq\" + \".p\", \"rb\" ))\n",
    "        self.assertSequenceEqual(f.labelGenerator(b1).tolist(),[3,1,0,2,0,0,0,0,0,6])\n",
    "        \n",
    "    def test_textFreqVec14(self):\n",
    "        b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"0660\" +  \"text\" + \".p\", \"rb\" ))\n",
    "        self.assertSequenceEqual(f.textFreqVec14(b1,1515042722290),[2, 3, 0, 0, 2, 5, 4, 2, 2, 2, 1, 1, 2, 0])\n",
    "        \n",
    "    def test_callFreqVec14(self):\n",
    "        b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"5904\" +  \"log\" + \".p\", \"rb\" ))\n",
    "        self.assertSequenceEqual(f.callFreqVec14(b1,1515042722290),[23, 34, 16, 6, 8, 26, 0, 0, 0, 0, 0, 0, 0, 0])    \n",
    "        \n",
    "    def test_dailyTextFre(self):\n",
    "        self.assertEqual(4,4)\n",
    "        \n",
    "    def add(self,x,y):\n",
    "        return x + y\n",
    "        \n",
    "    def test_errorexample(self):\n",
    "        self.assertRaises(TypeError,self.add,\"3\",4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1514323274"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"Instagram media\" + \".p\", \"rb\" ))\n",
    "len(b1)\n",
    "\n",
    "igpost_date = int(json.loads(b1[0])['created_time'])\n",
    "igpost_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linear', 'rbf', 'poly', 'sigmoid', 'precomputed')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## PREPROCESSING\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# replace missing values with mean of their corresponding features\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "mtr = imp.fit_transform(mtr)\n",
    "\n",
    "# normalize (features now have gauss dist., 0 mean and unit variance)\n",
    "mtr = sklearn.preprocessing.scale(mtr)\n",
    "\n",
    "# all of our data\n",
    "bigBadMatrix = g.featureMatrix\n",
    "\n",
    "# shuffle row-wise\n",
    "np.random.shuffle(bigBadMatrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DATA SPLIT (#nosnooping)\n",
    "\n",
    "# TEST DATA (%15 percent of data)\n",
    "numofppl_index = ftrMtrx.shape[0] - 1\n",
    "cut_index = int(ftrMtrx.shape[0] * 0.85)\n",
    "\n",
    "test_data = bigBadMatrix[cut_index:numofppl,:]\n",
    "\n",
    "# TRAINING AND TEST DATA (%85 percent of data)\n",
    "\n",
    "train_data = bigBadMatrix[0:cut_index,:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## hyper parameter optimization\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "c_range = list(range(1, 30))\n",
    "parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid', 'precomputed'), 'C':c_range}\n",
    "parameters['kernel']\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "np.random.shuffle\n",
    "\n",
    "\n",
    "\n",
    "svc = svm.SVC()\n",
    "grid = GridSearchCV(svc, parameters, cv=2, scoring='accuracy')\n",
    "\n",
    "clf.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Normal'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"Instagram media\" + \".p\", \"rb\" ))\n",
    "\n",
    "filt = json.loads(b1[0])['filter']\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 4,  5,  6],\n",
       "       [ 7,  8,  9],\n",
       "       [10, 11, 12]], dtype=int32)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], np.int32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [10]], dtype=int32)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0:1] # subcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]], dtype=int32)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:2,:] #subrow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  8,  9],\n",
       "       [ 1,  2,  3],\n",
       "       [10, 11, 12],\n",
       "       [ 4,  5,  6]], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "au\n",
      "ig\n",
      "txt\n",
      "con\n",
      "tw\n",
      "call\n",
      "all\n"
     ]
    }
   ],
   "source": [
    "ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"]\n",
    "for i in ftypes:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.,  2.,  3.,  3.,  2.,  2.,  1.,  0.,  2., 18.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(phq)\n",
    "phq = pickle.load(open( \"datafor23:41\" + \"/DP\" + \"9984\" +  \"phq\" + \".p\", \"rb\" ))\n",
    "labelVector = np.zeros((10,))\n",
    "sumOfScores = 0\n",
    "\n",
    "# print(phq)\n",
    "\n",
    "for i in range(0,9):\n",
    "    print(i)\n",
    "    temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "    \n",
    "    labelVector[i] = temp\n",
    "    labelVector[9] += temp\n",
    "\n",
    "labelVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3 as sql\n",
    "# connect to file\n",
    "conn = sql.connect('phonedata.db')\n",
    "# create cursor for making calls to database\n",
    "c = conn.cursor()\n",
    "\n",
    "list_of_approved_ids = []\n",
    "text_file = open(\"codes.txt\", \"r\")\n",
    "list_of_approved_ids = text_file.read().split('\\n')\n",
    "del list_of_approved_ids[-1]\n",
    "\n",
    "list_of_ids = []\n",
    "\n",
    "\n",
    "\n",
    "for row in c.execute('SELECT * FROM ids'):\n",
    "    # one result\n",
    "    if (row[0] in list_of_approved_ids):\n",
    "        list_of_ids.append(row[0])\n",
    "    \n",
    "\n",
    "len(list_of_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-949407e20156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_of_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# print(phq)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mphq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"datafor23:41\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/DP\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;34m\"phq\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabelVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "list_of_ids = l.lids()\n",
    "for idd in range(0,len(list_of_ids)):\n",
    "    # print(phq)\n",
    "    phq = pickle.load(open( \"datafor23:41\" + \"/DP\" + str(list_of_ids[idd]) +  \"phq\" + \".p\", \"rb\" ))\n",
    "    labelVector = np.zeros((10,))\n",
    "    sumOfScores = 0\n",
    "\n",
    "    # print(phq)\n",
    "\n",
    "    for i in range(0,9):\n",
    "        print(i)\n",
    "        temp = int(json.loads(phq[0])['Q' + str(i)])\n",
    "\n",
    "        labelVector[i] = temp\n",
    "        labelVector[9] += temp\n",
    "\n",
    "    print(labelVector[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  2],\n",
       "       [ 4,  6,  5],\n",
       "       [ 7,  9,  8],\n",
       "       [10, 12, 11]], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((x[:,0:1],x[:,2:3], x[:,1:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "featureVectorCF = np.zeros((14,1))\n",
    "featureVectorCF[2:3,:] = np.nan\n",
    "featureVectorCF\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pandas.read_csv('SMSSpamCollection',\n",
    "                           sep='\\t',names=['labels','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I slept pretty well last night '"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"text\" + \".p\", \"rb\" ))\n",
    "len(b1)\n",
    "\n",
    "q = json.loads(b1[0])[\"body\"]\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = text_process(q)\n",
    "text = nltk.Text(q)\n",
    "tags = nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('slept', 'NN'),\n",
       " ('pretty', 'RB'),\n",
       " ('well', 'RB'),\n",
       " ('last', 'JJ'),\n",
       " ('night', 'NN')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newtagdic = {}\n",
    "for key in tagdict.keys():\n",
    "    newtagdic[key] = 0\n",
    "\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n",
      "RB\n",
      "RB\n",
      "JJ\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "for tag in tags:\n",
    "    print(tag[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'JJ': 1, 'NN': 2, 'RB': 2})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(tag for word,tag in tags)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['RBS', 'JJS', 'RBR', 'CC', 'PDT', '(', 'MD', '$', 'VB', 'VBN', '``', 'NNS', 'WP$', 'CD', '.', 'PRP', ':', 'WP', 'JJR', 'FW', 'DT', 'LS', 'EX', 'JJ', 'IN', 'PRP$', 'NN', 'POS', 'VBG', 'WDT', 'VBZ', 'RB', ',', 'TO', 'NNP', 'VBP', 'SYM', 'WRB', ')', 'VBD', '--', 'NNPS', 'UH', \"''\", 'RP'])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "tagdict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"text\" + \".p\", \"rb\" ))\n",
    "\n",
    "q = json.loads(b1[0])[\"body\"]\n",
    "\n",
    "\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def SentAnalysis(text, scrapedate):\n",
    "    \n",
    "    if(len(text) == 0):\n",
    "        return np.zeros((14,))\n",
    "    \n",
    "    SentFreqVec = np.zeros((14,))\n",
    "    \n",
    "    def text_process(mess):\n",
    "        # Remove all punctuation, stopwords\n",
    "        nopunc = [char for char in mess if char not in string.punctuation]\n",
    "        nopunc = ''.join(nopunc)\n",
    "        return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    # moving average index\n",
    "    n = 10\n",
    "\n",
    "    mseconds_in_twoweeks = 1209600000;\n",
    "    mseconds_in_day = 86400000;\n",
    "\n",
    "    # assuming unordered texts (WHICH TURNS OUT IS THE CASE)\n",
    "    for day in range(0,14):\n",
    "        time_during_week_ub = scrapedate - ((day)*mseconds_in_day)    \n",
    "        time_during_week_lb = scrapedate - ((day+n)*mseconds_in_day)\n",
    "        for t in range(0,len(text)):\n",
    "            text_date = int(json.loads(text[t])['date'].encode('ascii','ignore'))\n",
    "            if ((time_during_week_ub > text_date) and (text_date > time_during_week_lb)):\n",
    "                \n",
    "                body = json.loads(text[t])[\"body\"]\n",
    "                blob = TextBlob(\" \".join(text_process(body)))\n",
    "                \n",
    "                SentFreqVec[day] += blob.sentiment.polarity\n",
    "    \n",
    "    return SentFreqVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.77980519,  9.87266234,  8.87948052,  8.35296537, 12.62329004,\n",
       "       14.52794372, 18.97794372, 20.28984848, 21.08984848, 23.68984848,\n",
       "       24.91484848, 26.03734578, 27.54290133, 27.79409181])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = pickle.load(open( \"datafor16:13\" + \"/DP\" + \"8619\" +  \"text\" + \".p\", \"rb\" ))\n",
    "\n",
    "x = SentAnalysis(b1, 1515043370034)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# read in the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "k_range = list(range(1, 31))\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-368-90b99ff2b9f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontactsStartEnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontactsStartEnd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "contactsStartEnd = [0,1]\n",
    "contactsStartEnd[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.grid_scores_[0].mean_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n gives nth phq answer\n",
    "# 10 gives sum of all phqs\n",
    "def Yer(y, n):\n",
    "    return y[:,n-1:n]\n",
    "\n",
    "\n",
    "contactsStartEnd = [0,1]\n",
    "twitterStartEnd = [1,5]\n",
    "textStartEnd = [5,78]\n",
    "callStartEnd = [78,92]\n",
    "instagramStartEnd = [92,110]\n",
    "audioStartEnd = [110,1693]\n",
    "allStartEnd = [contactsStartEnd[0], audioStartEnd[1]]\n",
    "\n",
    "\n",
    "ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"]\n",
    "\n",
    "# ftype = \"au\" audio / \"ig\" instagram / \"txt\" text / \"con\" contacts / \"tw\" twitter / \"call\" call\n",
    "# \"all\" = big matrix\n",
    "def Xer(X, ftype):\n",
    "    if(ftype == \"au\"):\n",
    "        return X[:,audioStartEnd[0]:audioStartEnd[1]]\n",
    "    if(ftype == \"ig\"):\n",
    "        return X[:,instagramStartEnd[0]:instagramStartEnd[1]]\n",
    "    if(ftype == \"txt\"):\n",
    "        return X[:,textStartEnd[0]:textStartEnd[1]]\n",
    "    if(ftype == \"con\"):\n",
    "        return X[:,contactsStartEnd[0]:contactsStartEnd[1]]\n",
    "    if(ftype == \"tw\"):\n",
    "        return X[:,twitterStartEnd[0]:twitterStartEnd[1]]\n",
    "    if(ftype == \"call\"):\n",
    "        return X[:,callStartEnd[0]:callStartEnd[1]]\n",
    "    if(ftype == \"all\"):\n",
    "        return X[:,allStartEnd[0]:allStartEnd[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ML TESTINGG\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261, 1703)"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtr = pd.read_csv(\"mtr.csv\").values\n",
    "mtr = np.delete(mtr, 0, axis = 1)\n",
    "mtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  1.,  2.,  1.,  2.,  3.,  2.,  1.,  2., 16.],\n",
       "       [ 1.,  1.,  3.,  2.,  2.,  1.,  2.,  0.,  1., 13.],\n",
       "       [ 2.,  3.,  2.,  0.,  0.,  1.,  0.,  1.,  2., 11.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  1.,  2.,  0.,  1.,  0.,  0.,  0.,  7.],\n",
       "       [ 0.,  0.,  1.,  2.,  1.,  1.,  0.,  0.,  0.,  5.],\n",
       "       [ 2.,  1.,  0.,  2.,  2.,  3.,  1.,  1.,  2., 14.],\n",
       "       [ 1.,  2.,  0.,  1.,  0.,  3.,  0.,  0.,  1.,  8.],\n",
       "       [ 2.,  3.,  2.,  2.,  3.,  2.,  2.,  3.,  2., 21.],\n",
       "       [ 3.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.,  5.]])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtr[0:10,1693:1704]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPROCESSING\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# shuffle row-wise\n",
    "np.random.shuffle(mtr)\n",
    "\n",
    "data = mtr[:,0:1693]\n",
    "labels = mtr[:,1693:1704]\n",
    "\n",
    "# normalize data (features now have gauss dist., 0 mean and unit variance)\n",
    "data = sklearn.preprocessing.scale(data)\n",
    "\n",
    "# replace missing values with mean of their corresponding features\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "data = imp.fit_transform(data)\n",
    "\n",
    "# If NaNs should be dropped instead:\n",
    "# mtr = mtr[~np.isnan(mtr).any(axis=1)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  2.,  1.,  1.,  3.,  3.,  2.,  0.,  0., 15.],\n",
       "       [ 2.,  3.,  2.,  0.,  0.,  1.,  0.,  1.,  2., 11.],\n",
       "       [ 1.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  7.],\n",
       "       [ 2.,  2.,  1.,  1.,  1.,  0.,  1.,  2.,  0., 10.],\n",
       "       [ 1.,  1.,  3.,  1.,  2.,  1.,  1.,  1.,  1., 12.],\n",
       "       [ 2.,  1.,  2.,  1.,  3.,  1.,  1.,  1.,  0., 12.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 1.,  2.,  1.,  2.,  1.,  3.,  2.,  0.,  1., 13.],\n",
       "       [ 2.,  3.,  2.,  3.,  3.,  3.,  3.,  2.,  1., 22.],\n",
       "       [ 2.,  2.,  3.,  3.,  1.,  1.,  2.,  0.,  2., 16.]])"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA SPLIT (#nosnooping)\n",
    "\n",
    "# TEST DATA (%15 percent of data)\n",
    "numofppl_index = mtr.shape[0] - 1\n",
    "cut_index = int(mtr.shape[0] * 0.85)\n",
    "\n",
    "test_label = labels[cut_index:numofppl_index,:]\n",
    "test_data = data[cut_index:numofppl_index,:]\n",
    "\n",
    "# TRAINING AND VALIDATION DATA (%85 percent of data)\n",
    "\n",
    "train_label = labels[0:cut_index,:]\n",
    "train_data = data[0:cut_index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lizz = train_label[:,9:10].tolist()\n",
    "lizzn = []\n",
    "for i in range(0,len(lizz)):\n",
    "    lizzn.append(lizz[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 8,\n",
       "         1.0: 6,\n",
       "         2.0: 7,\n",
       "         3.0: 6,\n",
       "         4.0: 14,\n",
       "         5.0: 12,\n",
       "         6.0: 13,\n",
       "         7.0: 11,\n",
       "         8.0: 8,\n",
       "         9.0: 10,\n",
       "         10.0: 10,\n",
       "         11.0: 13,\n",
       "         12.0: 9,\n",
       "         13.0: 6,\n",
       "         14.0: 12,\n",
       "         15.0: 7,\n",
       "         16.0: 10,\n",
       "         17.0: 8,\n",
       "         18.0: 13,\n",
       "         19.0: 8,\n",
       "         20.0: 6,\n",
       "         21.0: 11,\n",
       "         22.0: 2,\n",
       "         23.0: 1,\n",
       "         24.0: 4,\n",
       "         25.0: 3,\n",
       "         26.0: 2,\n",
       "         27.0: 1})"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(lizzn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYNK+WNecGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f68c2d5b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "labels, values = zip(*Counter(lizzn).items())\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes + width * 0.5, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "phqcutoff = 18\n",
    "\n",
    "for i in range(0,train_label[:,9:10].shape[0]):\n",
    "    if(train_label[i][9] > phqcutoff):\n",
    "        train_label[i][9] = 1\n",
    "    else:\n",
    "        train_label[i][9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_label[:,9:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "results = np.zeros((10,7))\n",
    "\n",
    "def HyperTunerSVM(trainX,trainy):\n",
    "    \n",
    "    ftypes = [\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"] # and \"all\"\n",
    "    phqscores = [1,2,3,4,5,6,7,8,9,10] # just for readability; 10 is sum\n",
    "    for i in range(0,len(ftypes)):\n",
    "        for j in range(0,len(phqscores)):\n",
    "            \n",
    "            X = Xer(trainX, ftypes[i])\n",
    "            y = Yer(trainy, phqscores[j]).reshape(221,)\n",
    "            \n",
    "            c_range = list(range(1, 10))\n",
    "            parameters = {'kernel':('linear', 'rbf', 'poly', 'sigmoid'), 'C':c_range}\n",
    "\n",
    "            svc = svm.SVC()\n",
    "\n",
    "            grid = GridSearchCV(svc, parameters, cv=2, scoring='accuracy')\n",
    "            grid.fit(X, y)\n",
    "\n",
    "            results[j][i] = grid.grid_scores_[0].mean_validation_score\n",
    "\n",
    "            \n",
    "HyperTunerSVM(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>au</th>\n",
       "      <th>ig</th>\n",
       "      <th>txt</th>\n",
       "      <th>con</th>\n",
       "      <th>tw</th>\n",
       "      <th>call</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.334842</td>\n",
       "      <td>0.321267</td>\n",
       "      <td>0.239819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.289593</td>\n",
       "      <td>0.389140</td>\n",
       "      <td>0.389140</td>\n",
       "      <td>0.389140</td>\n",
       "      <td>0.389140</td>\n",
       "      <td>0.375566</td>\n",
       "      <td>0.289593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.266968</td>\n",
       "      <td>0.276018</td>\n",
       "      <td>0.276018</td>\n",
       "      <td>0.266968</td>\n",
       "      <td>0.276018</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.257919</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.316742</td>\n",
       "      <td>0.276018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.303167</td>\n",
       "      <td>0.312217</td>\n",
       "      <td>0.312217</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.312217</td>\n",
       "      <td>0.312217</td>\n",
       "      <td>0.298643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.330317</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.325792</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.321267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.339367</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.343891</td>\n",
       "      <td>0.248869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.552036</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.579186</td>\n",
       "      <td>0.533937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.488688</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.542986</td>\n",
       "      <td>0.488688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.841629</td>\n",
       "      <td>0.841629</td>\n",
       "      <td>0.841629</td>\n",
       "      <td>0.841629</td>\n",
       "      <td>0.837104</td>\n",
       "      <td>0.760181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         au        ig       txt       con        tw      call       all\n",
       "0  0.307692  0.334842  0.334842  0.321267  0.334842  0.321267  0.239819\n",
       "1  0.289593  0.389140  0.389140  0.389140  0.389140  0.375566  0.289593\n",
       "2  0.266968  0.276018  0.276018  0.266968  0.276018  0.307692  0.235294\n",
       "3  0.257919  0.330317  0.330317  0.330317  0.330317  0.316742  0.276018\n",
       "4  0.303167  0.312217  0.312217  0.294118  0.312217  0.312217  0.298643\n",
       "5  0.330317  0.325792  0.325792  0.325792  0.325792  0.307692  0.321267\n",
       "6  0.294118  0.352941  0.352941  0.339367  0.352941  0.343891  0.248869\n",
       "7  0.552036  0.588235  0.588235  0.588235  0.588235  0.579186  0.533937\n",
       "8  0.488688  0.538462  0.538462  0.538462  0.538462  0.542986  0.488688\n",
       "9  0.769231  0.841629  0.841629  0.841629  0.841629  0.837104  0.760181"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results, columns=[\"au\",\"ig\",\"txt\",\"con\",\"tw\",\"call\",\"all\"])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
